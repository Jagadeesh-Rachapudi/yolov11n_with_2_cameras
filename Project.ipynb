{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jagadeesh-Rachapudi/yolov11n_with_2_cameras/blob/main/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otISTwC3zztK",
        "outputId": "4dc7b4be-d84e-4a07-ef06-567100885825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'EfficientDM'...\n",
            "remote: Enumerating objects: 129, done.\u001b[K\n",
            "remote: Counting objects: 100% (129/129), done.\u001b[K\n",
            "remote: Compressing objects: 100% (74/74), done.\u001b[K\n",
            "remote: Total 129 (delta 35), reused 120 (delta 31), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (129/129), 3.55 MiB | 7.90 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ThisisBillhe/EfficientDM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p models/ldm/cin256-v2/\n",
        "!wget -O models/ldm/cin256-v2/model.ckpt https://ommer-lab.com/files/latent-diffusion/nitro/cin/model.ckpt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfbQ9UAKz5nI",
        "outputId": "bdcd1b2f-622c-452e-a981-42d44e3f2644"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-11 05:20:37--  https://ommer-lab.com/files/latent-diffusion/nitro/cin/model.ckpt\n",
            "Resolving ommer-lab.com (ommer-lab.com)... 141.84.41.65\n",
            "Connecting to ommer-lab.com (ommer-lab.com)|141.84.41.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1827378153 (1.7G)\n",
            "Saving to: ‘models/ldm/cin256-v2/model.ckpt’\n",
            "\n",
            "models/ldm/cin256-v 100%[===================>]   1.70G  23.9MB/s    in 77s     \n",
            "\n",
            "2024-11-11 05:21:55 (22.7 MB/s) - ‘models/ldm/cin256-v2/model.ckpt’ saved [1827378153/1827378153]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf einops pytorch-lightning transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Ox9gnoW60t76",
        "outputId": "bdff9e1a-89f6-49f8-a34e-6f89391c4c33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting omegaconf\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.10.0)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.5.2-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.11.8-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.10.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.0)\n",
            "Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.8-py3-none-any.whl (26 kB)\n",
            "Downloading torchmetrics-1.5.2-py3-none-any.whl (891 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m891.4/891.4 kB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=660cf465014263e2d1e3975f461042ee3772f07f97743657e80534cf20ca2da3\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, omegaconf, lightning-utilities, torchmetrics, pytorch-lightning\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 lightning-utilities-0.11.8 omegaconf-2.3.0 pytorch-lightning-2.4.0 torchmetrics-1.5.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "553df79889a94039ab1323b7af6ea653"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/EfficientDM/quant_scripts/collect_input_4_calib.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VtNPvcWz83P",
        "outputId": "641aa961-0a22-4a72-d197-7f0ef4105715"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/EfficientDM/quant_scripts/collect_input_4_calib.py\", line 13, in <module>\n",
            "    from ldm.util import instantiate_from_config\n",
            "ModuleNotFoundError: No module named 'ldm'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import abc\n",
        "from einops import rearrange\n",
        "from functools import partial\n",
        "\n",
        "import multiprocessing as mp\n",
        "from threading import Thread\n",
        "from queue import Queue\n",
        "\n",
        "from inspect import isfunction\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "\n",
        "def log_txt_as_img(wh, xc, size=10):\n",
        "    # wh a tuple of (width, height)\n",
        "    # xc a list of captions to plot\n",
        "    b = len(xc)\n",
        "    txts = list()\n",
        "    for bi in range(b):\n",
        "        txt = Image.new(\"RGB\", wh, color=\"white\")\n",
        "        draw = ImageDraw.Draw(txt)\n",
        "        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n",
        "        nc = int(40 * (wh[0] / 256))\n",
        "        lines = \"\\n\".join(xc[bi][start:start + nc] for start in range(0, len(xc[bi]), nc))\n",
        "\n",
        "        try:\n",
        "            draw.text((0, 0), lines, fill=\"black\", font=font)\n",
        "        except UnicodeEncodeError:\n",
        "            print(\"Cant encode string for logging. Skipping.\")\n",
        "\n",
        "        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n",
        "        txts.append(txt)\n",
        "    txts = np.stack(txts)\n",
        "    txts = torch.tensor(txts)\n",
        "    return txts\n",
        "\n",
        "\n",
        "def ismap(x):\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        return False\n",
        "    return (len(x.shape) == 4) and (x.shape[1] > 3)\n",
        "\n",
        "\n",
        "def isimage(x):\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        return False\n",
        "    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\n",
        "\n",
        "\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def mean_flat(tensor):\n",
        "    \"\"\"\n",
        "    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n",
        "    Take the mean over all non-batch dimensions.\n",
        "    \"\"\"\n",
        "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
        "\n",
        "\n",
        "def count_params(model, verbose=False):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    if verbose:\n",
        "        print(f\"{model.__class__.__name__} has {total_params * 1.e-6:.2f} M params.\")\n",
        "    return total_params\n",
        "\n",
        "\n",
        "def instantiate_from_config(config):\n",
        "    if not \"target\" in config:\n",
        "        if config == '__is_first_stage__':\n",
        "            return None\n",
        "        elif config == \"__is_unconditional__\":\n",
        "            return None\n",
        "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
        "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
        "\n",
        "\n",
        "def get_obj_from_str(string, reload=False):\n",
        "    module, cls = string.rsplit(\".\", 1)\n",
        "    if reload:\n",
        "        module_imp = importlib.import_module(module)\n",
        "        importlib.reload(module_imp)\n",
        "    return getattr(importlib.import_module(module, package=None), cls)\n",
        "\n",
        "\n",
        "def _do_parallel_data_prefetch(func, Q, data, idx, idx_to_fn=False):\n",
        "    # create dummy dataset instance\n",
        "\n",
        "    # run prefetching\n",
        "    if idx_to_fn:\n",
        "        res = func(data, worker_id=idx)\n",
        "    else:\n",
        "        res = func(data)\n",
        "    Q.put([idx, res])\n",
        "    Q.put(\"Done\")\n",
        "\n",
        "\n",
        "def parallel_data_prefetch(\n",
        "        func: callable, data, n_proc, target_data_type=\"ndarray\", cpu_intensive=True, use_worker_id=False\n",
        "):\n",
        "    # if target_data_type not in [\"ndarray\", \"list\"]:\n",
        "    #     raise ValueError(\n",
        "    #         \"Data, which is passed to parallel_data_prefetch has to be either of type list or ndarray.\"\n",
        "    #     )\n",
        "    if isinstance(data, np.ndarray) and target_data_type == \"list\":\n",
        "        raise ValueError(\"list expected but function got ndarray.\")\n",
        "    elif isinstance(data, abc.Iterable):\n",
        "        if isinstance(data, dict):\n",
        "            print(\n",
        "                f'WARNING:\"data\" argument passed to parallel_data_prefetch is a dict: Using only its values and disregarding keys.'\n",
        "            )\n",
        "            data = list(data.values())\n",
        "        if target_data_type == \"ndarray\":\n",
        "            data = np.asarray(data)\n",
        "        else:\n",
        "            data = list(data)\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            f\"The data, that shall be processed parallel has to be either an np.ndarray or an Iterable, but is actually {type(data)}.\"\n",
        "        )\n",
        "\n",
        "    if cpu_intensive:\n",
        "        Q = mp.Queue(1000)\n",
        "        proc = mp.Process\n",
        "    else:\n",
        "        Q = Queue(1000)\n",
        "        proc = Thread\n",
        "    # spawn processes\n",
        "    if target_data_type == \"ndarray\":\n",
        "        arguments = [\n",
        "            [func, Q, part, i, use_worker_id]\n",
        "            for i, part in enumerate(np.array_split(data, n_proc))\n",
        "        ]\n",
        "    else:\n",
        "        step = (\n",
        "            int(len(data) / n_proc + 1)\n",
        "            if len(data) % n_proc != 0\n",
        "            else int(len(data) / n_proc)\n",
        "        )\n",
        "        arguments = [\n",
        "            [func, Q, part, i, use_worker_id]\n",
        "            for i, part in enumerate(\n",
        "                [data[i: i + step] for i in range(0, len(data), step)]\n",
        "            )\n",
        "        ]\n",
        "    processes = []\n",
        "    for i in range(n_proc):\n",
        "        p = proc(target=_do_parallel_data_prefetch, args=arguments[i])\n",
        "        processes += [p]\n",
        "\n",
        "    # start processes\n",
        "    print(f\"Start prefetching...\")\n",
        "    import time\n",
        "\n",
        "    start = time.time()\n",
        "    gather_res = [[] for _ in range(n_proc)]\n",
        "    try:\n",
        "        for p in processes:\n",
        "            p.start()\n",
        "\n",
        "        k = 0\n",
        "        while k < n_proc:\n",
        "            # get result\n",
        "            res = Q.get()\n",
        "            if res == \"Done\":\n",
        "                k += 1\n",
        "            else:\n",
        "                gather_res[res[0]] = res[1]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Exception: \", e)\n",
        "        for p in processes:\n",
        "            p.terminate()\n",
        "\n",
        "        raise e\n",
        "    finally:\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "        print(f\"Prefetching complete. [{time.time() - start} sec.]\")\n",
        "\n",
        "    if target_data_type == 'ndarray':\n",
        "        if not isinstance(gather_res[0], np.ndarray):\n",
        "            return np.concatenate([np.asarray(r) for r in gather_res], axis=0)\n",
        "\n",
        "        # order outputs\n",
        "        return np.concatenate(gather_res, axis=0)\n",
        "    elif target_data_type == 'list':\n",
        "        out = []\n",
        "        for r in gather_res:\n",
        "            out.extend(r)\n",
        "        return out\n",
        "    else:\n",
        "        return gather_res\n"
      ],
      "metadata": {
        "id": "oewbumlg1kxv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# catch diffusion input for calibration\n",
        "global diffusion_input_list\n",
        "diffusion_input_list = []\n",
        "\n",
        "def appendInput(value):\n",
        "    diffusion_input_list.append(value)\n",
        "\n",
        "def getInputList():\n",
        "    return diffusion_input_list\n",
        "\n",
        "\n",
        "global optimizer_state_list\n",
        "optimizer_state_list = []\n",
        "def init_state_list(num_steps):\n",
        "    for _ in range(num_steps):\n",
        "        optimizer_state_list.append([])\n",
        "\n",
        "def saveStep(step, state):\n",
        "    optimizer_state_list[step].append(state)\n",
        "\n",
        "def getStep(step):\n",
        "    if len(optimizer_state_list[step]) == 0:\n",
        "        return None\n",
        "    else:\n",
        "        return optimizer_state_list[step].pop()\n"
      ],
      "metadata": {
        "id": "oO24FXeC3Ib0"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xTeZQZRRDh5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/EfficientDM\")\n",
        "\n",
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import repeat\n",
        "\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "def make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
        "    if schedule == \"linear\":\n",
        "        betas = (\n",
        "                torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
        "        )\n",
        "\n",
        "    elif schedule == \"cosine\":\n",
        "        timesteps = (\n",
        "                torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n",
        "        )\n",
        "        alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
        "        alphas = torch.cos(alphas).pow(2)\n",
        "        alphas = alphas / alphas[0]\n",
        "        betas = 1 - alphas[1:] / alphas[:-1]\n",
        "        betas = np.clip(betas, a_min=0, a_max=0.999)\n",
        "\n",
        "    elif schedule == \"sqrt_linear\":\n",
        "        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n",
        "    elif schedule == \"sqrt\":\n",
        "        betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5\n",
        "    else:\n",
        "        raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
        "    return betas.numpy()\n",
        "\n",
        "\n",
        "def make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n",
        "    if ddim_discr_method == 'uniform':\n",
        "        c = num_ddpm_timesteps // num_ddim_timesteps\n",
        "        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
        "    elif ddim_discr_method == 'quad':\n",
        "        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n",
        "    else:\n",
        "        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n",
        "\n",
        "    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n",
        "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
        "    steps_out = ddim_timesteps + 1\n",
        "    if verbose:\n",
        "        print(f'Selected timesteps for ddim sampler: {steps_out}')\n",
        "    return steps_out\n",
        "\n",
        "\n",
        "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n",
        "    # select alphas for computing the variance schedule\n",
        "    alphas = alphacums[ddim_timesteps]\n",
        "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
        "\n",
        "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
        "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
        "    if verbose:\n",
        "\n",
        "      import math\n",
        "      import torch\n",
        "      import torch.nn as nn\n",
        "      import numpy as np\n",
        "      from einops import repeat\n",
        "\n",
        "      from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "      def make_beta_schedule(schedule, n_timestep, linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n",
        "          if schedule == \"linear\":\n",
        "              betas = (\n",
        "                      torch.linspace(linear_start ** 0.5, linear_end ** 0.5, n_timestep, dtype=torch.float64) ** 2\n",
        "              )\n",
        "\n",
        "          elif schedule == \"cosine\":\n",
        "              timesteps = (\n",
        "                      torch.arange(n_timestep + 1, dtype=torch.float64) / n_timestep + cosine_s\n",
        "              )\n",
        "              alphas = timesteps / (1 + cosine_s) * np.pi / 2\n",
        "              alphas = torch.cos(alphas).pow(2)\n",
        "              alphas = alphas / alphas[0]\n",
        "              betas = 1 - alphas[1:] / alphas[:-1]\n",
        "              betas = np.clip(betas, a_min=0, a_max=0.999)\n",
        "\n",
        "          elif schedule == \"sqrt_linear\":\n",
        "              betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64)\n",
        "          elif schedule == \"sqrt\":\n",
        "              betas = torch.linspace(linear_start, linear_end, n_timestep, dtype=torch.float64) ** 0.5\n",
        "          else:\n",
        "              raise ValueError(f\"schedule '{schedule}' unknown.\")\n",
        "          return betas.numpy()\n",
        "\n",
        "\n",
        "def make_ddim_timesteps(ddim_discr_method, num_ddim_timesteps, num_ddpm_timesteps, verbose=True):\n",
        "    if ddim_discr_method == 'uniform':\n",
        "        c = num_ddpm_timesteps // num_ddim_timesteps\n",
        "        ddim_timesteps = np.asarray(list(range(0, num_ddpm_timesteps, c)))\n",
        "    elif ddim_discr_method == 'quad':\n",
        "        ddim_timesteps = ((np.linspace(0, np.sqrt(num_ddpm_timesteps * .8), num_ddim_timesteps)) ** 2).astype(int)\n",
        "    else:\n",
        "        raise NotImplementedError(f'There is no ddim discretization method called \"{ddim_discr_method}\"')\n",
        "\n",
        "    # assert ddim_timesteps.shape[0] == num_ddim_timesteps\n",
        "    # add one to get the final alpha values right (the ones from first scale to data during sampling)\n",
        "    steps_out = ddim_timesteps + 1\n",
        "    if verbose:\n",
        "        print(f'Selected timesteps for ddim sampler: {steps_out}')\n",
        "    return steps_out\n",
        "\n",
        "\n",
        "def make_ddim_sampling_parameters(alphacums, ddim_timesteps, eta, verbose=True):\n",
        "    # select alphas for computing the variance schedule\n",
        "    alphas = alphacums[ddim_timesteps]\n",
        "    alphas_prev = np.asarray([alphacums[0]] + alphacums[ddim_timesteps[:-1]].tolist())\n",
        "\n",
        "    # according the the formula provided in https://arxiv.org/abs/2010.02502\n",
        "    sigmas = eta * np.sqrt((1 - alphas_prev) / (1 - alphas) * (1 - alphas / alphas_prev))\n",
        "    if verbose:\n",
        "        print(f'Selected alphas for ddim sampler: a_t: {alphas}; a_(t-1): {alphas_prev}')\n",
        "        print(f'For the chosen value of eta, which is {eta}, '\n",
        "              f'this results in the following sigma_t schedule for ddim sampler {sigmas}')\n",
        "    return sigmas, alphas, alphas_prev\n",
        "\n",
        "\n",
        "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n",
        "    \"\"\"\n",
        "    Create a beta schedule that discretizes the given alpha_t_bar function,\n",
        "    which defines the cumulative product of (1-beta) over time from t = [0,1].\n",
        "    :param num_diffusion_timesteps: the number of betas to produce.\n",
        "    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n",
        "                      produces the cumulative product of (1-beta) up to that\n",
        "                      part of the diffusion process.\n",
        "    :param max_beta: the maximum beta to use; use values lower than 1 to\n",
        "                     prevent singularities.\n",
        "    \"\"\"\n",
        "    betas = []\n",
        "    for i in range(num_diffusion_timesteps):\n",
        "        t1 = i / num_diffusion_timesteps\n",
        "        t2 = (i + 1) / num_diffusion_timesteps\n",
        "        betas.append(min(1 - alpha_bar(t2) / alpha_bar(t1), max_beta))\n",
        "    return np.array(betas)\n",
        "\n",
        "\n",
        "def extract_into_tensor(a, t, x_shape):\n",
        "    b, *_ = t.shape\n",
        "    out = a.gather(-1, t)\n",
        "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
        "\n",
        "\n",
        "def checkpoint(func, inputs, params, flag):\n",
        "    \"\"\"\n",
        "    Evaluate a function without caching intermediate activations, allowing for\n",
        "    reduced memory at the expense of extra compute in the backward pass.\n",
        "    :param func: the function to evaluate.\n",
        "    :param inputs: the argument sequence to pass to `func`.\n",
        "    :param params: a sequence of parameters `func` depends on but does not\n",
        "                   explicitly take as arguments.\n",
        "    :param flag: if False, disable gradient checkpointing.\n",
        "    \"\"\"\n",
        "    if flag:\n",
        "        args = tuple(inputs) + tuple(params)\n",
        "        return CheckpointFunction.apply(func, len(inputs), *args)\n",
        "    else:\n",
        "        return func(*inputs)\n",
        "\n",
        "\n",
        "class CheckpointFunction(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, run_function, length, *args):\n",
        "        ctx.run_function = run_function\n",
        "        ctx.input_tensors = list(args[:length])\n",
        "        ctx.input_params = list(args[length:])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_tensors = ctx.run_function(*ctx.input_tensors)\n",
        "        return output_tensors\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, *output_grads):\n",
        "        ctx.input_tensors = [x.detach().requires_grad_(True) for x in ctx.input_tensors]\n",
        "        with torch.enable_grad():\n",
        "            # Fixes a bug where the first op in run_function modifies the\n",
        "            # Tensor storage in place, which is not allowed for detach()'d\n",
        "            # Tensors.\n",
        "            shallow_copies = [x.view_as(x) for x in ctx.input_tensors]\n",
        "            output_tensors = ctx.run_function(*shallow_copies)\n",
        "        input_grads = torch.autograd.grad(\n",
        "            output_tensors,\n",
        "            ctx.input_tensors + ctx.input_params,\n",
        "            output_grads,\n",
        "            allow_unused=True,\n",
        "        )\n",
        "        del ctx.input_tensors\n",
        "        del ctx.input_params\n",
        "        del output_tensors\n",
        "        return (None, None) + input_grads\n",
        "\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
        "    \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
        "                      These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "    if not repeat_only:\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(\n",
        "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "        ).to(device=timesteps.device)\n",
        "        args = timesteps[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    else:\n",
        "        embedding = repeat(timesteps, 'b -> b d', d=dim)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "\n",
        "def scale_module(module, scale):\n",
        "    \"\"\"\n",
        "    Scale the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().mul_(scale)\n",
        "    return module\n",
        "\n",
        "\n",
        "def mean_flat(tensor):\n",
        "    \"\"\"\n",
        "    Take the mean over all non-batch dimensions.\n",
        "    \"\"\"\n",
        "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
        "\n",
        "\n",
        "def normalization(channels):\n",
        "    \"\"\"\n",
        "    Make a standard normalization layer.\n",
        "    :param channels: number of input channels.\n",
        "    :return: an nn.Module for normalization.\n",
        "    \"\"\"\n",
        "    return GroupNorm32(32, channels)\n",
        "\n",
        "\n",
        "# PyTorch 1.7 has SiLU, but we support PyTorch 1.5.\n",
        "class SiLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "class GroupNorm32(nn.GroupNorm):\n",
        "    def forward(self, x):\n",
        "        return super().forward(x.float()).type(x.dtype)\n",
        "\n",
        "def conv_nd(dims, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a 1D, 2D, or 3D convolution module.\n",
        "    \"\"\"\n",
        "    if dims == 1:\n",
        "        return nn.Conv1d(*args, **kwargs)\n",
        "    elif dims == 2:\n",
        "        return nn.Conv2d(*args, **kwargs)\n",
        "    elif dims == 3:\n",
        "        return nn.Conv3d(*args, **kwargs)\n",
        "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
        "\n",
        "\n",
        "def linear(*args, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a linear module.\n",
        "    \"\"\"\n",
        "    return nn.Linear(*args, **kwargs)\n",
        "\n",
        "\n",
        "def avg_pool_nd(dims, *args, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a 1D, 2D, or 3D average pooling module.\n",
        "    \"\"\"\n",
        "    if dims == 1:\n",
        "        return nn.AvgPool1d(*args, **kwargs)\n",
        "    elif dims == 2:\n",
        "        return nn.AvgPool2d(*args, **kwargs)\n",
        "    elif dims == 3:\n",
        "        return nn.AvgPool3d(*args, **kwargs)\n",
        "    raise ValueError(f\"unsupported dimensions: {dims}\")\n",
        "\n",
        "\n",
        "class HybridConditioner(nn.Module):\n",
        "\n",
        "    def __init__(self, c_concat_config, c_crossattn_config):\n",
        "        super().__init__()\n",
        "        self.concat_conditioner = instantiate_from_config(c_concat_config)\n",
        "        self.crossattn_conditioner = instantiate_from_config(c_crossattn_config)\n",
        "\n",
        "    def forward(self, c_concat, c_crossattn):\n",
        "        c_concat = self.concat_conditioner(c_concat)\n",
        "        c_crossattn = self.crossattn_conditioner(c_crossattn)\n",
        "        return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}\n",
        "\n",
        "\n",
        "def noise_like(shape, device, repeat=False):\n",
        "    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))\n",
        "    noise = lambda: torch.randn(shape, device=device)\n",
        "    return repeat_noise() if repeat else noise()"
      ],
      "metadata": {
        "id": "Wmeo3lrn3Qvi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"SAMPLING ONLY.\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "\n",
        "# from ldm.modules.diffusionmodules.util import make_ddim_sampling_parameters, make_ddim_timesteps, noise_like\n",
        "# import ldm.globalvar as globalvar\n",
        "\n",
        "class DDIMSampler(object):\n",
        "    def __init__(self, model, schedule=\"linear\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.ddpm_num_timesteps = model.num_timesteps\n",
        "        self.schedule = schedule\n",
        "\n",
        "    def register_buffer(self, name, attr):\n",
        "        if type(attr) == torch.Tensor:\n",
        "            if attr.device != torch.device(\"cuda\"):\n",
        "                attr = attr.to(torch.device(\"cuda\"))\n",
        "        setattr(self, name, attr)\n",
        "\n",
        "    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n",
        "        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n",
        "                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n",
        "        alphas_cumprod = self.model.alphas_cumprod\n",
        "        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n",
        "        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n",
        "\n",
        "        self.register_buffer('betas', to_torch(self.model.betas))\n",
        "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
        "        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n",
        "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n",
        "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n",
        "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n",
        "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n",
        "\n",
        "        # ddim sampling parameters\n",
        "        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n",
        "                                                                                   ddim_timesteps=self.ddim_timesteps,\n",
        "                                                                                   eta=ddim_eta,verbose=verbose)\n",
        "        self.register_buffer('ddim_sigmas', ddim_sigmas)\n",
        "        self.register_buffer('ddim_alphas', ddim_alphas)\n",
        "        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n",
        "        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n",
        "        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n",
        "            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n",
        "                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n",
        "        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def sample(self,\n",
        "               S,\n",
        "               batch_size,\n",
        "               shape,\n",
        "               conditioning=None,\n",
        "               callback=None,\n",
        "               normals_sequence=None,\n",
        "               img_callback=None,\n",
        "               quantize_x0=False,\n",
        "               eta=0.,\n",
        "               mask=None,\n",
        "               x0=None,\n",
        "               temperature=1.,\n",
        "               noise_dropout=0.,\n",
        "               score_corrector=None,\n",
        "               corrector_kwargs=None,\n",
        "               verbose=True,\n",
        "               x_T=None,\n",
        "               log_every_t=100,\n",
        "               unconditional_guidance_scale=1.,\n",
        "               unconditional_conditioning=None,\n",
        "               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n",
        "               **kwargs\n",
        "               ):\n",
        "        if conditioning is not None:\n",
        "            if isinstance(conditioning, dict):\n",
        "                cbs = conditioning[list(conditioning.keys())[0]].shape[0]\n",
        "                if cbs != batch_size:\n",
        "                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n",
        "            else:\n",
        "                if conditioning.shape[0] != batch_size:\n",
        "                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n",
        "\n",
        "        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n",
        "        # sampling\n",
        "        C, H, W = shape\n",
        "        size = (batch_size, C, H, W)\n",
        "        print(f'Data shape for DDIM sampling is {size}, eta {eta}')\n",
        "\n",
        "        samples, intermediates = self.ddim_sampling(conditioning, size,\n",
        "                                                    callback=callback,\n",
        "                                                    img_callback=img_callback,\n",
        "                                                    quantize_denoised=quantize_x0,\n",
        "                                                    mask=mask, x0=x0,\n",
        "                                                    ddim_use_original_steps=False,\n",
        "                                                    noise_dropout=noise_dropout,\n",
        "                                                    temperature=temperature,\n",
        "                                                    score_corrector=score_corrector,\n",
        "                                                    corrector_kwargs=corrector_kwargs,\n",
        "                                                    x_T=x_T,\n",
        "                                                    log_every_t=log_every_t,\n",
        "                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n",
        "                                                    unconditional_conditioning=unconditional_conditioning,\n",
        "                                                    )\n",
        "        return samples, intermediates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ddim_sampling(self, cond, shape,\n",
        "                      x_T=None, ddim_use_original_steps=False,\n",
        "                      callback=None, timesteps=None, quantize_denoised=False,\n",
        "                      mask=None, x0=None, img_callback=None, log_every_t=100,\n",
        "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
        "                      unconditional_guidance_scale=1., unconditional_conditioning=None,):\n",
        "        device = self.model.betas.device\n",
        "        b = shape[0]\n",
        "        if x_T is None:\n",
        "            img = torch.randn(shape, device=device)\n",
        "        else:\n",
        "            img = x_T\n",
        "\n",
        "        if timesteps is None:\n",
        "            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n",
        "        elif timesteps is not None and not ddim_use_original_steps:\n",
        "            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n",
        "            timesteps = self.ddim_timesteps[:subset_end]\n",
        "\n",
        "        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n",
        "        time_range = reversed(range(0,timesteps)) if ddim_use_original_steps else np.flip(timesteps)\n",
        "        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n",
        "        print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n",
        "\n",
        "        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n",
        "\n",
        "        for i, step in enumerate(iterator):\n",
        "            index = total_steps - i - 1\n",
        "            ts = torch.full((b,), step, device=device, dtype=torch.long)\n",
        "\n",
        "            if mask is not None:\n",
        "                assert x0 is not None\n",
        "                img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass?\n",
        "                img = img_orig * mask + (1. - mask) * img\n",
        "\n",
        "            outs = self.p_sample_ddim(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n",
        "                                      quantize_denoised=quantize_denoised, temperature=temperature,\n",
        "                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n",
        "                                      corrector_kwargs=corrector_kwargs,\n",
        "                                      unconditional_guidance_scale=unconditional_guidance_scale,\n",
        "                                      unconditional_conditioning=unconditional_conditioning)\n",
        "            img, pred_x0 = outs\n",
        "            if callback: callback(i)\n",
        "            if img_callback: img_callback(pred_x0, i)\n",
        "\n",
        "            if index % log_every_t == 0 or index == total_steps - 1:\n",
        "                intermediates['x_inter'].append(img)\n",
        "                intermediates['pred_x0'].append(pred_x0)\n",
        "\n",
        "        return img, intermediates\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def p_sample_ddim(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n",
        "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
        "                      unconditional_guidance_scale=1., unconditional_conditioning=None):\n",
        "        b, *_, device = *x.shape, x.device\n",
        "\n",
        "        if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n",
        "            e_t = self.model.apply_model(x, t, c)\n",
        "        else:\n",
        "            x_in = torch.cat([x] * 2)\n",
        "            t_in = torch.cat([t] * 2)\n",
        "            c_in = torch.cat([unconditional_conditioning, c])\n",
        "            e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(2)\n",
        "            e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n",
        "\n",
        "        if score_corrector is not None:\n",
        "            assert self.model.parameterization == \"eps\"\n",
        "            e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n",
        "\n",
        "        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n",
        "        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n",
        "        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n",
        "        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n",
        "        # select parameters corresponding to the currently considered timestep\n",
        "        a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
        "        a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
        "        sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n",
        "        sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n",
        "\n",
        "        # current prediction for x_0\n",
        "        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
        "        if quantize_denoised:\n",
        "            pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n",
        "        # direction pointing to x_t\n",
        "        dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
        "        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n",
        "        if noise_dropout > 0.:\n",
        "            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
        "        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
        "        return x_prev, pred_x0\n",
        "\n",
        "class DDIMSampler_trainer(object):\n",
        "    def __init__(self, model, quant_model, lr_scheduler, optimizer, schedule=\"linear\", **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.quant_model = quant_model\n",
        "        self.ddpm_num_timesteps = model.num_timesteps\n",
        "        self.schedule = schedule\n",
        "        self.hook_list = []\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def register_buffer(self, name, attr):\n",
        "        if type(attr) == torch.Tensor:\n",
        "            if attr.device != torch.device(\"cuda\"):\n",
        "                attr = attr.to(torch.device(\"cuda\"))\n",
        "        setattr(self, name, attr)\n",
        "\n",
        "    def make_schedule(self, ddim_num_steps, ddim_discretize=\"uniform\", ddim_eta=0., verbose=True):\n",
        "        self.ddim_timesteps = make_ddim_timesteps(ddim_discr_method=ddim_discretize, num_ddim_timesteps=ddim_num_steps,\n",
        "                                                  num_ddpm_timesteps=self.ddpm_num_timesteps,verbose=verbose)\n",
        "        alphas_cumprod = self.model.alphas_cumprod\n",
        "        assert alphas_cumprod.shape[0] == self.ddpm_num_timesteps, 'alphas have to be defined for each timestep'\n",
        "        to_torch = lambda x: x.clone().detach().to(torch.float32).to(self.model.device)\n",
        "\n",
        "        self.register_buffer('betas', to_torch(self.model.betas))\n",
        "        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n",
        "        self.register_buffer('alphas_cumprod_prev', to_torch(self.model.alphas_cumprod_prev))\n",
        "\n",
        "        # calculations for diffusion q(x_t | x_{t-1}) and others\n",
        "        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod.cpu())))\n",
        "        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod.cpu())))\n",
        "        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod.cpu())))\n",
        "        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu())))\n",
        "        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod.cpu() - 1)))\n",
        "\n",
        "        # ddim sampling parameters\n",
        "        ddim_sigmas, ddim_alphas, ddim_alphas_prev = make_ddim_sampling_parameters(alphacums=alphas_cumprod.cpu(),\n",
        "                                                                                   ddim_timesteps=self.ddim_timesteps,\n",
        "                                                                                   eta=ddim_eta,verbose=verbose)\n",
        "        self.register_buffer('ddim_sigmas', ddim_sigmas)\n",
        "        self.register_buffer('ddim_alphas', ddim_alphas)\n",
        "        self.register_buffer('ddim_alphas_prev', ddim_alphas_prev)\n",
        "        self.register_buffer('ddim_sqrt_one_minus_alphas', np.sqrt(1. - ddim_alphas))\n",
        "        sigmas_for_original_sampling_steps = ddim_eta * torch.sqrt(\n",
        "            (1 - self.alphas_cumprod_prev) / (1 - self.alphas_cumprod) * (\n",
        "                        1 - self.alphas_cumprod / self.alphas_cumprod_prev))\n",
        "        self.register_buffer('ddim_sigmas_for_original_num_steps', sigmas_for_original_sampling_steps)\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def sample(self,\n",
        "               S,\n",
        "               batch_size,\n",
        "               shape,\n",
        "               conditioning=None,\n",
        "               callback=None,\n",
        "               normals_sequence=None,\n",
        "               img_callback=None,\n",
        "               quantize_x0=False,\n",
        "               eta=0.,\n",
        "               mask=None,\n",
        "               x0=None,\n",
        "               temperature=1.,\n",
        "               noise_dropout=0.,\n",
        "               score_corrector=None,\n",
        "               corrector_kwargs=None,\n",
        "               verbose=True,\n",
        "               x_T=None,\n",
        "               log_every_t=100,\n",
        "               unconditional_guidance_scale=1.,\n",
        "               unconditional_conditioning=None,\n",
        "               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n",
        "               **kwargs\n",
        "               ):\n",
        "        if conditioning is not None:\n",
        "            if isinstance(conditioning, dict):\n",
        "                cbs = conditioning[list(conditioning.keys())[0]].shape[0]\n",
        "                if cbs != batch_size:\n",
        "                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n",
        "            else:\n",
        "                if conditioning.shape[0] != batch_size:\n",
        "                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n",
        "\n",
        "        self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)\n",
        "        # sampling\n",
        "        C, H, W = shape\n",
        "        size = (batch_size, C, H, W)\n",
        "        print(f'Data shape for DDIM sampling is {size}, eta {eta}')\n",
        "\n",
        "        samples, intermediates = self.ddim_sampling(conditioning, size,\n",
        "                                                    callback=callback,\n",
        "                                                    img_callback=img_callback,\n",
        "                                                    quantize_denoised=quantize_x0,\n",
        "                                                    mask=mask, x0=x0,\n",
        "                                                    ddim_use_original_steps=False,\n",
        "                                                    noise_dropout=noise_dropout,\n",
        "                                                    temperature=temperature,\n",
        "                                                    score_corrector=score_corrector,\n",
        "                                                    corrector_kwargs=corrector_kwargs,\n",
        "                                                    x_T=x_T,\n",
        "                                                    log_every_t=log_every_t,\n",
        "                                                    unconditional_guidance_scale=unconditional_guidance_scale,\n",
        "                                                    unconditional_conditioning=unconditional_conditioning,\n",
        "                                                    )\n",
        "        return samples, intermediates\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def ddim_sampling(self, cond, shape,\n",
        "                      x_T=None, ddim_use_original_steps=False,\n",
        "                      callback=None, timesteps=None, quantize_denoised=False,\n",
        "                      mask=None, x0=None, img_callback=None, log_every_t=100,\n",
        "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
        "                      unconditional_guidance_scale=1., unconditional_conditioning=None,):\n",
        "        device = self.model.betas.device\n",
        "        b = shape[0]\n",
        "        if x_T is None:\n",
        "            img = torch.randn(shape, device=device)\n",
        "        else:\n",
        "            img = x_T\n",
        "\n",
        "        if timesteps is None:\n",
        "            timesteps = self.ddpm_num_timesteps if ddim_use_original_steps else self.ddim_timesteps\n",
        "        elif timesteps is not None and not ddim_use_original_steps:\n",
        "            subset_end = int(min(timesteps / self.ddim_timesteps.shape[0], 1) * self.ddim_timesteps.shape[0]) - 1\n",
        "            timesteps = self.ddim_timesteps[:subset_end]\n",
        "\n",
        "        intermediates = {'x_inter': [img], 'pred_x0': [img]}\n",
        "        time_range = reversed(range(0,timesteps)) if ddim_use_original_steps else np.flip(timesteps)\n",
        "        total_steps = timesteps if ddim_use_original_steps else timesteps.shape[0]\n",
        "        print(f\"Running DDIM Sampling with {total_steps} timesteps\")\n",
        "\n",
        "        iterator = tqdm(time_range, desc='DDIM Sampler', total=total_steps)\n",
        "\n",
        "        for i, step in enumerate(iterator):\n",
        "            index = total_steps - i - 1\n",
        "            ts = torch.full((b,), step, device=device, dtype=img.dtype)\n",
        "\n",
        "            if mask is not None:\n",
        "                assert x0 is not None\n",
        "                img_orig = self.model.q_sample(x0, ts)  # TODO: deterministic forward pass?\n",
        "                img = img_orig * mask + (1. - mask) * img\n",
        "\n",
        "            outs = self.p_sample_ddim(img, cond, ts, index=index, use_original_steps=ddim_use_original_steps,\n",
        "                                      quantize_denoised=quantize_denoised, temperature=temperature,\n",
        "                                      noise_dropout=noise_dropout, score_corrector=score_corrector,\n",
        "                                      corrector_kwargs=corrector_kwargs,\n",
        "                                      unconditional_guidance_scale=unconditional_guidance_scale,\n",
        "                                      unconditional_conditioning=unconditional_conditioning)\n",
        "            img, pred_x0 = outs\n",
        "            if callback: callback(i)\n",
        "            if img_callback: img_callback(pred_x0, i)\n",
        "\n",
        "            if index % log_every_t == 0 or index == total_steps - 1:\n",
        "                intermediates['x_inter'].append(img)\n",
        "                intermediates['pred_x0'].append(pred_x0)\n",
        "\n",
        "        return img, intermediates\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    def p_sample_ddim(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,\n",
        "                      temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
        "                      unconditional_guidance_scale=1., unconditional_conditioning=None):\n",
        "        b, *_, device = *x.shape, x.device\n",
        "        # optimizer_state = globalvar.getStep(index)\n",
        "        # if optimizer_state is not None:\n",
        "        #     self.optimizer.load_state_dict(optimizer_state)\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        alphas = self.model.alphas_cumprod if use_original_steps else self.ddim_alphas\n",
        "        alphas_prev = self.model.alphas_cumprod_prev if use_original_steps else self.ddim_alphas_prev\n",
        "        sqrt_one_minus_alphas = self.model.sqrt_one_minus_alphas_cumprod if use_original_steps else self.ddim_sqrt_one_minus_alphas\n",
        "        sigmas = self.model.ddim_sigmas_for_original_num_steps if use_original_steps else self.ddim_sigmas\n",
        "        # select parameters corresponding to the currently considered timestep\n",
        "        a_t = torch.full((b, 1, 1, 1), alphas[index], device=device)\n",
        "        a_prev = torch.full((b, 1, 1, 1), alphas_prev[index], device=device)\n",
        "        sigma_t = torch.full((b, 1, 1, 1), sigmas[index], device=device)\n",
        "        sqrt_one_minus_at = torch.full((b, 1, 1, 1), sqrt_one_minus_alphas[index],device=device)\n",
        "\n",
        "        if unconditional_conditioning is None or unconditional_guidance_scale == 1.:\n",
        "\n",
        "            e_t = self.model.apply_model(x, t, c)\n",
        "            quant_e_t = self.quant_model.apply_model(x, t, c)\n",
        "\n",
        "        else: ## run here\n",
        "            x_in = torch.cat([x] * 2).detach()\n",
        "            t_in = torch.cat([t] * 2).detach()\n",
        "            c_in = torch.cat([unconditional_conditioning, c]).detach()\n",
        "\n",
        "            e_t_uncond, e_t = self.model.apply_model(x_in, t_in, c_in).chunk(2)\n",
        "            e_t = e_t_uncond + unconditional_guidance_scale * (e_t - e_t_uncond)\n",
        "\n",
        "            quant_e_t_uncond, quant_e_t = self.quant_model.apply_model(x_in, t_in, c_in).chunk(2)\n",
        "            quant_e_t = quant_e_t_uncond + unconditional_guidance_scale * (quant_e_t - quant_e_t_uncond)\n",
        "\n",
        "\n",
        "        loss = F.mse_loss(quant_e_t, e_t, size_average=False)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.lr_scheduler.step()\n",
        "\n",
        "        # globalvar.saveStep(index, self.optimizer.state_dict())\n",
        "\n",
        "        if score_corrector is not None: ## do not run\n",
        "            assert self.model.parameterization == \"eps\"\n",
        "            e_t = score_corrector.modify_score(self.model, e_t, x, t, c, **corrector_kwargs)\n",
        "\n",
        "        # current prediction for x_0\n",
        "        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n",
        "        if quantize_denoised: ## do not run\n",
        "            pred_x0, _, *_ = self.model.first_stage_model.quantize(pred_x0)\n",
        "        # direction pointing to x_t\n",
        "        dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t\n",
        "        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature\n",
        "        if noise_dropout > 0.:\n",
        "            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n",
        "        x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise\n",
        "        return x_prev, pred_x0\n"
      ],
      "metadata": {
        "id": "lXPvwDO_2_R2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch-lightning[extra]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UJV6OqE4UZI",
        "outputId": "f619c26e-0092-44d5-8359-bc79623705f6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning[extra] in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (2.5.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (4.66.6)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (2024.10.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (1.5.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (0.11.8)\n",
            "Requirement already satisfied: matplotlib>3.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (3.8.0)\n",
            "Requirement already satisfied: omegaconf>=2.2.3 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (2.3.0)\n",
            "Collecting hydra-core>=1.2.0 (from pytorch-lightning[extra])\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting jsonargparse>=4.27.7 (from jsonargparse[signatures]>=4.27.7; extra == \"extra\"->pytorch-lightning[extra])\n",
            "  Downloading jsonargparse-4.34.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning[extra]) (13.9.3)\n",
            "Collecting tensorboardX>=2.2 (from pytorch-lightning[extra])\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting bitsandbytes>=0.42.0 (from pytorch-lightning[extra])\n",
            "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes>=0.42.0->pytorch-lightning[extra]) (1.26.4)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (3.10.10)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.2.0->pytorch-lightning[extra]) (4.9.3)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.10/dist-packages (from jsonargparse[signatures]>=4.27.7; extra == \"extra\"->pytorch-lightning[extra]) (0.16)\n",
            "Collecting typeshed-client>=2.1.0 (from jsonargparse[signatures]>=4.27.7; extra == \"extra\"->pytorch-lightning[extra])\n",
            "  Downloading typeshed_client-2.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning[extra]) (75.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>3.1->pytorch-lightning[extra]) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>3.1->pytorch-lightning[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>3.1->pytorch-lightning[extra]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>3.1->pytorch-lightning[extra]) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>3.1->pytorch-lightning[extra]) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>3.1->pytorch-lightning[extra]) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>3.1->pytorch-lightning[extra]) (2.8.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.3.0->pytorch-lightning[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.3.0->pytorch-lightning[extra]) (2.18.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.2->pytorch-lightning[extra]) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning[extra]) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning[extra]) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->pytorch-lightning[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning[extra]) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (4.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->pytorch-lightning[extra]) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>3.1->pytorch-lightning[extra]) (1.16.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from typeshed-client>=2.1.0->jsonargparse[signatures]>=4.27.7; extra == \"extra\"->pytorch-lightning[extra]) (6.4.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning[extra]) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (3.10)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning[extra]) (0.2.0)\n",
            "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-4.34.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.6/210.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typeshed_client-2.7.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: typeshed-client, tensorboardX, jsonargparse, hydra-core, bitsandbytes\n",
            "Successfully installed bitsandbytes-0.44.1 hydra-core-1.3.2 jsonargparse-4.34.0 tensorboardX-2.6.2.2 typeshed-client-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/CompVis/taming-transformers.git\n"
      ],
      "metadata": {
        "id": "cX75j-VY4vpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "283caf18-f851-41af-aab7-c15fd35e2210"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/CompVis/taming-transformers.git\n",
            "  Cloning https://github.com/CompVis/taming-transformers.git to /tmp/pip-req-build-k2dngh23\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/CompVis/taming-transformers.git /tmp/pip-req-build-k2dngh23\n",
            "  Resolved https://github.com/CompVis/taming-transformers.git to commit 3ba01b241669f5ade541ce990f7650a3b8f65318\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from taming-transformers==0.0.1) (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from taming-transformers==0.0.1) (1.26.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from taming-transformers==0.0.1) (4.66.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->taming-transformers==0.0.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->taming-transformers==0.0.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->taming-transformers==0.0.1) (3.0.2)\n",
            "Building wheels for collected packages: taming-transformers\n",
            "  Building wheel for taming-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for taming-transformers: filename=taming_transformers-0.0.1-py3-none-any.whl size=1116 sha256=d757297bd98b5326be5afb11dbf47a43f9d624199de690fc531b1ad76ec48aeb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-afw9sqwu/wheels/a1/8f/41/1936f3b5b64d5575dc9a39c6c7af32130e417181473e9d04f4\n",
            "Successfully built taming-transformers\n",
            "Installing collected packages: taming-transformers\n",
            "Successfully installed taming-transformers-0.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y taming-transformers\n",
        "!git clone https://github.com/CompVis/taming-transformers.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekmC-6sS5wwS",
        "outputId": "412c1584-da24-43f9-beb2-e48da15a175c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: taming-transformers 0.0.1\n",
            "Uninstalling taming-transformers-0.0.1:\n",
            "  Successfully uninstalled taming-transformers-0.0.1\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1342, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 1342 (delta 0), reused 1 (delta 0), pack-reused 1340 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1342/1342), 409.77 MiB | 22.98 MiB/s, done.\n",
            "Resolving deltas: 100% (282/282), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show taming-transformers\n"
      ],
      "metadata": {
        "id": "Vvt45haF6NP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from torch import einsum\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "class VectorQuantizer(nn.Module):\n",
        "    \"\"\"\n",
        "    see https://github.com/MishaLaskin/vqvae/blob/d761a999e2267766400dc646d82d3ac3657771d4/models/quantizer.py\n",
        "    ____________________________________________\n",
        "    Discretization bottleneck part of the VQ-VAE.\n",
        "    Inputs:\n",
        "    - n_e : number of embeddings\n",
        "    - e_dim : dimension of embedding\n",
        "    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n",
        "    _____________________________________________\n",
        "    \"\"\"\n",
        "\n",
        "    # NOTE: this class contains a bug regarding beta; see VectorQuantizer2 for\n",
        "    # a fix and use legacy=False to apply that fix. VectorQuantizer2 can be\n",
        "    # used wherever VectorQuantizer has been used before and is additionally\n",
        "    # more efficient.\n",
        "    def __init__(self, n_e, e_dim, beta):\n",
        "        super(VectorQuantizer, self).__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "    def forward(self, z):\n",
        "        \"\"\"\n",
        "        Inputs the output of the encoder network z and maps it to a discrete\n",
        "        one-hot vector that is the index of the closest embedding vector e_j\n",
        "        z (continuous) -> z_q (discrete)\n",
        "        z.shape = (batch, channel, height, width)\n",
        "        quantization pipeline:\n",
        "            1. get encoder input (B,C,H,W)\n",
        "            2. flatten input to (B*H*W,C)\n",
        "        \"\"\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = z.permute(0, 2, 3, 1).contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.matmul(z_flattened, self.embedding.weight.t())\n",
        "\n",
        "        ## could possible replace this here\n",
        "        # #\\start...\n",
        "        # find closest encodings\n",
        "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1)\n",
        "\n",
        "        min_encodings = torch.zeros(\n",
        "            min_encoding_indices.shape[0], self.n_e).to(z)\n",
        "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
        "\n",
        "        # dtype min encodings: torch.float32\n",
        "        # min_encodings shape: torch.Size([2048, 512])\n",
        "        # min_encoding_indices.shape: torch.Size([2048, 1])\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
        "        #.........\\end\n",
        "\n",
        "        # with:\n",
        "        # .........\\start\n",
        "        #min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        #z_q = self.embedding(min_encoding_indices)\n",
        "        # ......\\end......... (TODO)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
        "            torch.mean((z_q - z.detach()) ** 2)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # perplexity\n",
        "        e_mean = torch.mean(min_encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        # TODO: check for more easy handling with nn.Embedding\n",
        "        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n",
        "        min_encodings.scatter_(1, indices[:,None], 1)\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n",
        "\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class GumbelQuantize(nn.Module):\n",
        "    \"\"\"\n",
        "    credit to @karpathy: https://github.com/karpathy/deep-vector-quantization/blob/main/model.py (thanks!)\n",
        "    Gumbel Softmax trick quantizer\n",
        "    Categorical Reparameterization with Gumbel-Softmax, Jang et al. 2016\n",
        "    https://arxiv.org/abs/1611.01144\n",
        "    \"\"\"\n",
        "    def __init__(self, num_hiddens, embedding_dim, n_embed, straight_through=True,\n",
        "                 kl_weight=5e-4, temp_init=1.0, use_vqinterface=True,\n",
        "                 remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n_embed = n_embed\n",
        "\n",
        "        self.straight_through = straight_through\n",
        "        self.temperature = temp_init\n",
        "        self.kl_weight = kl_weight\n",
        "\n",
        "        self.proj = nn.Conv2d(num_hiddens, n_embed, 1)\n",
        "        self.embed = nn.Embedding(n_embed, embedding_dim)\n",
        "\n",
        "        self.use_vqinterface = use_vqinterface\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, return_logits=False):\n",
        "        # force hard = True when we are in eval mode, as we must quantize. actually, always true seems to work\n",
        "        hard = self.straight_through if self.training else True\n",
        "        temp = self.temperature if temp is None else temp\n",
        "\n",
        "        logits = self.proj(z)\n",
        "        if self.remap is not None:\n",
        "            # continue only with used logits\n",
        "            full_zeros = torch.zeros_like(logits)\n",
        "            logits = logits[:,self.used,...]\n",
        "\n",
        "        soft_one_hot = F.gumbel_softmax(logits, tau=temp, dim=1, hard=hard)\n",
        "        if self.remap is not None:\n",
        "            # go back to all entries but unused set to zero\n",
        "            full_zeros[:,self.used,...] = soft_one_hot\n",
        "            soft_one_hot = full_zeros\n",
        "        z_q = einsum('b n h w, n d -> b d h w', soft_one_hot, self.embed.weight)\n",
        "\n",
        "        # + kl divergence to the prior loss\n",
        "        qy = F.softmax(logits, dim=1)\n",
        "        diff = self.kl_weight * torch.sum(qy * torch.log(qy * self.n_embed + 1e-10), dim=1).mean()\n",
        "\n",
        "        ind = soft_one_hot.argmax(dim=1)\n",
        "        if self.remap is not None:\n",
        "            ind = self.remap_to_used(ind)\n",
        "        if self.use_vqinterface:\n",
        "            if return_logits:\n",
        "                return z_q, diff, (None, None, ind), logits\n",
        "            return z_q, diff, (None, None, ind)\n",
        "        return z_q, diff, ind\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        b, h, w, c = shape\n",
        "        assert b*h*w == indices.shape[0]\n",
        "        indices = rearrange(indices, '(b h w) -> b h w', b=b, h=h, w=w)\n",
        "        if self.remap is not None:\n",
        "            indices = self.unmap_to_all(indices)\n",
        "        one_hot = F.one_hot(indices, num_classes=self.n_embed).permute(0, 3, 1, 2).float()\n",
        "        z_q = einsum('b n h w, n d -> b d h w', one_hot, self.embed.weight)\n",
        "        return z_q\n",
        "\n",
        "\n",
        "class VectorQuantizer2(nn.Module):\n",
        "    \"\"\"\n",
        "    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly\n",
        "    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n",
        "    \"\"\"\n",
        "    # NOTE: due to a bug the beta term was applied to the wrong term. for\n",
        "    # backwards compatibility we use the buggy version by default, but you can\n",
        "    # specify legacy=False to fix it.\n",
        "    def __init__(self, n_e, e_dim, beta, remap=None, unknown_index=\"random\",\n",
        "                 sane_index_shape=False, legacy=True):\n",
        "        super().__init__()\n",
        "        self.n_e = n_e\n",
        "        self.e_dim = e_dim\n",
        "        self.beta = beta\n",
        "        self.legacy = legacy\n",
        "\n",
        "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
        "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_e\n",
        "\n",
        "        self.sane_index_shape = sane_index_shape\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n",
        "        assert temp is None or temp==1.0, \"Only for interface compatible with Gumbel\"\n",
        "        assert rescale_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        assert return_logits==False, \"Only for interface compatible with Gumbel\"\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n",
        "        z_flattened = z.view(-1, self.e_dim)\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "\n",
        "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n",
        "            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n",
        "            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n",
        "\n",
        "        min_encoding_indices = torch.argmin(d, dim=1)\n",
        "        z_q = self.embedding(min_encoding_indices).view(z.shape)\n",
        "        perplexity = None\n",
        "        min_encodings = None\n",
        "\n",
        "        # compute loss for embedding\n",
        "        if not self.legacy:\n",
        "            loss = self.beta * torch.mean((z_q.detach()-z)**2) + \\\n",
        "                   torch.mean((z_q - z.detach()) ** 2)\n",
        "        else:\n",
        "            loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n",
        "                   torch.mean((z_q - z.detach()) ** 2)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n",
        "\n",
        "        if self.remap is not None:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(z.shape[0],-1) # add batch axis\n",
        "            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n",
        "            min_encoding_indices = min_encoding_indices.reshape(-1,1) # flatten\n",
        "\n",
        "        if self.sane_index_shape:\n",
        "            min_encoding_indices = min_encoding_indices.reshape(\n",
        "                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n",
        "\n",
        "        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n",
        "\n",
        "    def get_codebook_entry(self, indices, shape):\n",
        "        # shape specifying (batch, height, width, channel)\n",
        "        if self.remap is not None:\n",
        "            indices = indices.reshape(shape[0],-1) # add batch axis\n",
        "            indices = self.unmap_to_all(indices)\n",
        "            indices = indices.reshape(-1) # flatten again\n",
        "\n",
        "        # get quantized latent vectors\n",
        "        z_q = self.embedding(indices)\n",
        "\n",
        "        if shape is not None:\n",
        "            z_q = z_q.view(shape)\n",
        "            # reshape back to match original input shape\n",
        "            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        return z_q\n",
        "\n",
        "class EmbeddingEMA(nn.Module):\n",
        "    def __init__(self, num_tokens, codebook_dim, decay=0.99, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.decay = decay\n",
        "        self.eps = eps\n",
        "        weight = torch.randn(num_tokens, codebook_dim)\n",
        "        self.weight = nn.Parameter(weight, requires_grad = False)\n",
        "        self.cluster_size = nn.Parameter(torch.zeros(num_tokens), requires_grad = False)\n",
        "        self.embed_avg = nn.Parameter(weight.clone(), requires_grad = False)\n",
        "        self.update = True\n",
        "\n",
        "    def forward(self, embed_id):\n",
        "        return F.embedding(embed_id, self.weight)\n",
        "\n",
        "    def cluster_size_ema_update(self, new_cluster_size):\n",
        "        self.cluster_size.data.mul_(self.decay).add_(new_cluster_size, alpha=1 - self.decay)\n",
        "\n",
        "    def embed_avg_ema_update(self, new_embed_avg):\n",
        "        self.embed_avg.data.mul_(self.decay).add_(new_embed_avg, alpha=1 - self.decay)\n",
        "\n",
        "    def weight_update(self, num_tokens):\n",
        "        n = self.cluster_size.sum()\n",
        "        smoothed_cluster_size = (\n",
        "                (self.cluster_size + self.eps) / (n + num_tokens * self.eps) * n\n",
        "            )\n",
        "        #normalize embedding average with smoothed cluster size\n",
        "        embed_normalized = self.embed_avg / smoothed_cluster_size.unsqueeze(1)\n",
        "        self.weight.data.copy_(embed_normalized)\n",
        "\n",
        "\n",
        "class EMAVectorQuantizer(nn.Module):\n",
        "    def __init__(self, n_embed, embedding_dim, beta, decay=0.99, eps=1e-5,\n",
        "                remap=None, unknown_index=\"random\"):\n",
        "        super().__init__()\n",
        "        self.codebook_dim = codebook_dim\n",
        "        self.num_tokens = num_tokens\n",
        "        self.beta = beta\n",
        "        self.embedding = EmbeddingEMA(self.num_tokens, self.codebook_dim, decay, eps)\n",
        "\n",
        "        self.remap = remap\n",
        "        if self.remap is not None:\n",
        "            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n",
        "            self.re_embed = self.used.shape[0]\n",
        "            self.unknown_index = unknown_index # \"random\" or \"extra\" or integer\n",
        "            if self.unknown_index == \"extra\":\n",
        "                self.unknown_index = self.re_embed\n",
        "                self.re_embed = self.re_embed+1\n",
        "            print(f\"Remapping {self.n_embed} indices to {self.re_embed} indices. \"\n",
        "                  f\"Using {self.unknown_index} for unknown indices.\")\n",
        "        else:\n",
        "            self.re_embed = n_embed\n",
        "\n",
        "    def remap_to_used(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        match = (inds[:,:,None]==used[None,None,...]).long()\n",
        "        new = match.argmax(-1)\n",
        "        unknown = match.sum(2)<1\n",
        "        if self.unknown_index == \"random\":\n",
        "            new[unknown]=torch.randint(0,self.re_embed,size=new[unknown].shape).to(device=new.device)\n",
        "        else:\n",
        "            new[unknown] = self.unknown_index\n",
        "        return new.reshape(ishape)\n",
        "\n",
        "    def unmap_to_all(self, inds):\n",
        "        ishape = inds.shape\n",
        "        assert len(ishape)>1\n",
        "        inds = inds.reshape(ishape[0],-1)\n",
        "        used = self.used.to(inds)\n",
        "        if self.re_embed > self.used.shape[0]: # extra token\n",
        "            inds[inds>=self.used.shape[0]] = 0 # simply set to zero\n",
        "        back=torch.gather(used[None,:][inds.shape[0]*[0],:], 1, inds)\n",
        "        return back.reshape(ishape)\n",
        "\n",
        "    def forward(self, z):\n",
        "        # reshape z -> (batch, height, width, channel) and flatten\n",
        "        #z, 'b c h w -> b h w c'\n",
        "        z = rearrange(z, 'b c h w -> b h w c')\n",
        "        z_flattened = z.reshape(-1, self.codebook_dim)\n",
        "\n",
        "        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n",
        "        d = z_flattened.pow(2).sum(dim=1, keepdim=True) + \\\n",
        "            self.embedding.weight.pow(2).sum(dim=1) - 2 * \\\n",
        "            torch.einsum('bd,nd->bn', z_flattened, self.embedding.weight) # 'n d -> d n'\n",
        "\n",
        "\n",
        "        encoding_indices = torch.argmin(d, dim=1)\n",
        "\n",
        "        z_q = self.embedding(encoding_indices).view(z.shape)\n",
        "        encodings = F.one_hot(encoding_indices, self.num_tokens).type(z.dtype)\n",
        "        avg_probs = torch.mean(encodings, dim=0)\n",
        "        perplexity = torch.exp(-torch.sum(avg_probs * torch.log(avg_probs + 1e-10)))\n",
        "\n",
        "        if self.training and self.embedding.update:\n",
        "            #EMA cluster size\n",
        "            encodings_sum = encodings.sum(0)\n",
        "            self.embedding.cluster_size_ema_update(encodings_sum)\n",
        "            #EMA embedding average\n",
        "            embed_sum = encodings.transpose(0,1) @ z_flattened\n",
        "            self.embedding.embed_avg_ema_update(embed_sum)\n",
        "            #normalize embed_avg and update weight\n",
        "            self.embedding.weight_update(self.num_tokens)\n",
        "\n",
        "        # compute loss for embedding\n",
        "        loss = self.beta * F.mse_loss(z_q.detach(), z)\n",
        "\n",
        "        # preserve gradients\n",
        "        z_q = z + (z_q - z).detach()\n",
        "\n",
        "        # reshape back to match original input shape\n",
        "        #z_q, 'b h w c -> b c h w'\n",
        "        z_q = rearrange(z_q, 'b h w c -> b c h w')\n",
        "        return z_q, loss, (perplexity, encodings, encoding_indices)\n"
      ],
      "metadata": {
        "id": "YjTSVnlJ6v7O"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class AbstractDistribution:\n",
        "    def sample(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def mode(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "\n",
        "class DiracDistribution(AbstractDistribution):\n",
        "    def __init__(self, value):\n",
        "        self.value = value\n",
        "\n",
        "    def sample(self):\n",
        "        return self.value\n",
        "\n",
        "    def mode(self):\n",
        "        return self.value\n",
        "\n",
        "\n",
        "class DiagonalGaussianDistribution(object):\n",
        "    def __init__(self, parameters, deterministic=False):\n",
        "        self.parameters = parameters\n",
        "        self.mean, self.logvar = torch.chunk(parameters, 2, dim=1)\n",
        "        self.logvar = torch.clamp(self.logvar, -30.0, 20.0)\n",
        "        self.deterministic = deterministic\n",
        "        self.std = torch.exp(0.5 * self.logvar)\n",
        "        self.var = torch.exp(self.logvar)\n",
        "        if self.deterministic:\n",
        "            self.var = self.std = torch.zeros_like(self.mean).to(device=self.parameters.device)\n",
        "\n",
        "    def sample(self):\n",
        "        x = self.mean + self.std * torch.randn(self.mean.shape).to(device=self.parameters.device)\n",
        "        return x\n",
        "\n",
        "    def kl(self, other=None):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.])\n",
        "        else:\n",
        "            if other is None:\n",
        "                return 0.5 * torch.sum(torch.pow(self.mean, 2)\n",
        "                                       + self.var - 1.0 - self.logvar,\n",
        "                                       dim=[1, 2, 3])\n",
        "            else:\n",
        "                return 0.5 * torch.sum(\n",
        "                    torch.pow(self.mean - other.mean, 2) / other.var\n",
        "                    + self.var / other.var - 1.0 - self.logvar + other.logvar,\n",
        "                    dim=[1, 2, 3])\n",
        "\n",
        "    def nll(self, sample, dims=[1,2,3]):\n",
        "        if self.deterministic:\n",
        "            return torch.Tensor([0.])\n",
        "        logtwopi = np.log(2.0 * np.pi)\n",
        "        return 0.5 * torch.sum(\n",
        "            logtwopi + self.logvar + torch.pow(sample - self.mean, 2) / self.var,\n",
        "            dim=dims)\n",
        "\n",
        "    def mode(self):\n",
        "        return self.mean\n",
        "\n",
        "\n",
        "def normal_kl(mean1, logvar1, mean2, logvar2):\n",
        "    \"\"\"\n",
        "    source: https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/losses.py#L12\n",
        "    Compute the KL divergence between two gaussians.\n",
        "    Shapes are automatically broadcasted, so batches can be compared to\n",
        "    scalars, among other use cases.\n",
        "    \"\"\"\n",
        "    tensor = None\n",
        "    for obj in (mean1, logvar1, mean2, logvar2):\n",
        "        if isinstance(obj, torch.Tensor):\n",
        "            tensor = obj\n",
        "            break\n",
        "    assert tensor is not None, \"at least one argument must be a Tensor\"\n",
        "\n",
        "    # Force variances to be Tensors. Broadcasting helps convert scalars to\n",
        "    # Tensors, but it does not work for torch.exp().\n",
        "    logvar1, logvar2 = [\n",
        "        x if isinstance(x, torch.Tensor) else torch.tensor(x).to(tensor)\n",
        "        for x in (logvar1, logvar2)\n",
        "    ]\n",
        "\n",
        "    return 0.5 * (\n",
        "        -1.0\n",
        "        + logvar2\n",
        "        - logvar1\n",
        "        + torch.exp(logvar1 - logvar2)\n",
        "        + ((mean1 - mean2) ** 2) * torch.exp(-logvar2)\n",
        "    )\n"
      ],
      "metadata": {
        "id": "8iCNaA7C67pe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inspect import isfunction\n",
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, einsum\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "# from ldm.modules.diffusionmodules.util import checkpoint\n",
        "\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "\n",
        "def uniq(arr):\n",
        "    return{el: True for el in arr}.keys()\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def max_neg_value(t):\n",
        "    return -torch.finfo(t.dtype).max\n",
        "\n",
        "\n",
        "def init_(tensor):\n",
        "    dim = tensor.shape[-1]\n",
        "    std = 1 / math.sqrt(dim)\n",
        "    tensor.uniform_(-std, std)\n",
        "    return tensor\n",
        "\n",
        "\n",
        "# feedforward\n",
        "class GEGLU(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Linear(dim_in, dim_out * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, gate = self.proj(x).chunk(2, dim=-1)\n",
        "        return x * F.gelu(gate)\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, dim_out=None, mult=4, glu=False, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = int(dim * mult)\n",
        "        dim_out = default(dim_out, dim)\n",
        "        project_in = nn.Sequential(\n",
        "            nn.Linear(dim, inner_dim),\n",
        "            nn.GELU()\n",
        "        ) if not glu else GEGLU(dim, inner_dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            project_in,\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(inner_dim, dim_out)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"\n",
        "    Zero out the parameters of a module and return it.\n",
        "    \"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "\n",
        "def Normalize(in_channels):\n",
        "    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias = False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x)\n",
        "        q, k, v = rearrange(qkv, 'b (qkv heads c) h w -> qkv b heads c (h w)', heads = self.heads, qkv=3)\n",
        "        k = k.softmax(dim=-1)\n",
        "        context = torch.einsum('bhdn,bhen->bhde', k, v)\n",
        "        out = torch.einsum('bhde,bhdn->bhen', context, q)\n",
        "        out = rearrange(out, 'b heads c (h w) -> b (heads c) h w', heads=self.heads, h=h, w=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class SpatialSelfAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = rearrange(q, 'b c h w -> b (h w) c')\n",
        "        k = rearrange(k, 'b c h w -> b c (h w)')\n",
        "        w_ = torch.einsum('bij,bjk->bik', q, k)\n",
        "\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = rearrange(v, 'b c h w -> b c (h w)')\n",
        "        w_ = rearrange(w_, 'b i j -> b j i')\n",
        "        h_ = torch.einsum('bij,bjk->bik', v, w_)\n",
        "        h_ = rearrange(h_, 'b c (h w) -> b c h w', h=h)\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        context_dim = default(context_dim, query_dim)\n",
        "\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.heads = heads\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(\n",
        "            nn.Linear(inner_dim, query_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context=None, mask=None):\n",
        "        h = self.heads\n",
        "\n",
        "        q = self.to_q(x)\n",
        "        context = default(context, x)\n",
        "        k = self.to_k(context)\n",
        "        v = self.to_v(context)\n",
        "\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h=h), (q, k, v))\n",
        "\n",
        "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
        "\n",
        "        if exists(mask):\n",
        "            mask = rearrange(mask, 'b ... -> b (...)')\n",
        "            max_neg_value = -torch.finfo(sim.dtype).max\n",
        "            mask = repeat(mask, 'b j -> (b h) () j', h=h)\n",
        "            sim.masked_fill_(~mask, max_neg_value)\n",
        "\n",
        "        # attention, what we cannot get enough of\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
        "        out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class BasicTransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, n_heads, d_head, dropout=0., context_dim=None, gated_ff=True, checkpoint=False):\n",
        "        super().__init__()\n",
        "        self.attn1 = CrossAttention(query_dim=dim, heads=n_heads, dim_head=d_head, dropout=dropout)  # is a self-attention\n",
        "        self.ff = FeedForward(dim, dropout=dropout, glu=gated_ff)\n",
        "        self.attn2 = CrossAttention(query_dim=dim, context_dim=context_dim,\n",
        "                                    heads=n_heads, dim_head=d_head, dropout=dropout)  # is self-attn if context is none\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.norm3 = nn.LayerNorm(dim)\n",
        "        self.checkpoint = checkpoint\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        return checkpoint(self._forward, (x, context), self.parameters(), self.checkpoint)\n",
        "\n",
        "    def _forward(self, x, context=None):\n",
        "        x = self.attn1(self.norm1(x)) + x\n",
        "        x = self.attn2(self.norm2(x), context=context) + x\n",
        "        x = self.ff(self.norm3(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block for image-like data.\n",
        "    First, project the input (aka embedding)\n",
        "    and reshape to b, t, d.\n",
        "    Then apply standard transformer action.\n",
        "    Finally, reshape to image\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, n_heads, d_head,\n",
        "                 depth=1, dropout=0., context_dim=None):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        inner_dim = n_heads * d_head\n",
        "        self.norm = Normalize(in_channels)\n",
        "\n",
        "        self.proj_in = nn.Conv2d(in_channels,\n",
        "                                 inner_dim,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [BasicTransformerBlock(inner_dim, n_heads, d_head, dropout=dropout, context_dim=context_dim)\n",
        "                for d in range(depth)]\n",
        "        )\n",
        "\n",
        "        self.proj_out = zero_module(nn.Conv2d(inner_dim,\n",
        "                                              in_channels,\n",
        "                                              kernel_size=1,\n",
        "                                              stride=1,\n",
        "                                              padding=0))\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        # note: if no context is given, cross-attention defaults to self-attention\n",
        "        b, c, h, w = x.shape\n",
        "        x_in = x\n",
        "        x = self.norm(x)\n",
        "        x = self.proj_in(x)\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, context=context)\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=h, w=w)\n",
        "        x = self.proj_out(x)\n",
        "        return x + x_in"
      ],
      "metadata": {
        "id": "ER3_zFHs7Nh0"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch_diffusion + derived encoder decoder\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "# from ldm.util import instantiate_from_config\n",
        "# from ldm.modules.attention import LinearAttention\n",
        "\n",
        "\n",
        "def get_timestep_embedding(timesteps, embedding_dim):\n",
        "    \"\"\"\n",
        "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
        "    From Fairseq.\n",
        "    Build sinusoidal embeddings.\n",
        "    This matches the implementation in tensor2tensor, but differs slightly\n",
        "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
        "    \"\"\"\n",
        "    assert len(timesteps.shape) == 1\n",
        "\n",
        "    half_dim = embedding_dim // 2\n",
        "    emb = math.log(10000) / (half_dim - 1)\n",
        "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
        "    emb = emb.to(device=timesteps.device)\n",
        "    emb = timesteps.float()[:, None] * emb[None, :]\n",
        "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
        "    if embedding_dim % 2 == 1:  # zero pad\n",
        "        emb = torch.nn.functional.pad(emb, (0,1,0,0))\n",
        "    return emb\n",
        "\n",
        "\n",
        "def nonlinearity(x):\n",
        "    # swish\n",
        "    return x*torch.sigmoid(x)\n",
        "\n",
        "\n",
        "def Normalize(in_channels, num_groups=32):\n",
        "    return torch.nn.GroupNorm(num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode=\"nearest\")\n",
        "        if self.with_conv:\n",
        "            x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_channels, with_conv):\n",
        "        super().__init__()\n",
        "        self.with_conv = with_conv\n",
        "        if self.with_conv:\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=2,\n",
        "                                        padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_conv:\n",
        "            pad = (0,1,0,1)\n",
        "            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n",
        "            x = self.conv(x)\n",
        "        else:\n",
        "            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False,\n",
        "                 dropout, temb_channels=512):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        out_channels = in_channels if out_channels is None else out_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.use_conv_shortcut = conv_shortcut\n",
        "\n",
        "        self.norm1 = Normalize(in_channels)\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels,\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "        if temb_channels > 0:\n",
        "            self.temb_proj = torch.nn.Linear(temb_channels,\n",
        "                                             out_channels)\n",
        "        self.norm2 = Normalize(out_channels)\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        self.conv2 = torch.nn.Conv2d(out_channels,\n",
        "                                     out_channels,\n",
        "                                     kernel_size=3,\n",
        "                                     stride=1,\n",
        "                                     padding=1)\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                self.conv_shortcut = torch.nn.Conv2d(in_channels,\n",
        "                                                     out_channels,\n",
        "                                                     kernel_size=3,\n",
        "                                                     stride=1,\n",
        "                                                     padding=1)\n",
        "            else:\n",
        "                self.nin_shortcut = torch.nn.Conv2d(in_channels,\n",
        "                                                    out_channels,\n",
        "                                                    kernel_size=1,\n",
        "                                                    stride=1,\n",
        "                                                    padding=0)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = x\n",
        "        h = self.norm1(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv1(h)\n",
        "\n",
        "        if temb is not None:\n",
        "            h = h + self.temb_proj(nonlinearity(temb))[:,:,None,None]\n",
        "\n",
        "        h = self.norm2(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.dropout(h)\n",
        "        h = self.conv2(h)\n",
        "\n",
        "        if self.in_channels != self.out_channels:\n",
        "            if self.use_conv_shortcut:\n",
        "                x = self.conv_shortcut(x)\n",
        "            else:\n",
        "                x = self.nin_shortcut(x)\n",
        "\n",
        "        return x+h\n",
        "\n",
        "\n",
        "class LinAttnBlock(LinearAttention):\n",
        "    \"\"\"to match AttnBlock usage\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__(dim=in_channels, heads=1, dim_head=in_channels)\n",
        "\n",
        "\n",
        "class AttnBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.norm = Normalize(in_channels)\n",
        "        self.q = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.k = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.v = torch.nn.Conv2d(in_channels,\n",
        "                                 in_channels,\n",
        "                                 kernel_size=1,\n",
        "                                 stride=1,\n",
        "                                 padding=0)\n",
        "        self.proj_out = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=1,\n",
        "                                        stride=1,\n",
        "                                        padding=0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        h_ = x\n",
        "        h_ = self.norm(h_)\n",
        "        q = self.q(h_)\n",
        "        k = self.k(h_)\n",
        "        v = self.v(h_)\n",
        "\n",
        "        # compute attention\n",
        "        b,c,h,w = q.shape\n",
        "        q = q.reshape(b,c,h*w)\n",
        "        q = q.permute(0,2,1)   # b,hw,c\n",
        "        k = k.reshape(b,c,h*w) # b,c,hw\n",
        "        w_ = torch.bmm(q,k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n",
        "        w_ = w_ * (int(c)**(-0.5))\n",
        "        w_ = torch.nn.functional.softmax(w_, dim=2)\n",
        "\n",
        "        # attend to values\n",
        "        v = v.reshape(b,c,h*w)\n",
        "        w_ = w_.permute(0,2,1)   # b,hw,hw (first hw of k, second of q)\n",
        "        h_ = torch.bmm(v,w_)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n",
        "        h_ = h_.reshape(b,c,h,w)\n",
        "\n",
        "        h_ = self.proj_out(h_)\n",
        "\n",
        "        return x+h_\n",
        "\n",
        "\n",
        "def make_attn(in_channels, attn_type=\"vanilla\"):\n",
        "    assert attn_type in [\"vanilla\", \"linear\", \"none\"], f'attn_type {attn_type} unknown'\n",
        "    print(f\"making attention of type '{attn_type}' with {in_channels} in_channels\")\n",
        "    if attn_type == \"vanilla\":\n",
        "        return AttnBlock(in_channels)\n",
        "    elif attn_type == \"none\":\n",
        "        return nn.Identity(in_channels)\n",
        "    else:\n",
        "        return LinAttnBlock(in_channels)\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, use_timestep=True, use_linear_attn=False, attn_type=\"vanilla\"):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = self.ch*4\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        self.use_timestep = use_timestep\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            self.temb = nn.Module()\n",
        "            self.temb.dense = nn.ModuleList([\n",
        "                torch.nn.Linear(self.ch,\n",
        "                                self.temb_ch),\n",
        "                torch.nn.Linear(self.temb_ch,\n",
        "                                self.temb_ch),\n",
        "            ])\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            skip_in = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                if i_block == self.num_res_blocks:\n",
        "                    skip_in = ch*in_ch_mult[i_level]\n",
        "                block.append(ResnetBlock(in_channels=block_in+skip_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in,\n",
        "                                        out_ch,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x, t=None, context=None):\n",
        "        #assert x.shape[2] == x.shape[3] == self.resolution\n",
        "        if context is not None:\n",
        "            # assume aligned context, cat along channel axis\n",
        "            x = torch.cat((x, context), dim=1)\n",
        "        if self.use_timestep:\n",
        "            # timestep embedding\n",
        "            assert t is not None\n",
        "            temb = get_timestep_embedding(t, self.ch)\n",
        "            temb = self.temb.dense[0](temb)\n",
        "            temb = nonlinearity(temb)\n",
        "            temb = self.temb.dense[1](temb)\n",
        "        else:\n",
        "            temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](\n",
        "                    torch.cat([h, hs.pop()], dim=1), temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.conv_out.weight\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, double_z=True, use_linear_attn=False, attn_type=\"vanilla\",\n",
        "                 **ignore_kwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "\n",
        "        # downsampling\n",
        "        self.conv_in = torch.nn.Conv2d(in_channels,\n",
        "                                       self.ch,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        curr_res = resolution\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        self.in_ch_mult = in_ch_mult\n",
        "        self.down = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_in = ch*in_ch_mult[i_level]\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            down = nn.Module()\n",
        "            down.block = block\n",
        "            down.attn = attn\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                down.downsample = Downsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res // 2\n",
        "            self.down.append(down)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in,\n",
        "                                        2*z_channels if double_z else z_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # downsampling\n",
        "        hs = [self.conv_in(x)]\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            for i_block in range(self.num_res_blocks):\n",
        "                h = self.down[i_level].block[i_block](hs[-1], temb)\n",
        "                if len(self.down[i_level].attn) > 0:\n",
        "                    h = self.down[i_level].attn[i_block](h)\n",
        "                hs.append(h)\n",
        "            if i_level != self.num_resolutions-1:\n",
        "                hs.append(self.down[i_level].downsample(hs[-1]))\n",
        "\n",
        "        # middle\n",
        "        h = hs[-1]\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # end\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, *, ch, out_ch, ch_mult=(1,2,4,8), num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True, in_channels,\n",
        "                 resolution, z_channels, give_pre_end=False, tanh_out=False, use_linear_attn=False,\n",
        "                 attn_type=\"vanilla\", **ignorekwargs):\n",
        "        super().__init__()\n",
        "        if use_linear_attn: attn_type = \"linear\"\n",
        "        self.ch = ch\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        self.resolution = resolution\n",
        "        self.in_channels = in_channels\n",
        "        self.give_pre_end = give_pre_end\n",
        "        self.tanh_out = tanh_out\n",
        "\n",
        "        # compute in_ch_mult, block_in and curr_res at lowest res\n",
        "        in_ch_mult = (1,)+tuple(ch_mult)\n",
        "        block_in = ch*ch_mult[self.num_resolutions-1]\n",
        "        curr_res = resolution // 2**(self.num_resolutions-1)\n",
        "        self.z_shape = (1,z_channels,curr_res,curr_res)\n",
        "        print(\"Working with z of shape {} = {} dimensions.\".format(\n",
        "            self.z_shape, np.prod(self.z_shape)))\n",
        "\n",
        "        # z to block_in\n",
        "        self.conv_in = torch.nn.Conv2d(z_channels,\n",
        "                                       block_in,\n",
        "                                       kernel_size=3,\n",
        "                                       stride=1,\n",
        "                                       padding=1)\n",
        "\n",
        "        # middle\n",
        "        self.mid = nn.Module()\n",
        "        self.mid.block_1 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n",
        "        self.mid.block_2 = ResnetBlock(in_channels=block_in,\n",
        "                                       out_channels=block_in,\n",
        "                                       temb_channels=self.temb_ch,\n",
        "                                       dropout=dropout)\n",
        "\n",
        "        # upsampling\n",
        "        self.up = nn.ModuleList()\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            block = nn.ModuleList()\n",
        "            attn = nn.ModuleList()\n",
        "            block_out = ch*ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "                if curr_res in attn_resolutions:\n",
        "                    attn.append(make_attn(block_in, attn_type=attn_type))\n",
        "            up = nn.Module()\n",
        "            up.block = block\n",
        "            up.attn = attn\n",
        "            if i_level != 0:\n",
        "                up.upsample = Upsample(block_in, resamp_with_conv)\n",
        "                curr_res = curr_res * 2\n",
        "            self.up.insert(0, up) # prepend to get consistent order\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in,\n",
        "                                        out_ch,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        #assert z.shape[1:] == self.z_shape[1:]\n",
        "        self.last_z_shape = z.shape\n",
        "\n",
        "        # timestep embedding\n",
        "        temb = None\n",
        "\n",
        "        # z to block_in\n",
        "        h = self.conv_in(z)\n",
        "\n",
        "        # middle\n",
        "        h = self.mid.block_1(h, temb)\n",
        "        h = self.mid.attn_1(h)\n",
        "        h = self.mid.block_2(h, temb)\n",
        "\n",
        "        # upsampling\n",
        "        for i_level in reversed(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks+1):\n",
        "                h = self.up[i_level].block[i_block](h, temb)\n",
        "                if len(self.up[i_level].attn) > 0:\n",
        "                    h = self.up[i_level].attn[i_block](h)\n",
        "            if i_level != 0:\n",
        "                h = self.up[i_level].upsample(h)\n",
        "\n",
        "        # end\n",
        "        if self.give_pre_end:\n",
        "            return h\n",
        "\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        if self.tanh_out:\n",
        "            h = torch.tanh(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class SimpleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
        "        super().__init__()\n",
        "        self.model = nn.ModuleList([nn.Conv2d(in_channels, in_channels, 1),\n",
        "                                     ResnetBlock(in_channels=in_channels,\n",
        "                                                 out_channels=2 * in_channels,\n",
        "                                                 temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=2 * in_channels,\n",
        "                                                out_channels=4 * in_channels,\n",
        "                                                temb_channels=0, dropout=0.0),\n",
        "                                     ResnetBlock(in_channels=4 * in_channels,\n",
        "                                                out_channels=2 * in_channels,\n",
        "                                                temb_channels=0, dropout=0.0),\n",
        "                                     nn.Conv2d(2*in_channels, in_channels, 1),\n",
        "                                     Upsample(in_channels, with_conv=True)])\n",
        "        # end\n",
        "        self.norm_out = Normalize(in_channels)\n",
        "        self.conv_out = torch.nn.Conv2d(in_channels,\n",
        "                                        out_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i, layer in enumerate(self.model):\n",
        "            if i in [1,2,3]:\n",
        "                x = layer(x, None)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "\n",
        "        h = self.norm_out(x)\n",
        "        h = nonlinearity(h)\n",
        "        x = self.conv_out(h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpsampleDecoder(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, ch, num_res_blocks, resolution,\n",
        "                 ch_mult=(2,2), dropout=0.0):\n",
        "        super().__init__()\n",
        "        # upsampling\n",
        "        self.temb_ch = 0\n",
        "        self.num_resolutions = len(ch_mult)\n",
        "        self.num_res_blocks = num_res_blocks\n",
        "        block_in = in_channels\n",
        "        curr_res = resolution // 2 ** (self.num_resolutions - 1)\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "        self.upsample_blocks = nn.ModuleList()\n",
        "        for i_level in range(self.num_resolutions):\n",
        "            res_block = []\n",
        "            block_out = ch * ch_mult[i_level]\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                res_block.append(ResnetBlock(in_channels=block_in,\n",
        "                                         out_channels=block_out,\n",
        "                                         temb_channels=self.temb_ch,\n",
        "                                         dropout=dropout))\n",
        "                block_in = block_out\n",
        "            self.res_blocks.append(nn.ModuleList(res_block))\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                self.upsample_blocks.append(Upsample(block_in, True))\n",
        "                curr_res = curr_res * 2\n",
        "\n",
        "        # end\n",
        "        self.norm_out = Normalize(block_in)\n",
        "        self.conv_out = torch.nn.Conv2d(block_in,\n",
        "                                        out_channels,\n",
        "                                        kernel_size=3,\n",
        "                                        stride=1,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # upsampling\n",
        "        h = x\n",
        "        for k, i_level in enumerate(range(self.num_resolutions)):\n",
        "            for i_block in range(self.num_res_blocks + 1):\n",
        "                h = self.res_blocks[i_level][i_block](h, None)\n",
        "            if i_level != self.num_resolutions - 1:\n",
        "                h = self.upsample_blocks[k](h)\n",
        "        h = self.norm_out(h)\n",
        "        h = nonlinearity(h)\n",
        "        h = self.conv_out(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class LatentRescaler(nn.Module):\n",
        "    def __init__(self, factor, in_channels, mid_channels, out_channels, depth=2):\n",
        "        super().__init__()\n",
        "        # residual block, interpolate, residual block\n",
        "        self.factor = factor\n",
        "        self.conv_in = nn.Conv2d(in_channels,\n",
        "                                 mid_channels,\n",
        "                                 kernel_size=3,\n",
        "                                 stride=1,\n",
        "                                 padding=1)\n",
        "        self.res_block1 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,\n",
        "                                                     out_channels=mid_channels,\n",
        "                                                     temb_channels=0,\n",
        "                                                     dropout=0.0) for _ in range(depth)])\n",
        "        self.attn = AttnBlock(mid_channels)\n",
        "        self.res_block2 = nn.ModuleList([ResnetBlock(in_channels=mid_channels,\n",
        "                                                     out_channels=mid_channels,\n",
        "                                                     temb_channels=0,\n",
        "                                                     dropout=0.0) for _ in range(depth)])\n",
        "\n",
        "        self.conv_out = nn.Conv2d(mid_channels,\n",
        "                                  out_channels,\n",
        "                                  kernel_size=1,\n",
        "                                  )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_in(x)\n",
        "        for block in self.res_block1:\n",
        "            x = block(x, None)\n",
        "        x = torch.nn.functional.interpolate(x, size=(int(round(x.shape[2]*self.factor)), int(round(x.shape[3]*self.factor))))\n",
        "        x = self.attn(x)\n",
        "        for block in self.res_block2:\n",
        "            x = block(x, None)\n",
        "        x = self.conv_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleEncoder(nn.Module):\n",
        "    def __init__(self, in_channels, ch, resolution, out_ch, num_res_blocks,\n",
        "                 attn_resolutions, dropout=0.0, resamp_with_conv=True,\n",
        "                 ch_mult=(1,2,4,8), rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        intermediate_chn = ch * ch_mult[-1]\n",
        "        self.encoder = Encoder(in_channels=in_channels, num_res_blocks=num_res_blocks, ch=ch, ch_mult=ch_mult,\n",
        "                               z_channels=intermediate_chn, double_z=False, resolution=resolution,\n",
        "                               attn_resolutions=attn_resolutions, dropout=dropout, resamp_with_conv=resamp_with_conv,\n",
        "                               out_ch=None)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=intermediate_chn,\n",
        "                                       mid_channels=intermediate_chn, out_channels=out_ch, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.rescaler(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MergedRescaleDecoder(nn.Module):\n",
        "    def __init__(self, z_channels, out_ch, resolution, num_res_blocks, attn_resolutions, ch, ch_mult=(1,2,4,8),\n",
        "                 dropout=0.0, resamp_with_conv=True, rescale_factor=1.0, rescale_module_depth=1):\n",
        "        super().__init__()\n",
        "        tmp_chn = z_channels*ch_mult[-1]\n",
        "        self.decoder = Decoder(out_ch=out_ch, z_channels=tmp_chn, attn_resolutions=attn_resolutions, dropout=dropout,\n",
        "                               resamp_with_conv=resamp_with_conv, in_channels=None, num_res_blocks=num_res_blocks,\n",
        "                               ch_mult=ch_mult, resolution=resolution, ch=ch)\n",
        "        self.rescaler = LatentRescaler(factor=rescale_factor, in_channels=z_channels, mid_channels=tmp_chn,\n",
        "                                       out_channels=tmp_chn, depth=rescale_module_depth)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsampler(nn.Module):\n",
        "    def __init__(self, in_size, out_size, in_channels, out_channels, ch_mult=2):\n",
        "        super().__init__()\n",
        "        assert out_size >= in_size\n",
        "        num_blocks = int(np.log2(out_size//in_size))+1\n",
        "        factor_up = 1.+ (out_size % in_size)\n",
        "        print(f\"Building {self.__class__.__name__} with in_size: {in_size} --> out_size {out_size} and factor {factor_up}\")\n",
        "        self.rescaler = LatentRescaler(factor=factor_up, in_channels=in_channels, mid_channels=2*in_channels,\n",
        "                                       out_channels=in_channels)\n",
        "        self.decoder = Decoder(out_ch=out_channels, resolution=out_size, z_channels=in_channels, num_res_blocks=2,\n",
        "                               attn_resolutions=[], in_channels=None, ch=in_channels,\n",
        "                               ch_mult=[ch_mult for _ in range(num_blocks)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rescaler(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Resize(nn.Module):\n",
        "    def __init__(self, in_channels=None, learned=False, mode=\"bilinear\"):\n",
        "        super().__init__()\n",
        "        self.with_conv = learned\n",
        "        self.mode = mode\n",
        "        if self.with_conv:\n",
        "            print(f\"Note: {self.__class__.__name} uses learned downsampling and will ignore the fixed {mode} mode\")\n",
        "            raise NotImplementedError()\n",
        "            assert in_channels is not None\n",
        "            # no asymmetric padding in torch conv, must do it ourselves\n",
        "            self.conv = torch.nn.Conv2d(in_channels,\n",
        "                                        in_channels,\n",
        "                                        kernel_size=4,\n",
        "                                        stride=2,\n",
        "                                        padding=1)\n",
        "\n",
        "    def forward(self, x, scale_factor=1.0):\n",
        "        if scale_factor==1.0:\n",
        "            return x\n",
        "        else:\n",
        "            x = torch.nn.functional.interpolate(x, mode=self.mode, align_corners=False, scale_factor=scale_factor)\n",
        "        return x\n",
        "\n",
        "class FirstStagePostProcessor(nn.Module):\n",
        "\n",
        "    def __init__(self, ch_mult:list, in_channels,\n",
        "                 pretrained_model:nn.Module=None,\n",
        "                 reshape=False,\n",
        "                 n_channels=None,\n",
        "                 dropout=0.,\n",
        "                 pretrained_config=None):\n",
        "        super().__init__()\n",
        "        if pretrained_config is None:\n",
        "            assert pretrained_model is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.pretrained_model = pretrained_model\n",
        "        else:\n",
        "            assert pretrained_config is not None, 'Either \"pretrained_model\" or \"pretrained_config\" must not be None'\n",
        "            self.instantiate_pretrained(pretrained_config)\n",
        "\n",
        "        self.do_reshape = reshape\n",
        "\n",
        "        if n_channels is None:\n",
        "            n_channels = self.pretrained_model.encoder.ch\n",
        "\n",
        "        self.proj_norm = Normalize(in_channels,num_groups=in_channels//2)\n",
        "        self.proj = nn.Conv2d(in_channels,n_channels,kernel_size=3,\n",
        "                            stride=1,padding=1)\n",
        "\n",
        "        blocks = []\n",
        "        downs = []\n",
        "        ch_in = n_channels\n",
        "        for m in ch_mult:\n",
        "            blocks.append(ResnetBlock(in_channels=ch_in,out_channels=m*n_channels,dropout=dropout))\n",
        "            ch_in = m * n_channels\n",
        "            downs.append(Downsample(ch_in, with_conv=False))\n",
        "\n",
        "        self.model = nn.ModuleList(blocks)\n",
        "        self.downsampler = nn.ModuleList(downs)\n",
        "\n",
        "\n",
        "    def instantiate_pretrained(self, config):\n",
        "        model = instantiate_from_config(config)\n",
        "        self.pretrained_model = model.eval()\n",
        "        # self.pretrained_model.train = False\n",
        "        for param in self.pretrained_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def encode_with_pretrained(self,x):\n",
        "        c = self.pretrained_model.encode(x)\n",
        "        if isinstance(c, DiagonalGaussianDistribution):\n",
        "            c = c.mode()\n",
        "        return  c\n",
        "\n",
        "    def forward(self,x):\n",
        "        z_fs = self.encode_with_pretrained(x)\n",
        "        z = self.proj_norm(z_fs)\n",
        "        z = self.proj(z)\n",
        "        z = nonlinearity(z)\n",
        "\n",
        "        for submodel, downmodel in zip(self.model,self.downsampler):\n",
        "            z = submodel(z,temb=None)\n",
        "            z = downmodel(z)\n",
        "\n",
        "        if self.do_reshape:\n",
        "            z = rearrange(z,'b c h w -> b (h w) c')\n",
        "        return z\n",
        "\n"
      ],
      "metadata": {
        "id": "DlLoO-te7Dcr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "import torch.nn.functional as F\n",
        "from contextlib import contextmanager\n",
        "\n",
        "# from taming.modules.vqvae.quantize import VectorQuantizer2 as VectorQuantizer\n",
        "\n",
        "# from ldm.modules.diffusionmodules.model import Encoder, Decoder\n",
        "# from ldm.modules.distributions.distributions import DiagonalGaussianDistribution\n",
        "\n",
        "# from ldm.util import instantiate_from_config\n",
        "\n",
        "\n",
        "class VQModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 n_embed,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 batch_resize_range=None,\n",
        "                 scheduler_config=None,\n",
        "                 lr_g_factor=1.0,\n",
        "                 remap=None,\n",
        "                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n",
        "                 use_ema=False\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.n_embed = n_embed\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n",
        "                                        remap=remap,\n",
        "                                        sane_index_shape=sane_index_shape)\n",
        "        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        self.batch_resize_range = batch_resize_range\n",
        "        if self.batch_resize_range is not None:\n",
        "            print(f\"{self.__class__.__name__}: Using per-batch resizing in range {batch_resize_range}.\")\n",
        "\n",
        "        self.use_ema = use_ema\n",
        "        if self.use_ema:\n",
        "            self.model_ema = LitEma(self)\n",
        "            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n",
        "\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "        self.scheduler_config = scheduler_config\n",
        "        self.lr_g_factor = lr_g_factor\n",
        "\n",
        "    @contextmanager\n",
        "    def ema_scope(self, context=None):\n",
        "        if self.use_ema:\n",
        "            self.model_ema.store(self.parameters())\n",
        "            self.model_ema.copy_to(self)\n",
        "            if context is not None:\n",
        "                print(f\"{context}: Switched to EMA weights\")\n",
        "        try:\n",
        "            yield None\n",
        "        finally:\n",
        "            if self.use_ema:\n",
        "                self.model_ema.restore(self.parameters())\n",
        "                if context is not None:\n",
        "                    print(f\"{context}: Restored training weights\")\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        missing, unexpected = self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n",
        "        if len(missing) > 0:\n",
        "            print(f\"Missing Keys: {missing}\")\n",
        "            print(f\"Unexpected Keys: {unexpected}\")\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        if self.use_ema:\n",
        "            self.model_ema(self)\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        quant, emb_loss, info = self.quantize(h)\n",
        "        return quant, emb_loss, info\n",
        "\n",
        "    def encode_to_prequant(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, quant):\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "    def decode_code(self, code_b):\n",
        "        quant_b = self.quantize.embed_code(code_b)\n",
        "        dec = self.decode(quant_b)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, return_pred_indices=False):\n",
        "        quant, diff, (_,_,ind) = self.encode(input)\n",
        "        dec = self.decode(quant)\n",
        "        if return_pred_indices:\n",
        "            return dec, diff, ind\n",
        "        return dec, diff\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        if self.batch_resize_range is not None:\n",
        "            lower_size = self.batch_resize_range[0]\n",
        "            upper_size = self.batch_resize_range[1]\n",
        "            if self.global_step <= 4:\n",
        "                # do the first few batches with max size to avoid later oom\n",
        "                new_resize = upper_size\n",
        "            else:\n",
        "                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n",
        "            if new_resize != x.shape[2]:\n",
        "                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n",
        "            x = x.detach()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        # https://github.com/pytorch/pytorch/issues/37142\n",
        "        # try not to fool the heuristics\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # autoencode\n",
        "            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\",\n",
        "                                            predicted_indices=ind)\n",
        "\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # discriminator\n",
        "            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        log_dict = self._validation_step(batch, batch_idx)\n",
        "        with self.ema_scope():\n",
        "            log_dict_ema = self._validation_step(batch, batch_idx, suffix=\"_ema\")\n",
        "        return log_dict\n",
        "\n",
        "    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        xrec, qloss, ind = self(x, return_pred_indices=True)\n",
        "        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n",
        "                                        self.global_step,\n",
        "                                        last_layer=self.get_last_layer(),\n",
        "                                        split=\"val\"+suffix,\n",
        "                                        predicted_indices=ind\n",
        "                                        )\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n",
        "                                            self.global_step,\n",
        "                                            last_layer=self.get_last_layer(),\n",
        "                                            split=\"val\"+suffix,\n",
        "                                            predicted_indices=ind\n",
        "                                            )\n",
        "        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        self.log(f\"val{suffix}/aeloss\", aeloss,\n",
        "                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n",
        "        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n",
        "            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr_d = self.learning_rate\n",
        "        lr_g = self.lr_g_factor*self.learning_rate\n",
        "        print(\"lr_d\", lr_d)\n",
        "        print(\"lr_g\", lr_g)\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quantize.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr_g, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
        "                                    lr=lr_d, betas=(0.5, 0.9))\n",
        "\n",
        "        if self.scheduler_config is not None:\n",
        "            scheduler = instantiate_from_config(self.scheduler_config)\n",
        "\n",
        "            print(\"Setting up LambdaLR scheduler...\")\n",
        "            scheduler = [\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "                {\n",
        "                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n",
        "                    'interval': 'step',\n",
        "                    'frequency': 1\n",
        "                },\n",
        "            ]\n",
        "            return [opt_ae, opt_disc], scheduler\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if only_inputs:\n",
        "            log[\"inputs\"] = x\n",
        "            return log\n",
        "        xrec, _ = self(x)\n",
        "        if x.shape[1] > 3:\n",
        "            # colorize with random projection\n",
        "            assert xrec.shape[1] > 3\n",
        "            x = self.to_rgb(x)\n",
        "            xrec = self.to_rgb(xrec)\n",
        "        log[\"inputs\"] = x\n",
        "        log[\"reconstructions\"] = xrec\n",
        "        if plot_ema:\n",
        "            with self.ema_scope():\n",
        "                xrec_ema, _ = self(x)\n",
        "                if x.shape[1] > 3: xrec_ema = self.to_rgb(xrec_ema)\n",
        "                log[\"reconstructions_ema\"] = xrec_ema\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n",
        "\n",
        "class VQModelInterface(VQModel):\n",
        "    def __init__(self, embed_dim, *args, **kwargs):\n",
        "        super().__init__(embed_dim=embed_dim, *args, **kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = self.quant_conv(h)\n",
        "        return h\n",
        "\n",
        "    def decode(self, h, force_not_quantize=False):\n",
        "        # also go through quantization layer\n",
        "        if not force_not_quantize:\n",
        "            quant, emb_loss, info = self.quantize(h)\n",
        "        else:\n",
        "            quant = h\n",
        "        quant = self.post_quant_conv(quant)\n",
        "        dec = self.decoder(quant)\n",
        "        return dec\n",
        "\n",
        "\n",
        "class AutoencoderKL(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 ddconfig,\n",
        "                 lossconfig,\n",
        "                 embed_dim,\n",
        "                 ckpt_path=None,\n",
        "                 ignore_keys=[],\n",
        "                 image_key=\"image\",\n",
        "                 colorize_nlabels=None,\n",
        "                 monitor=None,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.image_key = image_key\n",
        "        self.encoder = Encoder(**ddconfig)\n",
        "        self.decoder = Decoder(**ddconfig)\n",
        "        self.loss = instantiate_from_config(lossconfig)\n",
        "        assert ddconfig[\"double_z\"]\n",
        "        self.quant_conv = torch.nn.Conv2d(2*ddconfig[\"z_channels\"], 2*embed_dim, 1)\n",
        "        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n",
        "        self.embed_dim = embed_dim\n",
        "        if colorize_nlabels is not None:\n",
        "            assert type(colorize_nlabels)==int\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n",
        "        if monitor is not None:\n",
        "            self.monitor = monitor\n",
        "        if ckpt_path is not None:\n",
        "            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys)\n",
        "\n",
        "    def init_from_ckpt(self, path, ignore_keys=list()):\n",
        "        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n",
        "        keys = list(sd.keys())\n",
        "        for k in keys:\n",
        "            for ik in ignore_keys:\n",
        "                if k.startswith(ik):\n",
        "                    print(\"Deleting key {} from state_dict.\".format(k))\n",
        "                    del sd[k]\n",
        "        self.load_state_dict(sd, strict=False)\n",
        "        print(f\"Restored from {path}\")\n",
        "\n",
        "    def encode(self, x):\n",
        "        h = self.encoder(x)\n",
        "        moments = self.quant_conv(h)\n",
        "        posterior = DiagonalGaussianDistribution(moments)\n",
        "        return posterior\n",
        "\n",
        "    def decode(self, z):\n",
        "        z = self.post_quant_conv(z)\n",
        "        dec = self.decoder(z)\n",
        "        return dec\n",
        "\n",
        "    def forward(self, input, sample_posterior=True):\n",
        "        posterior = self.encode(input)\n",
        "        if sample_posterior:\n",
        "            z = posterior.sample()\n",
        "        else:\n",
        "            z = posterior.mode()\n",
        "        dec = self.decode(z)\n",
        "        return dec, posterior\n",
        "\n",
        "    def get_input(self, batch, k):\n",
        "        x = batch[k]\n",
        "        if len(x.shape) == 3:\n",
        "            x = x[..., None]\n",
        "        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # train encoder+decoder+logvar\n",
        "            aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"train\")\n",
        "            self.log(\"aeloss\", aeloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return aeloss\n",
        "\n",
        "        if optimizer_idx == 1:\n",
        "            # train the discriminator\n",
        "            discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, optimizer_idx, self.global_step,\n",
        "                                                last_layer=self.get_last_layer(), split=\"train\")\n",
        "\n",
        "            self.log(\"discloss\", discloss, prog_bar=True, logger=True, on_step=True, on_epoch=True)\n",
        "            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "            return discloss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        inputs = self.get_input(batch, self.image_key)\n",
        "        reconstructions, posterior = self(inputs)\n",
        "        aeloss, log_dict_ae = self.loss(inputs, reconstructions, posterior, 0, self.global_step,\n",
        "                                        last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        discloss, log_dict_disc = self.loss(inputs, reconstructions, posterior, 1, self.global_step,\n",
        "                                            last_layer=self.get_last_layer(), split=\"val\")\n",
        "\n",
        "        self.log(\"val/rec_loss\", log_dict_ae[\"val/rec_loss\"])\n",
        "        self.log_dict(log_dict_ae)\n",
        "        self.log_dict(log_dict_disc)\n",
        "        return self.log_dict\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        lr = self.learning_rate\n",
        "        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n",
        "                                  list(self.decoder.parameters())+\n",
        "                                  list(self.quant_conv.parameters())+\n",
        "                                  list(self.post_quant_conv.parameters()),\n",
        "                                  lr=lr, betas=(0.5, 0.9))\n",
        "        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n",
        "                                    lr=lr, betas=(0.5, 0.9))\n",
        "        return [opt_ae, opt_disc], []\n",
        "\n",
        "    def get_last_layer(self):\n",
        "        return self.decoder.conv_out.weight\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def log_images(self, batch, only_inputs=False, **kwargs):\n",
        "        log = dict()\n",
        "        x = self.get_input(batch, self.image_key)\n",
        "        x = x.to(self.device)\n",
        "        if not only_inputs:\n",
        "            xrec, posterior = self(x)\n",
        "            if x.shape[1] > 3:\n",
        "                # colorize with random projection\n",
        "                assert xrec.shape[1] > 3\n",
        "                x = self.to_rgb(x)\n",
        "                xrec = self.to_rgb(xrec)\n",
        "            log[\"samples\"] = self.decode(torch.randn_like(posterior.sample()))\n",
        "            log[\"reconstructions\"] = xrec\n",
        "        log[\"inputs\"] = x\n",
        "        return log\n",
        "\n",
        "    def to_rgb(self, x):\n",
        "        assert self.image_key == \"segmentation\"\n",
        "        if not hasattr(self, \"colorize\"):\n",
        "            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n",
        "        x = F.conv2d(x, weight=self.colorize)\n",
        "        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n",
        "        return x\n",
        "\n",
        "\n",
        "class IdentityFirstStage(torch.nn.Module):\n",
        "    def __init__(self, *args, vq_interface=False, **kwargs):\n",
        "        self.vq_interface = vq_interface  # TODO: Should be true by default but check to not break older stuff\n",
        "        super().__init__()\n",
        "\n",
        "    def encode(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "    def quantize(self, x, *args, **kwargs):\n",
        "        if self.vq_interface:\n",
        "            return x, None, [None, None, None]\n",
        "        return x\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "W6DFWKub5AvZ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import abc\n",
        "from einops import rearrange\n",
        "from functools import partial\n",
        "\n",
        "import multiprocessing as mp\n",
        "from threading import Thread\n",
        "from queue import Queue\n",
        "\n",
        "from inspect import isfunction\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "\n",
        "def log_txt_as_img(wh, xc, size=10):\n",
        "    # wh a tuple of (width, height)\n",
        "    # xc a list of captions to plot\n",
        "    b = len(xc)\n",
        "    txts = list()\n",
        "    for bi in range(b):\n",
        "        txt = Image.new(\"RGB\", wh, color=\"white\")\n",
        "        draw = ImageDraw.Draw(txt)\n",
        "        font = ImageFont.truetype('data/DejaVuSans.ttf', size=size)\n",
        "        nc = int(40 * (wh[0] / 256))\n",
        "        lines = \"\\n\".join(xc[bi][start:start + nc] for start in range(0, len(xc[bi]), nc))\n",
        "\n",
        "        try:\n",
        "            draw.text((0, 0), lines, fill=\"black\", font=font)\n",
        "        except UnicodeEncodeError:\n",
        "            print(\"Cant encode string for logging. Skipping.\")\n",
        "\n",
        "        txt = np.array(txt).transpose(2, 0, 1) / 127.5 - 1.0\n",
        "        txts.append(txt)\n",
        "    txts = np.stack(txts)\n",
        "    txts = torch.tensor(txts)\n",
        "    return txts\n",
        "\n",
        "\n",
        "def ismap(x):\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        return False\n",
        "    return (len(x.shape) == 4) and (x.shape[1] > 3)\n",
        "\n",
        "\n",
        "def isimage(x):\n",
        "    if not isinstance(x, torch.Tensor):\n",
        "        return False\n",
        "    return (len(x.shape) == 4) and (x.shape[1] == 3 or x.shape[1] == 1)\n",
        "\n",
        "\n",
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "\n",
        "def mean_flat(tensor):\n",
        "    \"\"\"\n",
        "    https://github.com/openai/guided-diffusion/blob/27c20a8fab9cb472df5d6bdd6c8d11c8f430b924/guided_diffusion/nn.py#L86\n",
        "    Take the mean over all non-batch dimensions.\n",
        "    \"\"\"\n",
        "    return tensor.mean(dim=list(range(1, len(tensor.shape))))\n",
        "\n",
        "\n",
        "def count_params(model, verbose=False):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    if verbose:\n",
        "        print(f\"{model.__class__.__name__} has {total_params * 1.e-6:.2f} M params.\")\n",
        "    return total_params\n",
        "\n",
        "\n",
        "def instantiate_from_config(config):\n",
        "    if not \"target\" in config:\n",
        "        if config == '__is_first_stage__':\n",
        "            return None\n",
        "        elif config == \"__is_unconditional__\":\n",
        "            return None\n",
        "        raise KeyError(\"Expected key `target` to instantiate.\")\n",
        "    return get_obj_from_str(config[\"target\"])(**config.get(\"params\", dict()))\n",
        "\n",
        "\n",
        "def get_obj_from_str(string, reload=False):\n",
        "    module, cls = string.rsplit(\".\", 1)\n",
        "    if reload:\n",
        "        module_imp = importlib.import_module(module)\n",
        "        importlib.reload(module_imp)\n",
        "    return getattr(importlib.import_module(module, package=None), cls)\n",
        "\n",
        "\n",
        "def _do_parallel_data_prefetch(func, Q, data, idx, idx_to_fn=False):\n",
        "    # create dummy dataset instance\n",
        "\n",
        "    # run prefetching\n",
        "    if idx_to_fn:\n",
        "        res = func(data, worker_id=idx)\n",
        "    else:\n",
        "        res = func(data)\n",
        "    Q.put([idx, res])\n",
        "    Q.put(\"Done\")\n",
        "\n",
        "\n",
        "def parallel_data_prefetch(\n",
        "        func: callable, data, n_proc, target_data_type=\"ndarray\", cpu_intensive=True, use_worker_id=False\n",
        "):\n",
        "    # if target_data_type not in [\"ndarray\", \"list\"]:\n",
        "    #     raise ValueError(\n",
        "    #         \"Data, which is passed to parallel_data_prefetch has to be either of type list or ndarray.\"\n",
        "    #     )\n",
        "    if isinstance(data, np.ndarray) and target_data_type == \"list\":\n",
        "        raise ValueError(\"list expected but function got ndarray.\")\n",
        "    elif isinstance(data, abc.Iterable):\n",
        "        if isinstance(data, dict):\n",
        "            print(\n",
        "                f'WARNING:\"data\" argument passed to parallel_data_prefetch is a dict: Using only its values and disregarding keys.'\n",
        "            )\n",
        "            data = list(data.values())\n",
        "        if target_data_type == \"ndarray\":\n",
        "            data = np.asarray(data)\n",
        "        else:\n",
        "            data = list(data)\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            f\"The data, that shall be processed parallel has to be either an np.ndarray or an Iterable, but is actually {type(data)}.\"\n",
        "        )\n",
        "\n",
        "    if cpu_intensive:\n",
        "        Q = mp.Queue(1000)\n",
        "        proc = mp.Process\n",
        "    else:\n",
        "        Q = Queue(1000)\n",
        "        proc = Thread\n",
        "    # spawn processes\n",
        "    if target_data_type == \"ndarray\":\n",
        "        arguments = [\n",
        "            [func, Q, part, i, use_worker_id]\n",
        "            for i, part in enumerate(np.array_split(data, n_proc))\n",
        "        ]\n",
        "    else:\n",
        "        step = (\n",
        "            int(len(data) / n_proc + 1)\n",
        "            if len(data) % n_proc != 0\n",
        "            else int(len(data) / n_proc)\n",
        "        )\n",
        "        arguments = [\n",
        "            [func, Q, part, i, use_worker_id]\n",
        "            for i, part in enumerate(\n",
        "                [data[i: i + step] for i in range(0, len(data), step)]\n",
        "            )\n",
        "        ]\n",
        "    processes = []\n",
        "    for i in range(n_proc):\n",
        "        p = proc(target=_do_parallel_data_prefetch, args=arguments[i])\n",
        "        processes += [p]\n",
        "\n",
        "    # start processes\n",
        "    print(f\"Start prefetching...\")\n",
        "    import time\n",
        "\n",
        "    start = time.time()\n",
        "    gather_res = [[] for _ in range(n_proc)]\n",
        "    try:\n",
        "        for p in processes:\n",
        "            p.start()\n",
        "\n",
        "        k = 0\n",
        "        while k < n_proc:\n",
        "            # get result\n",
        "            res = Q.get()\n",
        "            if res == \"Done\":\n",
        "                k += 1\n",
        "            else:\n",
        "                gather_res[res[0]] = res[1]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Exception: \", e)\n",
        "        for p in processes:\n",
        "            p.terminate()\n",
        "\n",
        "        raise e\n",
        "    finally:\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "        print(f\"Prefetching complete. [{time.time() - start} sec.]\")\n",
        "\n",
        "    if target_data_type == 'ndarray':\n",
        "        if not isinstance(gather_res[0], np.ndarray):\n",
        "            return np.concatenate([np.asarray(r) for r in gather_res], axis=0)\n",
        "\n",
        "        # order outputs\n",
        "        return np.concatenate(gather_res, axis=0)\n",
        "    elif target_data_type == 'list':\n",
        "        out = []\n",
        "        for r in gather_res:\n",
        "            out.extend(r)\n",
        "        return out\n",
        "    else:\n",
        "        return gather_res\n"
      ],
      "metadata": {
        "id": "zbPVigGv-IG_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clip kornia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avyM1MmBGbxO",
        "outputId": "2a023e36-3ef0-4f15-e6c7-a668735d8e39"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting clip\n",
            "  Downloading clip-0.2.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kornia\n",
            "  Downloading kornia-0.7.4-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting kornia-rs>=0.1.0 (from kornia)\n",
            "  Downloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from kornia) (24.1)\n",
            "Requirement already satisfied: torch>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from kornia) (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.1->kornia) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.9.1->kornia) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.1->kornia) (3.0.2)\n",
            "Downloading kornia-0.7.4-py2.py3-none-any.whl (899 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.4/899.4 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=6988 sha256=d67fc886b780173ec74ad65a458e46b5878f2ccd2f04e3d10d3408cab7e811df\n",
            "  Stored in directory: /root/.cache/pip/wheels/7f/5c/e6/2c0fdb453a3569188864b17e9676bea8b3b7e160c037117869\n",
            "Successfully built clip\n",
            "Installing collected packages: clip, kornia-rs, kornia\n",
            "Successfully installed clip-0.2.0 kornia-0.7.4 kornia-rs-0.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\".\")\n",
        "# sys.path.append('./taming-transformers')\n",
        "import sys\n",
        "sys.path.append('./quant_scripts')  # Adjust this path if needed\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# from ldm.util import instantiate_from_config\n",
        "# from ldm.models.diffusion.ddim import DDIMSampler\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def load_model_from_config(config, ckpt):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    config = OmegaConf.load(\"/content/EfficientDM/configs/latent-diffusion/cin256-v2.yaml\")\n",
        "    model = load_model_from_config(config, \"/content/models/ldm/cin256-v2/model.ckpt\")\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = get_model()\n",
        "    sampler = DDIMSampler(model)\n",
        "\n",
        "    batch_size = 8\n",
        "\n",
        "    ddim_steps = 250\n",
        "    ddim_eta = 1.0\n",
        "    scale = 1.5\n",
        "\n",
        "    all_samples = list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with model.ema_scope():\n",
        "            uc = model.get_learned_conditioning(\n",
        "                {model.cond_stage_key: torch.tensor(batch_size*[1000]).to(model.device)}\n",
        "                )\n",
        "            xc = torch.randint(0,1000,(batch_size,)).to(model.device)\n",
        "            c = model.get_learned_conditioning({model.cond_stage_key: xc.to(model.device)})\n",
        "\n",
        "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                            conditioning=c,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shape=[3, 64, 64],\n",
        "                                            verbose=False,\n",
        "                                            unconditional_guidance_scale=scale,\n",
        "                                            unconditional_conditioning=uc,\n",
        "                                            eta=ddim_eta)\n",
        "\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
        "                                        min=0.0, max=1.0)\n",
        "            all_samples.append(x_samples_ddim)\n",
        "\n",
        "    ## save diffusion input data\n",
        "\n",
        "    import ldm.globalvar as globalvar\n",
        "    input_list = globalvar.getInputList()\n",
        "    torch.save(input_list, 'DiffusionInput_{}steps.pth'.format(ddim_steps))\n",
        "    sys.exit(0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "B4GtETF20lxj",
        "outputId": "2246f567-b2f3-4918-b6f6-3d44f8695432"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/models/ldm/cin256-v2/model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-eca959e5295c>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pl_sd = torch.load(ckpt, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-eca959e5295c>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0msampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDIMSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-eca959e5295c>\u001b[0m in \u001b[0;36mget_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/EfficientDM/configs/latent-diffusion/cin256-v2.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/models/ldm/cin256-v2/model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-eca959e5295c>\u001b[0m in \u001b[0;36mload_model_from_config\u001b[0;34m(config, ckpt)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading model from {ckpt}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mpl_sd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_sd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstantiate_from_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1324\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1326\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
          ]
        }
      ]
    },
    {
      "source": [
        "import sys\n",
        "sys.path.append(\".\")\n",
        "# sys.path.append('./taming-transformers')\n",
        "import sys\n",
        "sys.path.append('./quant_scripts')  # Adjust this path if needed\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# from ldm.util import instantiate_from_config\n",
        "# from ldm.models.diffusion.ddim import DDIMSampler\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def load_model_from_config(config, ckpt):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    config = OmegaConf.load(\"/content/EfficientDM/configs/latent-diffusion/cin256-v2.yaml\")\n",
        "    model = load_model_from_config(config, \"/content/models/ldm/cin256-v2/model.ckpt\")\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = get_model()\n",
        "    sampler = DDIMSampler(model)\n",
        "\n",
        "    batch_size = 8\n",
        "\n",
        "    ddim_steps = 250\n",
        "    ddim_eta = 1.0\n",
        "    scale = 1.5\n",
        "\n",
        "    all_samples = list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with model.ema_scope():\n",
        "            uc = model.get_learned_conditioning(\n",
        "                {model.cond_stage_key: torch.tensor(batch_size*[1000]).to(model.device)}\n",
        "                )\n",
        "            xc = torch.randint(0,1000,(batch_size,)).to(model.device)\n",
        "            c = model.get_learned_conditioning({model.cond_stage_key: xc.to(model.device)})\n",
        "\n",
        "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                            conditioning=c,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shape=[3, 64, 64],\n",
        "                                            verbose=False,\n",
        "                                            unconditional_guidance_scale=scale,\n",
        "                                            unconditional_conditioning=uc,\n",
        "                                            eta=ddim_eta)\n",
        "\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
        "                                        min=0.0, max=1.0)\n",
        "            all_samples.append(x_samples_ddim)\n",
        "\n",
        "    ## save diffusion input data\n",
        "\n",
        "    import ldm.globalvar as globalvar\n",
        "    input_list = globalvar.getInputList()\n",
        "    torch.save(input_list, 'DiffusionInput_{}steps.pth'.format(ddim_steps))\n",
        "    # sys.exit(0) # Remove or comment out this line to prevent premature exit\n",
        "    print(\"Diffusion input data saved. Continuing execution...\") # Add this line to confirm saving and continuation"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2LkL-XMLnQz",
        "outputId": "b0ff6868-bbb6-4460-ab2b-a312ce175326"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/models/ldm/cin256-v2/model.ckpt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-0dc45eb59d08>:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pl_sd = torch.load(ckpt, map_location=\"cpu\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 400.92 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Data shape for DDIM sampling is (8, 3, 64, 64), eta 1.0\n",
            "Running DDIM Sampling with 250 timesteps\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DDIM Sampler: 100%|██████████| 250/250 [03:00<00:00,  1.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diffusion input data saved. Continuing execution...\n"
          ]
        }
      ]
    },
    {
      "source": [
        "!git clone https://github.com/CompVis/latent-diffusion.git"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "JQJ-tdmM_nEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install clip\n"
      ],
      "metadata": {
        "id": "cq4pJCGoBTFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kornia"
      ],
      "metadata": {
        "id": "0x9RYSdkBcEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "import sys\n",
        "sys.path.append('/content/latent-diffusion')  # Adjust this path based on your directory structure\n",
        "\n",
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "# The following line was causing the error\n",
        "# from ldm.util import instantiate_from_config\n",
        "# Import it from the ipython-input-3 file instead:\n",
        "from ldm.util import instantiate_from_config\n",
        "\n",
        "from ldm.models.diffusion.ddim import DDIMSampler\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def load_model_from_config(config, ckpt):\n",
        "    print(f\"Loading model from {ckpt}\")\n",
        "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
        "    sd = pl_sd[\"state_dict\"]\n",
        "    model = instantiate_from_config(config.model)\n",
        "    m, u = model.load_state_dict(sd, strict=False)\n",
        "    model.cuda()\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_model():\n",
        "    config = OmegaConf.load(\"/content/EfficientDM/configs/latent-diffusion/cin256-v2.yaml\")\n",
        "    model = load_model_from_config(config, \"/content/models/ldm/cin256-v2/model.ckpt\")\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = get_model()\n",
        "    sampler = DDIMSampler(model)\n",
        "\n",
        "    batch_size = 8\n",
        "\n",
        "    ddim_steps = 250\n",
        "    ddim_eta = 1.0\n",
        "    scale = 1.5\n",
        "\n",
        "    all_samples = list()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        with model.ema_scope():\n",
        "            uc = model.get_learned_conditioning(\n",
        "                {model.cond_stage_key: torch.tensor(batch_size*[1000]).to(model.device)}\n",
        "                )\n",
        "            xc = torch.randint(0,1000,(batch_size,)).to(model.device)\n",
        "            c = model.get_learned_conditioning({model.cond_stage_key: xc.to(model.device)})\n",
        "\n",
        "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
        "                                            conditioning=c,\n",
        "                                            batch_size=batch_size,\n",
        "                                            shape=[3, 64, 64],\n",
        "                                            verbose=False,\n",
        "                                            unconditional_guidance_scale=scale,\n",
        "                                            unconditional_conditioning=uc,\n",
        "                                            eta=ddim_eta)\n",
        "\n",
        "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
        "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0,\n",
        "                                        min=0.0, max=1.0)\n",
        "            all_samples.append(x_samples_ddim)\n",
        "\n",
        "    ## save diffusion input data\n",
        "    # import ldm.globalvar as globalvar\n",
        "    # input_list = globalvar.getInputList()\n",
        "    # torch.save(input_list, 'DiffusionInput_{}steps.pth'.format(ddim_steps))\n",
        "    # sys.exit(0)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "nWHdrEnw_nVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/EfficientDM/quant_scripts/quantize_ldm_naive.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "torsPP8H1x0g",
        "outputId": "bfdd5df4-60ea-42ed-a558-6acd85599bc9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from /content/models/ldm/cin256-v2/model.ckpt\n",
            "/content/EfficientDM/quant_scripts/quantize_ldm_naive.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
            "LatentDiffusion: Running in eps-prediction mode\n",
            "DiffusionWrapper has 400.92 M params.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Working with z of shape (1, 3, 64, 64) = 12288 dimensions.\n",
            "making attention of type 'vanilla' with 512 in_channels\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/EfficientDM/quant_scripts/quantize_ldm_naive.py\", line 57, in <module>\n",
            "    model.cuda()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1050, in cuda\n",
            "    return self._apply(lambda t: t.cuda(device))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  [Previous line repeated 4 more times]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1050, in <lambda>\n",
            "    return self._apply(lambda t: t.cuda(device))\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 1.06 MiB is free. Process 7381 has 13.16 GiB memory in use. Process 77133 has 1.58 GiB memory in use. Of the allocated memory 1.46 GiB is allocated by PyTorch, and 22.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Set the folder path you want to zip\n",
        "folder_to_zip = '/content/your_folder'  # Replace 'your_folder' with the path to the folder you want to zip\n",
        "\n",
        "# Specify the name of the output zip file\n",
        "output_filename = '/content/your_folder.zip'  # This is the path and name for the zip file\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(output_filename.replace('.zip', ''), 'zip', folder_to_zip)\n",
        "\n",
        "# Download the zip file\n",
        "files.download(output_filename)"
      ],
      "metadata": {
        "id": "LwYbq36NMmpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "895889db-6a2a-4df5-f4cf-c9a5242e03e8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a4ff4959-73d3-4230-b161-a0f983041410\", \"your_folder.zip\", 22)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/EfficientDM.zip /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGPyAsm1uOdg",
        "outputId": "23e47fdf-ea1a-444e-e746-679e4344f005"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/ (stored 0%)\n",
            "  adding: content/.config/ (stored 0%)\n",
            "  adding: content/.config/gce (stored 0%)\n",
            "  adding: content/.config/.last_update_check.json (deflated 24%)\n",
            "  adding: content/.config/configurations/ (stored 0%)\n",
            "  adding: content/.config/configurations/config_default (deflated 15%)\n",
            "  adding: content/.config/logs/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.11.07/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.09.372862.log (deflated 58%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.26.532854.log (deflated 57%)\n",
            "  adding: content/.config/logs/2024.11.07/20.55.50.913514.log (deflated 57%)\n",
            "  adding: content/.config/logs/2024.11.07/20.55.26.763095.log (deflated 92%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.27.350325.log (deflated 56%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.08.122576.log (deflated 85%)\n",
            "  adding: content/.config/default_configs.db (deflated 98%)\n",
            "  adding: content/.config/active_config (stored 0%)\n",
            "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
            "  adding: content/.config/config_sentinel (stored 0%)\n",
            "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "  adding: content/DiffusionInput_250steps.pth (deflated 68%)\n",
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/ldm/ (stored 0%)\n",
            "  adding: content/models/ldm/cin256-v2/ (stored 0%)\n",
            "  adding: content/models/ldm/cin256-v2/model.ckpt (deflated 7%)\n",
            "  adding: content/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/taming-transformers/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/HEAD (stored 0%)\n",
            "  adding: content/taming-transformers/.git/branches/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/info/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/info/exclude (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/refs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/heads/master (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/taming-transformers/.git/hooks/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/taming-transformers/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/taming-transformers/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/taming-transformers/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/taming-transformers/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/taming-transformers/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/taming-transformers/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/taming-transformers/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/taming-transformers/.git/index (deflated 63%)\n",
            "  adding: content/taming-transformers/.git/logs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/HEAD (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/heads/master (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/origin/HEAD (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/packed-refs (deflated 42%)\n",
            "  adding: content/taming-transformers/.git/description (deflated 14%)\n",
            "  adding: content/taming-transformers/.git/objects/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/info/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/pack-34e24108d627c8d57cba87fd86552b8cca48134a.pack (deflated 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/pack-34e24108d627c8d57cba87fd86552b8cca48134a.idx (deflated 2%)\n",
            "  adding: content/taming-transformers/.git/config (deflated 33%)\n",
            "  adding: content/taming-transformers/environment.yaml (deflated 45%)\n",
            "  adding: content/taming-transformers/configs/ (stored 0%)\n",
            "  adding: content/taming-transformers/configs/coco_cond_stage.yaml (deflated 60%)\n",
            "  adding: content/taming-transformers/configs/faceshq_vqgan.yaml (deflated 56%)\n",
            "  adding: content/taming-transformers/configs/imagenetdepth_vqgan.yaml (deflated 54%)\n",
            "  adding: content/taming-transformers/configs/faceshq_transformer.yaml (deflated 64%)\n",
            "  adding: content/taming-transformers/configs/imagenet_vqgan.yaml (deflated 55%)\n",
            "  adding: content/taming-transformers/configs/drin_transformer.yaml (deflated 72%)\n",
            "  adding: content/taming-transformers/configs/coco_scene_images_transformer.yaml (deflated 67%)\n",
            "  adding: content/taming-transformers/configs/custom_vqgan.yaml (deflated 55%)\n",
            "  adding: content/taming-transformers/configs/sflckr_cond_stage.yaml (deflated 56%)\n",
            "  adding: content/taming-transformers/configs/open_images_scene_images_transformer.yaml (deflated 70%)\n",
            "  adding: content/taming-transformers/License.txt (deflated 41%)\n",
            "  adding: content/taming-transformers/taming/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/models/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/models/cond_transformer.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/models/dummy_cond_stage.py (deflated 58%)\n",
            "  adding: content/taming-transformers/taming/models/vqgan.py (deflated 85%)\n",
            "  adding: content/taming-transformers/taming/data/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/data/utils.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/data/coco.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_open_images.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_dataset.py (deflated 76%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_coco.py (deflated 72%)\n",
            "  adding: content/taming-transformers/taming/data/base.py (deflated 67%)\n",
            "  adding: content/taming-transformers/taming/data/custom.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/data/image_transforms.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/data/faceshq.py (deflated 82%)\n",
            "  adding: content/taming-transformers/taming/data/imagenet.py (deflated 78%)\n",
            "  adding: content/taming-transformers/taming/data/sflckr.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/utils.py (deflated 66%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/objects_bbox.py (deflated 64%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/objects_center_points.py (deflated 71%)\n",
            "  adding: content/taming-transformers/taming/data/open_images_helper.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/data/helper_types.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/data/ade20k.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/lr_scheduler.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/modules/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/segmentation.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/lpips.py (deflated 69%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/__init__.py (deflated 2%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/vqperceptual.py (deflated 76%)\n",
            "  adding: content/taming-transformers/taming/modules/discriminator/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/discriminator/model.py (deflated 64%)\n",
            "  adding: content/taming-transformers/taming/modules/diffusionmodules/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/diffusionmodules/model.py (deflated 89%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/quantize.py (deflated 78%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/__pycache__/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/__pycache__/quantize.cpython-310.pyc (deflated 51%)\n",
            "  adding: content/taming-transformers/taming/modules/misc/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/misc/coord.py (deflated 62%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/permuter.py (deflated 83%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/mingpt.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/modules/util.py (deflated 71%)\n",
            "  adding: content/taming-transformers/taming/util.py (deflated 64%)\n",
            "  adding: content/taming-transformers/data/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/celebahqvalidation.txt (deflated 80%)\n",
            "  adding: content/taming-transformers/data/sflckr_examples.txt (deflated 47%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001498.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000509.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000125.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001845.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000573.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001578.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000734.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001583.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001698.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000203.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001388.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001412.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000880.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000532.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000126.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001766.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000123.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000875.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000289.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001209.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000262.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001177.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000636.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001966.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000287.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000603.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001851.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001200.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001947.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ILSVRC2012_val_00032333.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ILSVRC2012_val_00047325.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02085782/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02085782/ILSVRC2012_val_00012298.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02111889/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02111889/ILSVRC2012_val_00042625.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00046802.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00001336.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00008236.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02102318/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02102318/ILSVRC2012_val_00024691.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02086646/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02086646/ILSVRC2012_val_00011473.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02089973/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02089973/ILSVRC2012_val_00000028.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02105505/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02105505/ILSVRC2012_val_00031252.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ILSVRC2012_val_00047491.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ILSVRC2012_val_00034784.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02088466/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02088466/ILSVRC2012_val_00013651.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02096294/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02096294/ILSVRC2012_val_00042133.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02110627/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02110627/ILSVRC2012_val_00008310.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01847000/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01847000/ILSVRC2012_val_00022364.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02093256/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02093256/ILSVRC2012_val_00046547.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099712/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099712/ILSVRC2012_val_00023471.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01795545/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01795545/ILSVRC2012_val_00023344.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101556/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101556/ILSVRC2012_val_00030540.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02100877/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02100877/ILSVRC2012_val_00039863.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099601/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099601/ILSVRC2012_val_00005697.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01819313/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01819313/ILSVRC2012_val_00003068.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01843065/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01843065/ILSVRC2012_val_00022439.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation-annotations-bbox.csv (deflated 77%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation-images.csv (deflated 10%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train-images-boxable.csv (deflated 89%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/class-descriptions-boxable.csv (deflated 53%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000afe7726e121ea.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba40bf7a2b458.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b59a7822679e6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1b3b85edd850.jpg (deflated 9%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc5ad4cc3ae73.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b5bc07c0c5df7.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4671075914cd.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b09d5d3fc821f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b38d9f2f664fe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1971d8daaeef.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc75d38907c78.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bcd3bcd95cbb3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000af180a3163f17.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adef7197e3118.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab31e6be35fed.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b3940e7d25c03.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abc075d659122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9c365c9e307a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b567c26dd4e5d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b825dea3016eb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9a97776b3634.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b397382b2464a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad0ecfb21ee63.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb0ae453283b0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4935979bf4b5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba3ca8a2ca955.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000aee0af66d4237.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000acf666d991c39.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9b00d7aef8f5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000baa6f7dae9b79.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad3d42653f5f6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba28d70b1a999.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b393437134262.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b299b5f5ed902.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000aecd78b230135.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9b61afea2cd4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b7dfaa1810a83.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac8c676b6077a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bbdf0dc8099d8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab7bec71cc50a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc7b0a1889bcb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9f3ba4891c11.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2a982a903d0d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ae235808cc1e8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b260e1f08a32a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b8d80f7386698.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b50bdd1933a36.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b06c0eed42a4c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb200fc78fc30.jpg (deflated 6%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad20b5e452b24.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba940f8cfc9bf.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bcee5bed5446b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abc821f66a892.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b21663becc68e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc387c731dd97.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b168e791f591d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba221f70676c6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b72e1446f8849.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b432ae644b679.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b093da01e5bfe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b76a9b80ba43a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ae28755d2d20e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac34008b0ba4c.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc1eb7f74adae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b0f5159f54105.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb81adefe7332.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b70a84aab664b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b63a1445f53c8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad6c520be9ec5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b55559b0244d7.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2b00065e564a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b65a36ad46f9e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adfe5b817011c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4fcdf1af3361.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb8bd9b1bca65.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adcdd7244ce4a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b93644609911f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9007a01f7405.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b485cedacbf97.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b42cae15622e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac95750ac7399.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bab5b1a67844e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9814a07fd974.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab8c20b3e5b58.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b606e130bdf5e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc5006eb7fd98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b81b5757963e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b87119cc301cf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b29496f75c8e5.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abe5eddc5b303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9d6c0f7d794d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2d1789d5f80d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc33717a6371f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b55e339f0b131.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000af631fb329557.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b0f235dcf2caa.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb2f7132013dc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb846e2629e83.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1b92f0800e94.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad6fa67b5ad96.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/oidv6-train-annotations-bbox.csv (deflated 76%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ada35baba28134b.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c6ddd2c210450e.jpg (deflated 6%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad96a2881998657.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09df63bd01367ca3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a41cda5f44baaf6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac166d12e401a98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a94296ff543a1dc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aaad833ac61ac9d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0adc1330287b2e66.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f8e760f60df0da.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab2b64f27f8baca.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09e094375efab7fe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d64f43c7111879.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9ff75a7897e757.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ade7aef439e2102.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab050b51e78acdb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a02e8b6820064f5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a917bbca24cf75d.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a78374f2d3949ae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7c597abf1e90d4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1f4761dc7fe1eb.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a08a4711c728078.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aacbdb54e853a0a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a03326036647703.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a556c8163b58fae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a392d80c905a9df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9f183e46c76019.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac2f91a7995aa8b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a23d3f0e7d850f4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1bd356f90aaab6.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a02c648d24f39fb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aad9fc79a35bd53.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c7f89055cf399b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f8b77a88f224d9.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aa206fa7ea80036.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab5c690eebfad95.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09cbba9f5e097a19.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a2c6ef66896fb92.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7ffd65766a4741.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09dcb9b52055d40f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a4db5693da70448.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a73064c82730ff5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a47e7d602855f93.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d354dbd3dcc857.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c67960e389e4df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c5b4d6bc25788d.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7074a2a5515531.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d45c49c4adbae4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad99d610a9092e6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac3c1db1b3645f2.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a37aa0734ac8016.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09ebcee57699eb98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d2112596d9155b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0adc373e996aadc2.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d8aa2d19ff724d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c993afacd01547.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a4abf0a8071b917.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a563d05ebab4fe3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a13dcaaab9a35e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f531fe4f6d95f3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a82f0443c940816.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a50911d08250183.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c863d76bcf6b00.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a278d979b63fc72.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aae34863935e33a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a34d80ee1db201e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac51477636a6933.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac52440f73b5c80.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3c01759e77a02d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a877314ca2039d9.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7d56b2fb989fe8.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a8657e8b5c9d7bb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a72fef43a51c479.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7fbc1d68e4e5ae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a39325e5ad7f5a0.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7f4d9a0ccb9afe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a600f1148d1023c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a599940d33b6b2b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9f73b3c2557150.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09fa093bcd300c1a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a120822d362dddf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a5972c68b6bb265.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09e617d9d3120b32.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7be0b883a12966.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0add91a2efb3f33d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a6a03c8f23ee744.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab10a6417ef2301.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3f577a327ca7cc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09ea349ee555b61d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7f13330a5d0023.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09dd0671cd633432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a6bc386b28f2aac.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3f9b3d57ef354a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aa3a6c33fca122b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad7884032419621.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad7bad30cd432df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3873442ad329c2.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1b11867383b13e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad602c943c9f568.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0acfa779589204bf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000303653.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000205834.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000406997.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000255824.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000356347.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000522393.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000361180.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000119445.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000166259.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000350405.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000057672.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000299720.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000517069.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000348045.png (deflated 30%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000237928.png (deflated 11%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000110638.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000569273.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000323895.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000403385.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000299355.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000348481.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000175387.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000052507.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000185599.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000154358.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000098392.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000335529.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000256775.png (deflated 11%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000064898.png (deflated 35%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000166563.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000018380.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000491464.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000231169.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000128658.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000452122.png (deflated 27%)\n",
            "  adding: content/taming-transformers/data/flickr_tags.txt (deflated 56%)\n",
            "  adding: content/taming-transformers/data/ade20k_examples.txt (deflated 79%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000126.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000636.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001412.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001498.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000603.png (deflated 10%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000532.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000734.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001845.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001177.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000303.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000203.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000289.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000875.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000287.png (deflated 33%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001766.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001388.png (deflated 24%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001578.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000509.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001966.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000262.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001851.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001947.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001583.png (deflated 16%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000123.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000880.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001200.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000573.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000125.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001698.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001209.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/instances_train2017.json (deflated 70%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/instances_val2017.json (deflated 70%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/stuff_train2017.json (deflated 54%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/stuff_val2017.json (deflated 54%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016598.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018150.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013546.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013774.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014007.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016439.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012120.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015660.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012062.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018193.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015335.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013177.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014038.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010092.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018737.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015440.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019924.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016451.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017031.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015079.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016958.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014439.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010707.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018491.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016502.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019042.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020059.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017436.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013201.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013348.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018575.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015254.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015751.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017182.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010995.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017029.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013597.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017379.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016249.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018837.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015746.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017905.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017627.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018833.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016010.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011051.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020333.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018519.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015272.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019402.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011511.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017714.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013659.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010363.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017207.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017178.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016228.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013004.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019221.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012639.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019109.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012667.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012670.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011149.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010977.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015517.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011813.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011197.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013291.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014831.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013729.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011699.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011615.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010583.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020247.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014888.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012576.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015597.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011760.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014226.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019742.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019786.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015278.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015956.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017959.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015338.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012748.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017115.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010764.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017899.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015497.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018770.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012280.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013923.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020107.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014473.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010145.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010229.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010014.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010123.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010343.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010179.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010400.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010058.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010309.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010313.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010056.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010142.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010012.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010463.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010449.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010430.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010393.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010161.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010290.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010241.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010114.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010276.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010176.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010440.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010219.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010073.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010281.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010040.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010039.jpg (deflated 10%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010024.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010107.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010321.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010239.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010244.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010248.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010046.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010232.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010324.jpg (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010005.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010407.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010358.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010108.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010369.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010327.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010405.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010115.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010211.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010386.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010249.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010444.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010256.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010008.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010205.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010319.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010192.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010230.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010216.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010136.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010442.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010175.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010265.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010275.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010337.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010243.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010346.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010023.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010138.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010041.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010421.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010130.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010434.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010428.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010166.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010097.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010094.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010125.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010083.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010015.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010403.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010414.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010263.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010388.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010077.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010318.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010342.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010069.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010420.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010104.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010037.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010149.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010445.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010082.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010222.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010084.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010217.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010196.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010245.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010395.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ffhqtrain.txt (deflated 70%)\n",
            "  adding: content/taming-transformers/data/celebahqtrain.txt (deflated 80%)\n",
            "  adding: content/taming-transformers/data/subreddits.txt (deflated 37%)\n",
            "  adding: content/taming-transformers/data/ffhqvalidation.txt (deflated 69%)\n",
            "  adding: content/taming-transformers/data/coco_examples.txt (deflated 71%)\n",
            "  adding: content/taming-transformers/data/coco_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000403385.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000064898.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000299720.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000569273.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000166563.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000205834.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000348045.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000237928.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000356347.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000350405.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000303653.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000119445.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000154358.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000522393.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000231169.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000018380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000361180.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000323895.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000452122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000491464.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000335529.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000110638.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000348481.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000098392.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000057672.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000185599.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000255824.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000256775.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000175387.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000166259.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000517069.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000299355.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000406997.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000128658.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000052507.jpg (deflated 7%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/newzealand_np/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/newzealand_np/7942812194_9348729b93_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/4748115806_7219c2b3be_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/14996762478_a9bdbf959a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/3542389801_a2cbfee1e1_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/26320755536_7c769b6218_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/meadow/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/meadow/18864473291_844325caab_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/volcano/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/volcano/50254383883_27ed6ea93a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/desert/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/desert/4534149722_3cc4f92891_b.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/25735082181_999927fe5a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/20099378793_cc2df820af_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ireland/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ireland/15570753471_74db396d14_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_mountains/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_mountains/33509672006_bf4c416afd_b.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/mongolia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/mongolia/6076373946_e9ea2aee32_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/cliff_ocean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/cliff_ocean/36142796444_45d452f567_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/8364557382_c6c9ee2fd6_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/44974691685_8e7372e2b1_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/14351041152_ef77484a1f_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/18176301_c9d27557cf_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/alaska_lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/alaska_lakes/43259216952_59352d7204_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_landscape/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_landscape/4079319632_0133685b2c_b.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/12822389285_a7723081b5_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/8720651218_ca82a6608e_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/256743165_9f130ba95b_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/2883773_881c197107_c.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/lakes/39933489595_f0e5d85b6d_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/newzealand_np/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/newzealand_np/7942812194_9348729b93_b.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/3542389801_a2cbfee1e1_b.png (deflated 14%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/26320755536_7c769b6218_b.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/14996762478_a9bdbf959a_b.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/4748115806_7219c2b3be_b.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/meadow/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/meadow/18864473291_844325caab_b.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/volcano/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/volcano/50254383883_27ed6ea93a_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/desert/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/desert/4534149722_3cc4f92891_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/25735082181_999927fe5a_b.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/20099378793_cc2df820af_b.png (deflated 10%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ireland/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ireland/15570753471_74db396d14_b.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_mountains/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_mountains/33509672006_bf4c416afd_b.png (deflated 32%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/mongolia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/mongolia/6076373946_e9ea2aee32_b.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/cliff_ocean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/cliff_ocean/36142796444_45d452f567_b.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/8364557382_c6c9ee2fd6_b.png (deflated 43%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/44974691685_8e7372e2b1_b.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/14351041152_ef77484a1f_b.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/18176301_c9d27557cf_b.png (deflated 28%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/alaska_lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/alaska_lakes/43259216952_59352d7204_b.png (deflated 29%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_landscape/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_landscape/4079319632_0133685b2c_b.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/12822389285_a7723081b5_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/8720651218_ca82a6608e_b.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/2883773_881c197107_c.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/256743165_9f130ba95b_b.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/lakes/39933489595_f0e5d85b6d_b.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/drin_examples.txt (deflated 75%)\n",
            "  adding: content/taming-transformers/data/drin_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ILSVRC2012_val_00032333.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ILSVRC2012_val_00047325.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02085782/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02085782/ILSVRC2012_val_00012298.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02111889/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02111889/ILSVRC2012_val_00042625.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00046802.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00001336.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00008236.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02102318/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02102318/ILSVRC2012_val_00024691.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02086646/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02086646/ILSVRC2012_val_00011473.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02089973/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02089973/ILSVRC2012_val_00000028.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02105505/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02105505/ILSVRC2012_val_00031252.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ILSVRC2012_val_00034784.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ILSVRC2012_val_00047491.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02088466/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02088466/ILSVRC2012_val_00013651.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02096294/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02096294/ILSVRC2012_val_00042133.JPEG (deflated 3%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02110627/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02110627/ILSVRC2012_val_00008310.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01847000/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01847000/ILSVRC2012_val_00022364.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02093256/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02093256/ILSVRC2012_val_00046547.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099712/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099712/ILSVRC2012_val_00023471.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01795545/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01795545/ILSVRC2012_val_00023344.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101556/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101556/ILSVRC2012_val_00030540.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02100877/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02100877/ILSVRC2012_val_00039863.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099601/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099601/ILSVRC2012_val_00005697.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01819313/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01819313/ILSVRC2012_val_00003068.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01843065/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01843065/ILSVRC2012_val_00022439.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/scripts/ (stored 0%)\n",
            "  adding: content/taming-transformers/scripts/taming-transformers.ipynb (deflated 25%)\n",
            "  adding: content/taming-transformers/scripts/make_scene_samples.py (deflated 67%)\n",
            "  adding: content/taming-transformers/scripts/sample_conditional.py (deflated 68%)\n",
            "  adding: content/taming-transformers/scripts/extract_submodel.py (deflated 49%)\n",
            "  adding: content/taming-transformers/scripts/extract_segmentation.py (deflated 60%)\n",
            "  adding: content/taming-transformers/scripts/reconstruction_usage.ipynb (deflated 25%)\n",
            "  adding: content/taming-transformers/scripts/sample_fast.py (deflated 70%)\n",
            "  adding: content/taming-transformers/scripts/make_samples.py (deflated 69%)\n",
            "  adding: content/taming-transformers/scripts/extract_depth.py (deflated 63%)\n",
            "  adding: content/taming-transformers/README.md (deflated 65%)\n",
            "  adding: content/taming-transformers/main.py (deflated 74%)\n",
            "  adding: content/taming-transformers/setup.py (deflated 39%)\n",
            "  adding: content/taming-transformers/assets/ (stored 0%)\n",
            "  adding: content/taming-transformers/assets/coco_scene_images_training.svg (deflated 85%)\n",
            "  adding: content/taming-transformers/assets/teaser.png (deflated 1%)\n",
            "  adding: content/taming-transformers/assets/lake_in_the_mountains.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/drin.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/assets/faceshq.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/imagenet.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/stormy.jpeg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/sunset_and_ocean.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/mountain.jpeg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/scene_images_samples.svg (deflated 25%)\n",
            "  adding: content/taming-transformers/assets/first_stage_mushrooms.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/first_stage_squirrels.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/birddrawnbyachild.png (deflated 0%)\n",
            "  adding: content/EfficientDM/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/HEAD (stored 0%)\n",
            "  adding: content/EfficientDM/.git/branches/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/info/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/info/exclude (deflated 28%)\n",
            "  adding: content/EfficientDM/.git/refs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/heads/main (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/EfficientDM/.git/hooks/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/EfficientDM/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/EfficientDM/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/EfficientDM/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/EfficientDM/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/EfficientDM/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/EfficientDM/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/EfficientDM/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/EfficientDM/.git/index (deflated 55%)\n",
            "  adding: content/EfficientDM/.git/logs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/HEAD (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/heads/main (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/origin/HEAD (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/packed-refs (deflated 9%)\n",
            "  adding: content/EfficientDM/.git/description (deflated 14%)\n",
            "  adding: content/EfficientDM/.git/objects/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/info/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/pack-981733d82cdf3418a37a580efdd4179c422f0000.idx (deflated 18%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/pack-981733d82cdf3418a37a580efdd4179c422f0000.pack (deflated 0%)\n",
            "  adding: content/EfficientDM/.git/config (deflated 31%)\n",
            "  adding: content/EfficientDM/configs/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/cin-ldm-vq-f8.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/lsun_bedrooms-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/cin256-v2.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/lsun_churches-ldm-kl-8.yaml (deflated 60%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/txt2img-1p4B-eval.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/celebahq-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/ffhq-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_8x8x64.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_32x32x4.yaml (deflated 55%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_64x64x3.yaml (deflated 55%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_16x16x16.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/configs/retrieval-augmented-diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/retrieval-augmented-diffusion/768x768.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis512/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis512/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ldm/layout2img-openimages256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/layout2img-openimages256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_beds256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_beds256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/cin256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/cin256/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/ffhq256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/ffhq256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/inpainting_big/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/inpainting_big/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/bsr_sr/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/bsr_sr/config.yaml (deflated 67%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_churches256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_churches256/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/celeba256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/celeba256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/text2img256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/text2img256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8-n256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8-n256/config.yaml (deflated 56%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4-noattn/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4-noattn/config.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f16/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f16/config.yaml (deflated 58%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f32/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f32/config.yaml (deflated 58%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f4/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f4/config.yaml (deflated 56%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f8/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f8/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4/config.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f16/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f16/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/ldm/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/autoencoder.py (deflated 80%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ddpm.py (deflated 79%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ddim.py (deflated 86%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/plms.py (deflated 76%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ddpm.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/__init__.cpython-310.pyc (deflated 24%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ddim.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/classifier.py (deflated 71%)\n",
            "  adding: content/EfficientDM/ldm/models/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/__pycache__/autoencoder.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/EfficientDM/ldm/globalvar.py (deflated 61%)\n",
            "  adding: content/EfficientDM/ldm/data/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/data/base.py (deflated 54%)\n",
            "  adding: content/EfficientDM/ldm/data/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/data/imagenet.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/data/lsun.py (deflated 72%)\n",
            "  adding: content/EfficientDM/ldm/lr_scheduler.py (deflated 80%)\n",
            "  adding: content/EfficientDM/ldm/modules/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/modules.py (deflated 70%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/__init__.cpython-310.pyc (deflated 26%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/modules.cpython-310.pyc (deflated 55%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/contperceptual.py (deflated 76%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/vqperceptual.py (deflated 74%)\n",
            "  adding: content/EfficientDM/ldm/modules/ema.py (deflated 68%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/distributions.py (deflated 66%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/__init__.cpython-310.pyc (deflated 25%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/distributions.cpython-310.pyc (deflated 50%)\n",
            "  adding: content/EfficientDM/ldm/modules/attention.py (deflated 74%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/openaimodel.py (deflated 82%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/model.py (deflated 85%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/__init__.cpython-310.pyc (deflated 29%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/model.cpython-310.pyc (deflated 57%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/openaimodel.cpython-310.pyc (deflated 58%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/util.cpython-310.pyc (deflated 50%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/util.py (deflated 66%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/bsrgan.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/__init__.py (deflated 60%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils/test.png (deflated 1%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils_image.py (deflated 77%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/bsrgan_light.py (deflated 73%)\n",
            "  adding: content/EfficientDM/ldm/modules/x_transformer.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/x_transformer.cpython-310.pyc (deflated 52%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/attention.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/ema.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/util.cpython-310.pyc (deflated 43%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/globalvar.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/EfficientDM/ldm/util.py (deflated 62%)\n",
            "  adding: content/EfficientDM/LICENSE (deflated 41%)\n",
            "  adding: content/EfficientDM/.gitignore (deflated 4%)\n",
            "  adding: content/EfficientDM/quant_scripts/ (stored 0%)\n",
            "  adding: content/EfficientDM/quant_scripts/quantize_ldm_naive.py (deflated 60%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_layer.py (deflated 82%)\n",
            "  adding: content/EfficientDM/quant_scripts/collect_input_4_calib.py (deflated 61%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_dataset.py (deflated 78%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_model.py (deflated 84%)\n",
            "  adding: content/EfficientDM/quant_scripts/train_efficientdm.py (deflated 71%)\n",
            "  adding: content/EfficientDM/quant_scripts/save_naive_2_intmodel.py (deflated 62%)\n",
            "  adding: content/EfficientDM/quant_scripts/downsample_talsq_ckpt.py (deflated 41%)\n",
            "  adding: content/EfficientDM/quant_scripts/sample_lora_intmodel.py (deflated 64%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/quant_model.cpython-310.pyc (deflated 59%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/quant_layer.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/EfficientDM/scripts/ (stored 0%)\n",
            "  adding: content/EfficientDM/scripts/download_models.sh (deflated 80%)\n",
            "  adding: content/EfficientDM/scripts/sample_diffusion.py (deflated 68%)\n",
            "  adding: content/EfficientDM/scripts/txt2img.py (deflated 67%)\n",
            "  adding: content/EfficientDM/scripts/download_first_stages.sh (deflated 84%)\n",
            "  adding: content/EfficientDM/scripts/inpaint.py (deflated 65%)\n",
            "  adding: content/EfficientDM/scripts/knn2img.py (deflated 71%)\n",
            "  adding: content/EfficientDM/scripts/generate_samples_4_evaluation_FP.py (deflated 69%)\n",
            "  adding: content/EfficientDM/scripts/latent_imagenet_diffusion.ipynb (deflated 25%)\n",
            "  adding: content/EfficientDM/scripts/train_searcher.py (deflated 68%)\n",
            "  adding: content/EfficientDM/README.md (deflated 51%)\n",
            "  adding: content/sample_data/ (stored 0%)\n",
            "  adding: content/sample_data/anscombe.json (deflated 83%)\n",
            "  adding: content/sample_data/README.md (deflated 39%)\n",
            "  adding: content/sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "  adding: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "  adding: content/sample_data/mnist_test.csv (deflated 88%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/models.zip /content/\n",
        "!zip -r /content/sample_data.zip /content/\n",
        "!zip -r /content/taming-transformers.zip /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s70UnascvZmc",
        "outputId": "6fe3d750-48fe-4c97-9f44-9a932803ac8d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/ (stored 0%)\n",
            "  adding: content/.config/ (stored 0%)\n",
            "  adding: content/.config/gce (stored 0%)\n",
            "  adding: content/.config/.last_update_check.json (deflated 24%)\n",
            "  adding: content/.config/configurations/ (stored 0%)\n",
            "  adding: content/.config/configurations/config_default (deflated 15%)\n",
            "  adding: content/.config/logs/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.11.07/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.09.372862.log (deflated 58%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.26.532854.log (deflated 57%)\n",
            "  adding: content/.config/logs/2024.11.07/20.55.50.913514.log (deflated 57%)\n",
            "  adding: content/.config/logs/2024.11.07/20.55.26.763095.log (deflated 92%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.27.350325.log (deflated 56%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.08.122576.log (deflated 85%)\n",
            "  adding: content/.config/default_configs.db (deflated 98%)\n",
            "  adding: content/.config/active_config (stored 0%)\n",
            "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
            "  adding: content/.config/config_sentinel (stored 0%)\n",
            "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "  adding: content/DiffusionInput_250steps.pth (deflated 68%)\n",
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/ldm/ (stored 0%)\n",
            "  adding: content/models/ldm/cin256-v2/ (stored 0%)\n",
            "  adding: content/models/ldm/cin256-v2/model.ckpt (deflated 7%)\n",
            "  adding: content/EfficientDM.zip (stored 0%)\n",
            "  adding: content/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/taming-transformers/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/HEAD (stored 0%)\n",
            "  adding: content/taming-transformers/.git/branches/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/info/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/info/exclude (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/refs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/heads/master (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/taming-transformers/.git/hooks/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/taming-transformers/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/taming-transformers/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/taming-transformers/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/taming-transformers/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/taming-transformers/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/taming-transformers/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/taming-transformers/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/taming-transformers/.git/index (deflated 63%)\n",
            "  adding: content/taming-transformers/.git/logs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/HEAD (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/heads/master (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/origin/HEAD (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/packed-refs (deflated 42%)\n",
            "  adding: content/taming-transformers/.git/description (deflated 14%)\n",
            "  adding: content/taming-transformers/.git/objects/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/info/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/pack-34e24108d627c8d57cba87fd86552b8cca48134a.pack (deflated 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/pack-34e24108d627c8d57cba87fd86552b8cca48134a.idx (deflated 2%)\n",
            "  adding: content/taming-transformers/.git/config (deflated 33%)\n",
            "  adding: content/taming-transformers/environment.yaml (deflated 45%)\n",
            "  adding: content/taming-transformers/configs/ (stored 0%)\n",
            "  adding: content/taming-transformers/configs/coco_cond_stage.yaml (deflated 60%)\n",
            "  adding: content/taming-transformers/configs/faceshq_vqgan.yaml (deflated 56%)\n",
            "  adding: content/taming-transformers/configs/imagenetdepth_vqgan.yaml (deflated 54%)\n",
            "  adding: content/taming-transformers/configs/faceshq_transformer.yaml (deflated 64%)\n",
            "  adding: content/taming-transformers/configs/imagenet_vqgan.yaml (deflated 55%)\n",
            "  adding: content/taming-transformers/configs/drin_transformer.yaml (deflated 72%)\n",
            "  adding: content/taming-transformers/configs/coco_scene_images_transformer.yaml (deflated 67%)\n",
            "  adding: content/taming-transformers/configs/custom_vqgan.yaml (deflated 55%)\n",
            "  adding: content/taming-transformers/configs/sflckr_cond_stage.yaml (deflated 56%)\n",
            "  adding: content/taming-transformers/configs/open_images_scene_images_transformer.yaml (deflated 70%)\n",
            "  adding: content/taming-transformers/License.txt (deflated 41%)\n",
            "  adding: content/taming-transformers/taming/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/models/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/models/cond_transformer.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/models/dummy_cond_stage.py (deflated 58%)\n",
            "  adding: content/taming-transformers/taming/models/vqgan.py (deflated 85%)\n",
            "  adding: content/taming-transformers/taming/data/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/data/utils.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/data/coco.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_open_images.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_dataset.py (deflated 76%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_coco.py (deflated 72%)\n",
            "  adding: content/taming-transformers/taming/data/base.py (deflated 67%)\n",
            "  adding: content/taming-transformers/taming/data/custom.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/data/image_transforms.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/data/faceshq.py (deflated 82%)\n",
            "  adding: content/taming-transformers/taming/data/imagenet.py (deflated 78%)\n",
            "  adding: content/taming-transformers/taming/data/sflckr.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/utils.py (deflated 66%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/objects_bbox.py (deflated 64%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/objects_center_points.py (deflated 71%)\n",
            "  adding: content/taming-transformers/taming/data/open_images_helper.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/data/helper_types.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/data/ade20k.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/lr_scheduler.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/modules/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/segmentation.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/lpips.py (deflated 69%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/__init__.py (deflated 2%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/vqperceptual.py (deflated 76%)\n",
            "  adding: content/taming-transformers/taming/modules/discriminator/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/discriminator/model.py (deflated 64%)\n",
            "  adding: content/taming-transformers/taming/modules/diffusionmodules/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/diffusionmodules/model.py (deflated 89%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/quantize.py (deflated 78%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/__pycache__/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/__pycache__/quantize.cpython-310.pyc (deflated 51%)\n",
            "  adding: content/taming-transformers/taming/modules/misc/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/misc/coord.py (deflated 62%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/permuter.py (deflated 83%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/mingpt.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/modules/util.py (deflated 71%)\n",
            "  adding: content/taming-transformers/taming/util.py (deflated 64%)\n",
            "  adding: content/taming-transformers/data/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/celebahqvalidation.txt (deflated 80%)\n",
            "  adding: content/taming-transformers/data/sflckr_examples.txt (deflated 47%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001498.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000509.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000125.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001845.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000573.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001578.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000734.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001583.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001698.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000203.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001388.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001412.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000880.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000532.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000126.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001766.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000123.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000875.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000289.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001209.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000262.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001177.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000636.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001966.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000287.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000603.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001851.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001200.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001947.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ILSVRC2012_val_00032333.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ILSVRC2012_val_00047325.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02085782/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02085782/ILSVRC2012_val_00012298.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02111889/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02111889/ILSVRC2012_val_00042625.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00046802.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00001336.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00008236.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02102318/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02102318/ILSVRC2012_val_00024691.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02086646/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02086646/ILSVRC2012_val_00011473.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02089973/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02089973/ILSVRC2012_val_00000028.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02105505/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02105505/ILSVRC2012_val_00031252.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ILSVRC2012_val_00047491.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ILSVRC2012_val_00034784.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02088466/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02088466/ILSVRC2012_val_00013651.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02096294/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02096294/ILSVRC2012_val_00042133.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02110627/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02110627/ILSVRC2012_val_00008310.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01847000/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01847000/ILSVRC2012_val_00022364.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02093256/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02093256/ILSVRC2012_val_00046547.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099712/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099712/ILSVRC2012_val_00023471.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01795545/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01795545/ILSVRC2012_val_00023344.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101556/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101556/ILSVRC2012_val_00030540.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02100877/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02100877/ILSVRC2012_val_00039863.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099601/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099601/ILSVRC2012_val_00005697.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01819313/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01819313/ILSVRC2012_val_00003068.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01843065/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01843065/ILSVRC2012_val_00022439.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation-annotations-bbox.csv (deflated 77%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation-images.csv (deflated 10%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train-images-boxable.csv (deflated 89%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/class-descriptions-boxable.csv (deflated 53%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000afe7726e121ea.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba40bf7a2b458.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b59a7822679e6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1b3b85edd850.jpg (deflated 9%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc5ad4cc3ae73.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b5bc07c0c5df7.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4671075914cd.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b09d5d3fc821f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b38d9f2f664fe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1971d8daaeef.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc75d38907c78.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bcd3bcd95cbb3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000af180a3163f17.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adef7197e3118.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab31e6be35fed.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b3940e7d25c03.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abc075d659122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9c365c9e307a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b567c26dd4e5d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b825dea3016eb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9a97776b3634.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b397382b2464a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad0ecfb21ee63.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb0ae453283b0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4935979bf4b5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba3ca8a2ca955.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000aee0af66d4237.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000acf666d991c39.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9b00d7aef8f5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000baa6f7dae9b79.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad3d42653f5f6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba28d70b1a999.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b393437134262.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b299b5f5ed902.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000aecd78b230135.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9b61afea2cd4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b7dfaa1810a83.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac8c676b6077a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bbdf0dc8099d8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab7bec71cc50a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc7b0a1889bcb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9f3ba4891c11.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2a982a903d0d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ae235808cc1e8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b260e1f08a32a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b8d80f7386698.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b50bdd1933a36.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b06c0eed42a4c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb200fc78fc30.jpg (deflated 6%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad20b5e452b24.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba940f8cfc9bf.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bcee5bed5446b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abc821f66a892.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b21663becc68e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc387c731dd97.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b168e791f591d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba221f70676c6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b72e1446f8849.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b432ae644b679.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b093da01e5bfe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b76a9b80ba43a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ae28755d2d20e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac34008b0ba4c.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc1eb7f74adae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b0f5159f54105.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb81adefe7332.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b70a84aab664b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b63a1445f53c8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad6c520be9ec5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b55559b0244d7.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2b00065e564a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b65a36ad46f9e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adfe5b817011c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4fcdf1af3361.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb8bd9b1bca65.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adcdd7244ce4a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b93644609911f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9007a01f7405.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b485cedacbf97.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b42cae15622e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac95750ac7399.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bab5b1a67844e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9814a07fd974.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab8c20b3e5b58.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b606e130bdf5e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc5006eb7fd98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b81b5757963e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b87119cc301cf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b29496f75c8e5.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abe5eddc5b303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9d6c0f7d794d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2d1789d5f80d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc33717a6371f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b55e339f0b131.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000af631fb329557.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b0f235dcf2caa.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb2f7132013dc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb846e2629e83.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1b92f0800e94.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad6fa67b5ad96.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/oidv6-train-annotations-bbox.csv (deflated 76%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ada35baba28134b.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c6ddd2c210450e.jpg (deflated 6%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad96a2881998657.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09df63bd01367ca3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a41cda5f44baaf6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac166d12e401a98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a94296ff543a1dc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aaad833ac61ac9d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0adc1330287b2e66.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f8e760f60df0da.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab2b64f27f8baca.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09e094375efab7fe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d64f43c7111879.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9ff75a7897e757.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ade7aef439e2102.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab050b51e78acdb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a02e8b6820064f5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a917bbca24cf75d.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a78374f2d3949ae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7c597abf1e90d4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1f4761dc7fe1eb.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a08a4711c728078.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aacbdb54e853a0a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a03326036647703.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a556c8163b58fae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a392d80c905a9df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9f183e46c76019.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac2f91a7995aa8b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a23d3f0e7d850f4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1bd356f90aaab6.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a02c648d24f39fb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aad9fc79a35bd53.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c7f89055cf399b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f8b77a88f224d9.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aa206fa7ea80036.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab5c690eebfad95.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09cbba9f5e097a19.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a2c6ef66896fb92.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7ffd65766a4741.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09dcb9b52055d40f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a4db5693da70448.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a73064c82730ff5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a47e7d602855f93.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d354dbd3dcc857.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c67960e389e4df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c5b4d6bc25788d.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7074a2a5515531.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d45c49c4adbae4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad99d610a9092e6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac3c1db1b3645f2.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a37aa0734ac8016.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09ebcee57699eb98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d2112596d9155b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0adc373e996aadc2.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d8aa2d19ff724d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c993afacd01547.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a4abf0a8071b917.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a563d05ebab4fe3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a13dcaaab9a35e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f531fe4f6d95f3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a82f0443c940816.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a50911d08250183.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c863d76bcf6b00.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a278d979b63fc72.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aae34863935e33a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a34d80ee1db201e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac51477636a6933.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac52440f73b5c80.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3c01759e77a02d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a877314ca2039d9.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7d56b2fb989fe8.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a8657e8b5c9d7bb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a72fef43a51c479.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7fbc1d68e4e5ae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a39325e5ad7f5a0.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7f4d9a0ccb9afe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a600f1148d1023c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a599940d33b6b2b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9f73b3c2557150.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09fa093bcd300c1a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a120822d362dddf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a5972c68b6bb265.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09e617d9d3120b32.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7be0b883a12966.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0add91a2efb3f33d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a6a03c8f23ee744.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab10a6417ef2301.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3f577a327ca7cc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09ea349ee555b61d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7f13330a5d0023.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09dd0671cd633432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a6bc386b28f2aac.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3f9b3d57ef354a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aa3a6c33fca122b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad7884032419621.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad7bad30cd432df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3873442ad329c2.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1b11867383b13e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad602c943c9f568.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0acfa779589204bf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000303653.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000205834.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000406997.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000255824.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000356347.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000522393.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000361180.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000119445.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000166259.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000350405.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000057672.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000299720.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000517069.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000348045.png (deflated 30%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000237928.png (deflated 11%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000110638.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000569273.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000323895.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000403385.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000299355.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000348481.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000175387.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000052507.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000185599.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000154358.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000098392.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000335529.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000256775.png (deflated 11%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000064898.png (deflated 35%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000166563.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000018380.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000491464.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000231169.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000128658.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000452122.png (deflated 27%)\n",
            "  adding: content/taming-transformers/data/flickr_tags.txt (deflated 56%)\n",
            "  adding: content/taming-transformers/data/ade20k_examples.txt (deflated 79%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000126.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000636.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001412.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001498.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000603.png (deflated 10%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000532.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000734.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001845.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001177.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000303.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000203.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000289.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000875.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000287.png (deflated 33%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001766.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001388.png (deflated 24%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001578.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000509.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001966.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000262.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001851.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001947.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001583.png (deflated 16%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000123.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000880.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001200.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000573.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000125.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001698.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001209.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/instances_train2017.json (deflated 70%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/instances_val2017.json (deflated 70%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/stuff_train2017.json (deflated 54%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/stuff_val2017.json (deflated 54%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016598.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018150.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013546.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013774.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014007.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016439.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012120.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015660.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012062.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018193.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015335.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013177.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014038.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010092.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018737.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015440.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019924.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016451.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017031.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015079.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016958.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014439.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010707.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018491.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016502.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019042.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020059.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017436.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013201.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013348.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018575.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015254.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015751.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017182.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010995.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017029.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013597.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017379.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016249.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018837.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015746.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017905.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017627.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018833.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016010.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011051.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020333.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018519.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015272.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019402.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011511.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017714.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013659.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010363.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017207.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017178.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016228.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013004.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019221.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012639.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019109.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012667.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012670.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011149.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010977.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015517.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011813.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011197.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013291.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014831.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013729.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011699.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011615.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010583.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020247.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014888.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012576.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015597.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011760.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014226.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019742.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019786.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015278.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015956.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017959.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015338.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012748.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017115.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010764.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017899.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015497.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018770.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012280.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013923.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020107.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014473.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010145.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010229.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010014.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010123.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010343.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010179.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010400.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010058.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010309.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010313.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010056.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010142.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010012.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010463.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010449.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010430.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010393.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010161.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010290.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010241.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010114.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010276.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010176.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010440.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010219.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010073.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010281.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010040.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010039.jpg (deflated 10%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010024.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010107.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010321.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010239.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010244.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010248.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010046.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010232.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010324.jpg (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010005.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010407.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010358.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010108.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010369.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010327.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010405.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010115.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010211.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010386.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010249.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010444.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010256.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010008.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010205.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010319.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010192.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010230.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010216.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010136.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010442.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010175.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010265.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010275.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010337.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010243.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010346.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010023.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010138.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010041.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010421.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010130.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010434.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010428.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010166.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010097.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010094.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010125.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010083.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010015.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010403.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010414.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010263.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010388.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010077.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010318.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010342.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010069.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010420.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010104.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010037.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010149.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010445.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010082.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010222.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010084.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010217.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010196.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010245.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010395.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ffhqtrain.txt (deflated 70%)\n",
            "  adding: content/taming-transformers/data/celebahqtrain.txt (deflated 80%)\n",
            "  adding: content/taming-transformers/data/subreddits.txt (deflated 37%)\n",
            "  adding: content/taming-transformers/data/ffhqvalidation.txt (deflated 69%)\n",
            "  adding: content/taming-transformers/data/coco_examples.txt (deflated 71%)\n",
            "  adding: content/taming-transformers/data/coco_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000403385.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000064898.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000299720.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000569273.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000166563.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000205834.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000348045.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000237928.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000356347.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000350405.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000303653.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000119445.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000154358.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000522393.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000231169.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000018380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000361180.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000323895.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000452122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000491464.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000335529.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000110638.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000348481.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000098392.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000057672.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000185599.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000255824.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000256775.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000175387.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000166259.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000517069.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000299355.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000406997.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000128658.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000052507.jpg (deflated 7%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/newzealand_np/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/newzealand_np/7942812194_9348729b93_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/4748115806_7219c2b3be_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/14996762478_a9bdbf959a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/3542389801_a2cbfee1e1_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/26320755536_7c769b6218_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/meadow/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/meadow/18864473291_844325caab_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/volcano/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/volcano/50254383883_27ed6ea93a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/desert/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/desert/4534149722_3cc4f92891_b.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/25735082181_999927fe5a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/20099378793_cc2df820af_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ireland/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ireland/15570753471_74db396d14_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_mountains/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_mountains/33509672006_bf4c416afd_b.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/mongolia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/mongolia/6076373946_e9ea2aee32_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/cliff_ocean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/cliff_ocean/36142796444_45d452f567_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/8364557382_c6c9ee2fd6_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/44974691685_8e7372e2b1_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/14351041152_ef77484a1f_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/18176301_c9d27557cf_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/alaska_lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/alaska_lakes/43259216952_59352d7204_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_landscape/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_landscape/4079319632_0133685b2c_b.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/12822389285_a7723081b5_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/8720651218_ca82a6608e_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/256743165_9f130ba95b_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/2883773_881c197107_c.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/lakes/39933489595_f0e5d85b6d_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/newzealand_np/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/newzealand_np/7942812194_9348729b93_b.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/3542389801_a2cbfee1e1_b.png (deflated 14%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/26320755536_7c769b6218_b.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/14996762478_a9bdbf959a_b.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/4748115806_7219c2b3be_b.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/meadow/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/meadow/18864473291_844325caab_b.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/volcano/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/volcano/50254383883_27ed6ea93a_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/desert/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/desert/4534149722_3cc4f92891_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/25735082181_999927fe5a_b.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/20099378793_cc2df820af_b.png (deflated 10%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ireland/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ireland/15570753471_74db396d14_b.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_mountains/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_mountains/33509672006_bf4c416afd_b.png (deflated 32%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/mongolia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/mongolia/6076373946_e9ea2aee32_b.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/cliff_ocean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/cliff_ocean/36142796444_45d452f567_b.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/8364557382_c6c9ee2fd6_b.png (deflated 43%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/44974691685_8e7372e2b1_b.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/14351041152_ef77484a1f_b.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/18176301_c9d27557cf_b.png (deflated 28%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/alaska_lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/alaska_lakes/43259216952_59352d7204_b.png (deflated 29%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_landscape/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_landscape/4079319632_0133685b2c_b.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/12822389285_a7723081b5_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/8720651218_ca82a6608e_b.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/2883773_881c197107_c.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/256743165_9f130ba95b_b.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/lakes/39933489595_f0e5d85b6d_b.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/drin_examples.txt (deflated 75%)\n",
            "  adding: content/taming-transformers/data/drin_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ILSVRC2012_val_00032333.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ILSVRC2012_val_00047325.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02085782/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02085782/ILSVRC2012_val_00012298.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02111889/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02111889/ILSVRC2012_val_00042625.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00046802.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00001336.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00008236.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02102318/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02102318/ILSVRC2012_val_00024691.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02086646/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02086646/ILSVRC2012_val_00011473.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02089973/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02089973/ILSVRC2012_val_00000028.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02105505/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02105505/ILSVRC2012_val_00031252.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ILSVRC2012_val_00034784.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ILSVRC2012_val_00047491.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02088466/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02088466/ILSVRC2012_val_00013651.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02096294/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02096294/ILSVRC2012_val_00042133.JPEG (deflated 3%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02110627/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02110627/ILSVRC2012_val_00008310.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01847000/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01847000/ILSVRC2012_val_00022364.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02093256/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02093256/ILSVRC2012_val_00046547.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099712/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099712/ILSVRC2012_val_00023471.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01795545/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01795545/ILSVRC2012_val_00023344.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101556/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101556/ILSVRC2012_val_00030540.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02100877/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02100877/ILSVRC2012_val_00039863.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099601/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099601/ILSVRC2012_val_00005697.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01819313/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01819313/ILSVRC2012_val_00003068.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01843065/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01843065/ILSVRC2012_val_00022439.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/scripts/ (stored 0%)\n",
            "  adding: content/taming-transformers/scripts/taming-transformers.ipynb (deflated 25%)\n",
            "  adding: content/taming-transformers/scripts/make_scene_samples.py (deflated 67%)\n",
            "  adding: content/taming-transformers/scripts/sample_conditional.py (deflated 68%)\n",
            "  adding: content/taming-transformers/scripts/extract_submodel.py (deflated 49%)\n",
            "  adding: content/taming-transformers/scripts/extract_segmentation.py (deflated 60%)\n",
            "  adding: content/taming-transformers/scripts/reconstruction_usage.ipynb (deflated 25%)\n",
            "  adding: content/taming-transformers/scripts/sample_fast.py (deflated 70%)\n",
            "  adding: content/taming-transformers/scripts/make_samples.py (deflated 69%)\n",
            "  adding: content/taming-transformers/scripts/extract_depth.py (deflated 63%)\n",
            "  adding: content/taming-transformers/README.md (deflated 65%)\n",
            "  adding: content/taming-transformers/main.py (deflated 74%)\n",
            "  adding: content/taming-transformers/setup.py (deflated 39%)\n",
            "  adding: content/taming-transformers/assets/ (stored 0%)\n",
            "  adding: content/taming-transformers/assets/coco_scene_images_training.svg (deflated 85%)\n",
            "  adding: content/taming-transformers/assets/teaser.png (deflated 1%)\n",
            "  adding: content/taming-transformers/assets/lake_in_the_mountains.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/drin.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/assets/faceshq.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/imagenet.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/stormy.jpeg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/sunset_and_ocean.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/mountain.jpeg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/scene_images_samples.svg (deflated 25%)\n",
            "  adding: content/taming-transformers/assets/first_stage_mushrooms.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/first_stage_squirrels.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/birddrawnbyachild.png (deflated 0%)\n",
            "  adding: content/EfficientDM/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/HEAD (stored 0%)\n",
            "  adding: content/EfficientDM/.git/branches/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/info/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/info/exclude (deflated 28%)\n",
            "  adding: content/EfficientDM/.git/refs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/heads/main (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/EfficientDM/.git/hooks/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/EfficientDM/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/EfficientDM/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/EfficientDM/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/EfficientDM/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/EfficientDM/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/EfficientDM/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/EfficientDM/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/EfficientDM/.git/index (deflated 55%)\n",
            "  adding: content/EfficientDM/.git/logs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/HEAD (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/heads/main (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/origin/HEAD (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/packed-refs (deflated 9%)\n",
            "  adding: content/EfficientDM/.git/description (deflated 14%)\n",
            "  adding: content/EfficientDM/.git/objects/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/info/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/pack-981733d82cdf3418a37a580efdd4179c422f0000.idx (deflated 18%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/pack-981733d82cdf3418a37a580efdd4179c422f0000.pack (deflated 0%)\n",
            "  adding: content/EfficientDM/.git/config (deflated 31%)\n",
            "  adding: content/EfficientDM/configs/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/cin-ldm-vq-f8.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/lsun_bedrooms-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/cin256-v2.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/lsun_churches-ldm-kl-8.yaml (deflated 60%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/txt2img-1p4B-eval.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/celebahq-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/ffhq-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_8x8x64.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_32x32x4.yaml (deflated 55%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_64x64x3.yaml (deflated 55%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_16x16x16.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/configs/retrieval-augmented-diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/retrieval-augmented-diffusion/768x768.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis512/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis512/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ldm/layout2img-openimages256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/layout2img-openimages256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_beds256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_beds256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/cin256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/cin256/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/ffhq256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/ffhq256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/inpainting_big/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/inpainting_big/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/bsr_sr/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/bsr_sr/config.yaml (deflated 67%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_churches256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_churches256/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/celeba256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/celeba256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/text2img256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/text2img256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8-n256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8-n256/config.yaml (deflated 56%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4-noattn/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4-noattn/config.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f16/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f16/config.yaml (deflated 58%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f32/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f32/config.yaml (deflated 58%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f4/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f4/config.yaml (deflated 56%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f8/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f8/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4/config.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f16/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f16/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/ldm/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/autoencoder.py (deflated 80%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ddpm.py (deflated 79%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ddim.py (deflated 86%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/plms.py (deflated 76%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ddpm.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/__init__.cpython-310.pyc (deflated 24%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ddim.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/classifier.py (deflated 71%)\n",
            "  adding: content/EfficientDM/ldm/models/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/__pycache__/autoencoder.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/EfficientDM/ldm/globalvar.py (deflated 61%)\n",
            "  adding: content/EfficientDM/ldm/data/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/data/base.py (deflated 54%)\n",
            "  adding: content/EfficientDM/ldm/data/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/data/imagenet.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/data/lsun.py (deflated 72%)\n",
            "  adding: content/EfficientDM/ldm/lr_scheduler.py (deflated 80%)\n",
            "  adding: content/EfficientDM/ldm/modules/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/modules.py (deflated 70%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/__init__.cpython-310.pyc (deflated 26%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/modules.cpython-310.pyc (deflated 55%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/contperceptual.py (deflated 76%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/vqperceptual.py (deflated 74%)\n",
            "  adding: content/EfficientDM/ldm/modules/ema.py (deflated 68%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/distributions.py (deflated 66%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/__init__.cpython-310.pyc (deflated 25%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/distributions.cpython-310.pyc (deflated 50%)\n",
            "  adding: content/EfficientDM/ldm/modules/attention.py (deflated 74%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/openaimodel.py (deflated 82%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/model.py (deflated 85%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/__init__.cpython-310.pyc (deflated 29%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/model.cpython-310.pyc (deflated 57%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/openaimodel.cpython-310.pyc (deflated 58%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/util.cpython-310.pyc (deflated 50%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/util.py (deflated 66%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/bsrgan.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/__init__.py (deflated 60%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils/test.png (deflated 1%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils_image.py (deflated 77%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/bsrgan_light.py (deflated 73%)\n",
            "  adding: content/EfficientDM/ldm/modules/x_transformer.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/x_transformer.cpython-310.pyc (deflated 52%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/attention.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/ema.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/util.cpython-310.pyc (deflated 43%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/globalvar.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/EfficientDM/ldm/util.py (deflated 62%)\n",
            "  adding: content/EfficientDM/LICENSE (deflated 41%)\n",
            "  adding: content/EfficientDM/.gitignore (deflated 4%)\n",
            "  adding: content/EfficientDM/quant_scripts/ (stored 0%)\n",
            "  adding: content/EfficientDM/quant_scripts/quantize_ldm_naive.py (deflated 60%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_layer.py (deflated 82%)\n",
            "  adding: content/EfficientDM/quant_scripts/collect_input_4_calib.py (deflated 61%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_dataset.py (deflated 78%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_model.py (deflated 84%)\n",
            "  adding: content/EfficientDM/quant_scripts/train_efficientdm.py (deflated 71%)\n",
            "  adding: content/EfficientDM/quant_scripts/save_naive_2_intmodel.py (deflated 62%)\n",
            "  adding: content/EfficientDM/quant_scripts/downsample_talsq_ckpt.py (deflated 41%)\n",
            "  adding: content/EfficientDM/quant_scripts/sample_lora_intmodel.py (deflated 64%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/quant_model.cpython-310.pyc (deflated 59%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/quant_layer.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/EfficientDM/scripts/ (stored 0%)\n",
            "  adding: content/EfficientDM/scripts/download_models.sh (deflated 80%)\n",
            "  adding: content/EfficientDM/scripts/sample_diffusion.py (deflated 68%)\n",
            "  adding: content/EfficientDM/scripts/txt2img.py (deflated 67%)\n",
            "  adding: content/EfficientDM/scripts/download_first_stages.sh (deflated 84%)\n",
            "  adding: content/EfficientDM/scripts/inpaint.py (deflated 65%)\n",
            "  adding: content/EfficientDM/scripts/knn2img.py (deflated 71%)\n",
            "  adding: content/EfficientDM/scripts/generate_samples_4_evaluation_FP.py (deflated 69%)\n",
            "  adding: content/EfficientDM/scripts/latent_imagenet_diffusion.ipynb (deflated 25%)\n",
            "  adding: content/EfficientDM/scripts/train_searcher.py (deflated 68%)\n",
            "  adding: content/EfficientDM/README.md (deflated 51%)\n",
            "  adding: content/sample_data/ (stored 0%)\n",
            "  adding: content/sample_data/anscombe.json (deflated 83%)\n",
            "  adding: content/sample_data/README.md (deflated 39%)\n",
            "  adding: content/sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "  adding: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "  adding: content/sample_data/mnist_test.csv (deflated 88%)\n",
            "  adding: content/ (stored 0%)\n",
            "  adding: content/.config/ (stored 0%)\n",
            "  adding: content/.config/gce (stored 0%)\n",
            "  adding: content/.config/.last_update_check.json (deflated 24%)\n",
            "  adding: content/.config/configurations/ (stored 0%)\n",
            "  adding: content/.config/configurations/config_default (deflated 15%)\n",
            "  adding: content/.config/logs/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.11.07/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.09.372862.log (deflated 58%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.26.532854.log (deflated 57%)\n",
            "  adding: content/.config/logs/2024.11.07/20.55.50.913514.log (deflated 57%)\n",
            "  adding: content/.config/logs/2024.11.07/20.55.26.763095.log (deflated 92%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.27.350325.log (deflated 56%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.08.122576.log (deflated 85%)\n",
            "  adding: content/.config/default_configs.db (deflated 98%)\n",
            "  adding: content/.config/active_config (stored 0%)\n",
            "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
            "  adding: content/.config/config_sentinel (stored 0%)\n",
            "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "  adding: content/DiffusionInput_250steps.pth (deflated 68%)\n",
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/ldm/ (stored 0%)\n",
            "  adding: content/models/ldm/cin256-v2/ (stored 0%)\n",
            "  adding: content/models/ldm/cin256-v2/model.ckpt (deflated 7%)\n",
            "  adding: content/models.zip (stored 0%)\n",
            "  adding: content/EfficientDM.zip (stored 0%)\n",
            "  adding: content/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/taming-transformers/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/HEAD (stored 0%)\n",
            "  adding: content/taming-transformers/.git/branches/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/info/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/info/exclude (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/refs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/heads/master (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/taming-transformers/.git/hooks/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/taming-transformers/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/taming-transformers/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/taming-transformers/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/taming-transformers/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/taming-transformers/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/taming-transformers/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/taming-transformers/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/taming-transformers/.git/index (deflated 63%)\n",
            "  adding: content/taming-transformers/.git/logs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/HEAD (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/heads/master (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/origin/HEAD (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/packed-refs (deflated 42%)\n",
            "  adding: content/taming-transformers/.git/description (deflated 14%)\n",
            "  adding: content/taming-transformers/.git/objects/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/info/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/pack-34e24108d627c8d57cba87fd86552b8cca48134a.pack (deflated 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/pack-34e24108d627c8d57cba87fd86552b8cca48134a.idx (deflated 2%)\n",
            "  adding: content/taming-transformers/.git/config (deflated 33%)\n",
            "  adding: content/taming-transformers/environment.yaml (deflated 45%)\n",
            "  adding: content/taming-transformers/configs/ (stored 0%)\n",
            "  adding: content/taming-transformers/configs/coco_cond_stage.yaml (deflated 60%)\n",
            "  adding: content/taming-transformers/configs/faceshq_vqgan.yaml (deflated 56%)\n",
            "  adding: content/taming-transformers/configs/imagenetdepth_vqgan.yaml (deflated 54%)\n",
            "  adding: content/taming-transformers/configs/faceshq_transformer.yaml (deflated 64%)\n",
            "  adding: content/taming-transformers/configs/imagenet_vqgan.yaml (deflated 55%)\n",
            "  adding: content/taming-transformers/configs/drin_transformer.yaml (deflated 72%)\n",
            "  adding: content/taming-transformers/configs/coco_scene_images_transformer.yaml (deflated 67%)\n",
            "  adding: content/taming-transformers/configs/custom_vqgan.yaml (deflated 55%)\n",
            "  adding: content/taming-transformers/configs/sflckr_cond_stage.yaml (deflated 56%)\n",
            "  adding: content/taming-transformers/configs/open_images_scene_images_transformer.yaml (deflated 70%)\n",
            "  adding: content/taming-transformers/License.txt (deflated 41%)\n",
            "  adding: content/taming-transformers/taming/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/models/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/models/cond_transformer.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/models/dummy_cond_stage.py (deflated 58%)\n",
            "  adding: content/taming-transformers/taming/models/vqgan.py (deflated 85%)\n",
            "  adding: content/taming-transformers/taming/data/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/data/utils.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/data/coco.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_open_images.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_dataset.py (deflated 76%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_coco.py (deflated 72%)\n",
            "  adding: content/taming-transformers/taming/data/base.py (deflated 67%)\n",
            "  adding: content/taming-transformers/taming/data/custom.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/data/image_transforms.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/data/faceshq.py (deflated 82%)\n",
            "  adding: content/taming-transformers/taming/data/imagenet.py (deflated 78%)\n",
            "  adding: content/taming-transformers/taming/data/sflckr.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/utils.py (deflated 66%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/objects_bbox.py (deflated 64%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/objects_center_points.py (deflated 71%)\n",
            "  adding: content/taming-transformers/taming/data/open_images_helper.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/data/helper_types.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/data/ade20k.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/lr_scheduler.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/modules/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/segmentation.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/lpips.py (deflated 69%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/__init__.py (deflated 2%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/vqperceptual.py (deflated 76%)\n",
            "  adding: content/taming-transformers/taming/modules/discriminator/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/discriminator/model.py (deflated 64%)\n",
            "  adding: content/taming-transformers/taming/modules/diffusionmodules/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/diffusionmodules/model.py (deflated 89%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/quantize.py (deflated 78%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/__pycache__/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/__pycache__/quantize.cpython-310.pyc (deflated 51%)\n",
            "  adding: content/taming-transformers/taming/modules/misc/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/misc/coord.py (deflated 62%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/permuter.py (deflated 83%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/mingpt.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/modules/util.py (deflated 71%)\n",
            "  adding: content/taming-transformers/taming/util.py (deflated 64%)\n",
            "  adding: content/taming-transformers/data/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/celebahqvalidation.txt (deflated 80%)\n",
            "  adding: content/taming-transformers/data/sflckr_examples.txt (deflated 47%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001498.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000509.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000125.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001845.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000573.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001578.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000734.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001583.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001698.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000203.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001388.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001412.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000880.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000532.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000126.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001766.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000123.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000875.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000289.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001209.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000262.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001177.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000636.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001966.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000287.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000603.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001851.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001200.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001947.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ILSVRC2012_val_00032333.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ILSVRC2012_val_00047325.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02085782/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02085782/ILSVRC2012_val_00012298.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02111889/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02111889/ILSVRC2012_val_00042625.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00046802.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00001336.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00008236.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02102318/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02102318/ILSVRC2012_val_00024691.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02086646/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02086646/ILSVRC2012_val_00011473.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02089973/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02089973/ILSVRC2012_val_00000028.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02105505/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02105505/ILSVRC2012_val_00031252.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ILSVRC2012_val_00047491.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ILSVRC2012_val_00034784.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02088466/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02088466/ILSVRC2012_val_00013651.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02096294/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02096294/ILSVRC2012_val_00042133.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02110627/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02110627/ILSVRC2012_val_00008310.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01847000/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01847000/ILSVRC2012_val_00022364.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02093256/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02093256/ILSVRC2012_val_00046547.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099712/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099712/ILSVRC2012_val_00023471.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01795545/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01795545/ILSVRC2012_val_00023344.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101556/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101556/ILSVRC2012_val_00030540.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02100877/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02100877/ILSVRC2012_val_00039863.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099601/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099601/ILSVRC2012_val_00005697.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01819313/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01819313/ILSVRC2012_val_00003068.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01843065/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01843065/ILSVRC2012_val_00022439.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation-annotations-bbox.csv (deflated 77%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation-images.csv (deflated 10%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train-images-boxable.csv (deflated 89%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/class-descriptions-boxable.csv (deflated 53%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000afe7726e121ea.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba40bf7a2b458.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b59a7822679e6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1b3b85edd850.jpg (deflated 9%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc5ad4cc3ae73.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b5bc07c0c5df7.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4671075914cd.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b09d5d3fc821f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b38d9f2f664fe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1971d8daaeef.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc75d38907c78.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bcd3bcd95cbb3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000af180a3163f17.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adef7197e3118.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab31e6be35fed.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b3940e7d25c03.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abc075d659122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9c365c9e307a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b567c26dd4e5d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b825dea3016eb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9a97776b3634.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b397382b2464a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad0ecfb21ee63.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb0ae453283b0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4935979bf4b5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba3ca8a2ca955.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000aee0af66d4237.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000acf666d991c39.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9b00d7aef8f5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000baa6f7dae9b79.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad3d42653f5f6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba28d70b1a999.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b393437134262.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b299b5f5ed902.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000aecd78b230135.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9b61afea2cd4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b7dfaa1810a83.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac8c676b6077a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bbdf0dc8099d8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab7bec71cc50a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc7b0a1889bcb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9f3ba4891c11.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2a982a903d0d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ae235808cc1e8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b260e1f08a32a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b8d80f7386698.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b50bdd1933a36.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b06c0eed42a4c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb200fc78fc30.jpg (deflated 6%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad20b5e452b24.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba940f8cfc9bf.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bcee5bed5446b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abc821f66a892.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b21663becc68e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc387c731dd97.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b168e791f591d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba221f70676c6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b72e1446f8849.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b432ae644b679.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b093da01e5bfe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b76a9b80ba43a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ae28755d2d20e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac34008b0ba4c.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc1eb7f74adae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b0f5159f54105.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb81adefe7332.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b70a84aab664b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b63a1445f53c8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad6c520be9ec5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b55559b0244d7.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2b00065e564a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b65a36ad46f9e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adfe5b817011c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4fcdf1af3361.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb8bd9b1bca65.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adcdd7244ce4a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b93644609911f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9007a01f7405.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b485cedacbf97.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b42cae15622e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac95750ac7399.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bab5b1a67844e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9814a07fd974.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab8c20b3e5b58.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b606e130bdf5e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc5006eb7fd98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b81b5757963e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b87119cc301cf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b29496f75c8e5.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abe5eddc5b303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9d6c0f7d794d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2d1789d5f80d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc33717a6371f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b55e339f0b131.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000af631fb329557.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b0f235dcf2caa.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb2f7132013dc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb846e2629e83.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1b92f0800e94.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad6fa67b5ad96.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/oidv6-train-annotations-bbox.csv (deflated 76%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ada35baba28134b.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c6ddd2c210450e.jpg (deflated 6%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad96a2881998657.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09df63bd01367ca3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a41cda5f44baaf6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac166d12e401a98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a94296ff543a1dc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aaad833ac61ac9d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0adc1330287b2e66.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f8e760f60df0da.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab2b64f27f8baca.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09e094375efab7fe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d64f43c7111879.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9ff75a7897e757.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ade7aef439e2102.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab050b51e78acdb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a02e8b6820064f5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a917bbca24cf75d.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a78374f2d3949ae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7c597abf1e90d4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1f4761dc7fe1eb.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a08a4711c728078.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aacbdb54e853a0a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a03326036647703.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a556c8163b58fae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a392d80c905a9df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9f183e46c76019.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac2f91a7995aa8b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a23d3f0e7d850f4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1bd356f90aaab6.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a02c648d24f39fb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aad9fc79a35bd53.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c7f89055cf399b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f8b77a88f224d9.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aa206fa7ea80036.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab5c690eebfad95.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09cbba9f5e097a19.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a2c6ef66896fb92.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7ffd65766a4741.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09dcb9b52055d40f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a4db5693da70448.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a73064c82730ff5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a47e7d602855f93.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d354dbd3dcc857.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c67960e389e4df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c5b4d6bc25788d.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7074a2a5515531.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d45c49c4adbae4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad99d610a9092e6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac3c1db1b3645f2.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a37aa0734ac8016.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09ebcee57699eb98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d2112596d9155b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0adc373e996aadc2.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d8aa2d19ff724d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c993afacd01547.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a4abf0a8071b917.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a563d05ebab4fe3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a13dcaaab9a35e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f531fe4f6d95f3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a82f0443c940816.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a50911d08250183.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c863d76bcf6b00.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a278d979b63fc72.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aae34863935e33a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a34d80ee1db201e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac51477636a6933.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac52440f73b5c80.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3c01759e77a02d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a877314ca2039d9.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7d56b2fb989fe8.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a8657e8b5c9d7bb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a72fef43a51c479.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7fbc1d68e4e5ae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a39325e5ad7f5a0.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7f4d9a0ccb9afe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a600f1148d1023c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a599940d33b6b2b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9f73b3c2557150.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09fa093bcd300c1a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a120822d362dddf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a5972c68b6bb265.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09e617d9d3120b32.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7be0b883a12966.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0add91a2efb3f33d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a6a03c8f23ee744.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab10a6417ef2301.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3f577a327ca7cc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09ea349ee555b61d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7f13330a5d0023.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09dd0671cd633432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a6bc386b28f2aac.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3f9b3d57ef354a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aa3a6c33fca122b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad7884032419621.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad7bad30cd432df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3873442ad329c2.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1b11867383b13e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad602c943c9f568.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0acfa779589204bf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000303653.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000205834.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000406997.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000255824.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000356347.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000522393.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000361180.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000119445.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000166259.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000350405.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000057672.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000299720.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000517069.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000348045.png (deflated 30%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000237928.png (deflated 11%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000110638.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000569273.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000323895.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000403385.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000299355.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000348481.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000175387.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000052507.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000185599.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000154358.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000098392.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000335529.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000256775.png (deflated 11%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000064898.png (deflated 35%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000166563.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000018380.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000491464.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000231169.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000128658.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000452122.png (deflated 27%)\n",
            "  adding: content/taming-transformers/data/flickr_tags.txt (deflated 56%)\n",
            "  adding: content/taming-transformers/data/ade20k_examples.txt (deflated 79%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000126.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000636.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001412.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001498.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000603.png (deflated 10%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000532.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000734.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001845.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001177.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000303.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000203.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000289.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000875.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000287.png (deflated 33%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001766.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001388.png (deflated 24%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001578.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000509.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001966.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000262.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001851.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001947.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001583.png (deflated 16%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000123.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000880.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001200.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000573.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000125.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001698.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001209.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/instances_train2017.json (deflated 70%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/instances_val2017.json (deflated 70%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/stuff_train2017.json (deflated 54%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/stuff_val2017.json (deflated 54%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016598.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018150.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013546.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013774.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014007.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016439.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012120.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015660.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012062.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018193.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015335.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013177.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014038.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010092.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018737.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015440.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019924.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016451.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017031.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015079.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016958.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014439.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010707.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018491.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016502.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019042.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020059.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017436.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013201.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013348.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018575.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015254.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015751.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017182.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010995.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017029.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013597.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017379.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016249.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018837.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015746.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017905.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017627.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018833.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016010.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011051.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020333.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018519.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015272.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019402.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011511.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017714.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013659.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010363.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017207.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017178.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016228.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013004.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019221.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012639.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019109.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012667.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012670.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011149.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010977.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015517.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011813.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011197.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013291.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014831.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013729.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011699.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011615.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010583.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020247.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014888.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012576.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015597.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011760.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014226.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019742.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019786.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015278.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015956.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017959.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015338.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012748.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017115.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010764.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017899.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015497.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018770.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012280.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013923.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020107.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014473.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010145.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010229.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010014.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010123.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010343.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010179.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010400.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010058.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010309.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010313.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010056.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010142.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010012.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010463.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010449.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010430.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010393.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010161.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010290.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010241.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010114.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010276.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010176.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010440.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010219.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010073.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010281.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010040.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010039.jpg (deflated 10%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010024.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010107.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010321.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010239.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010244.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010248.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010046.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010232.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010324.jpg (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010005.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010407.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010358.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010108.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010369.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010327.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010405.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010115.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010211.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010386.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010249.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010444.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010256.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010008.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010205.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010319.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010192.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010230.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010216.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010136.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010442.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010175.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010265.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010275.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010337.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010243.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010346.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010023.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010138.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010041.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010421.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010130.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010434.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010428.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010166.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010097.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010094.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010125.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010083.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010015.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010403.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010414.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010263.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010388.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010077.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010318.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010342.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010069.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010420.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010104.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010037.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010149.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010445.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010082.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010222.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010084.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010217.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010196.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010245.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010395.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ffhqtrain.txt (deflated 70%)\n",
            "  adding: content/taming-transformers/data/celebahqtrain.txt (deflated 80%)\n",
            "  adding: content/taming-transformers/data/subreddits.txt (deflated 37%)\n",
            "  adding: content/taming-transformers/data/ffhqvalidation.txt (deflated 69%)\n",
            "  adding: content/taming-transformers/data/coco_examples.txt (deflated 71%)\n",
            "  adding: content/taming-transformers/data/coco_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000403385.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000064898.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000299720.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000569273.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000166563.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000205834.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000348045.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000237928.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000356347.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000350405.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000303653.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000119445.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000154358.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000522393.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000231169.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000018380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000361180.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000323895.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000452122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000491464.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000335529.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000110638.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000348481.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000098392.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000057672.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000185599.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000255824.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000256775.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000175387.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000166259.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000517069.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000299355.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000406997.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000128658.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000052507.jpg (deflated 7%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/newzealand_np/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/newzealand_np/7942812194_9348729b93_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/4748115806_7219c2b3be_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/14996762478_a9bdbf959a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/3542389801_a2cbfee1e1_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/26320755536_7c769b6218_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/meadow/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/meadow/18864473291_844325caab_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/volcano/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/volcano/50254383883_27ed6ea93a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/desert/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/desert/4534149722_3cc4f92891_b.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/25735082181_999927fe5a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/20099378793_cc2df820af_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ireland/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ireland/15570753471_74db396d14_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_mountains/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_mountains/33509672006_bf4c416afd_b.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/mongolia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/mongolia/6076373946_e9ea2aee32_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/cliff_ocean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/cliff_ocean/36142796444_45d452f567_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/8364557382_c6c9ee2fd6_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/44974691685_8e7372e2b1_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/14351041152_ef77484a1f_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/18176301_c9d27557cf_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/alaska_lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/alaska_lakes/43259216952_59352d7204_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_landscape/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_landscape/4079319632_0133685b2c_b.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/12822389285_a7723081b5_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/8720651218_ca82a6608e_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/256743165_9f130ba95b_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/2883773_881c197107_c.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/lakes/39933489595_f0e5d85b6d_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/newzealand_np/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/newzealand_np/7942812194_9348729b93_b.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/3542389801_a2cbfee1e1_b.png (deflated 14%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/26320755536_7c769b6218_b.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/14996762478_a9bdbf959a_b.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/4748115806_7219c2b3be_b.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/meadow/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/meadow/18864473291_844325caab_b.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/volcano/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/volcano/50254383883_27ed6ea93a_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/desert/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/desert/4534149722_3cc4f92891_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/25735082181_999927fe5a_b.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/20099378793_cc2df820af_b.png (deflated 10%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ireland/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ireland/15570753471_74db396d14_b.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_mountains/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_mountains/33509672006_bf4c416afd_b.png (deflated 32%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/mongolia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/mongolia/6076373946_e9ea2aee32_b.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/cliff_ocean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/cliff_ocean/36142796444_45d452f567_b.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/8364557382_c6c9ee2fd6_b.png (deflated 43%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/44974691685_8e7372e2b1_b.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/14351041152_ef77484a1f_b.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/18176301_c9d27557cf_b.png (deflated 28%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/alaska_lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/alaska_lakes/43259216952_59352d7204_b.png (deflated 29%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_landscape/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_landscape/4079319632_0133685b2c_b.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/12822389285_a7723081b5_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/8720651218_ca82a6608e_b.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/2883773_881c197107_c.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/256743165_9f130ba95b_b.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/lakes/39933489595_f0e5d85b6d_b.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/drin_examples.txt (deflated 75%)\n",
            "  adding: content/taming-transformers/data/drin_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ILSVRC2012_val_00032333.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ILSVRC2012_val_00047325.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02085782/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02085782/ILSVRC2012_val_00012298.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02111889/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02111889/ILSVRC2012_val_00042625.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00046802.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00001336.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00008236.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02102318/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02102318/ILSVRC2012_val_00024691.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02086646/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02086646/ILSVRC2012_val_00011473.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02089973/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02089973/ILSVRC2012_val_00000028.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02105505/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02105505/ILSVRC2012_val_00031252.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ILSVRC2012_val_00034784.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ILSVRC2012_val_00047491.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02088466/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02088466/ILSVRC2012_val_00013651.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02096294/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02096294/ILSVRC2012_val_00042133.JPEG (deflated 3%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02110627/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02110627/ILSVRC2012_val_00008310.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01847000/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01847000/ILSVRC2012_val_00022364.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02093256/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02093256/ILSVRC2012_val_00046547.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099712/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099712/ILSVRC2012_val_00023471.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01795545/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01795545/ILSVRC2012_val_00023344.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101556/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101556/ILSVRC2012_val_00030540.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02100877/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02100877/ILSVRC2012_val_00039863.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099601/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099601/ILSVRC2012_val_00005697.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01819313/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01819313/ILSVRC2012_val_00003068.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01843065/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01843065/ILSVRC2012_val_00022439.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/scripts/ (stored 0%)\n",
            "  adding: content/taming-transformers/scripts/taming-transformers.ipynb (deflated 25%)\n",
            "  adding: content/taming-transformers/scripts/make_scene_samples.py (deflated 67%)\n",
            "  adding: content/taming-transformers/scripts/sample_conditional.py (deflated 68%)\n",
            "  adding: content/taming-transformers/scripts/extract_submodel.py (deflated 49%)\n",
            "  adding: content/taming-transformers/scripts/extract_segmentation.py (deflated 60%)\n",
            "  adding: content/taming-transformers/scripts/reconstruction_usage.ipynb (deflated 25%)\n",
            "  adding: content/taming-transformers/scripts/sample_fast.py (deflated 70%)\n",
            "  adding: content/taming-transformers/scripts/make_samples.py (deflated 69%)\n",
            "  adding: content/taming-transformers/scripts/extract_depth.py (deflated 63%)\n",
            "  adding: content/taming-transformers/README.md (deflated 65%)\n",
            "  adding: content/taming-transformers/main.py (deflated 74%)\n",
            "  adding: content/taming-transformers/setup.py (deflated 39%)\n",
            "  adding: content/taming-transformers/assets/ (stored 0%)\n",
            "  adding: content/taming-transformers/assets/coco_scene_images_training.svg (deflated 85%)\n",
            "  adding: content/taming-transformers/assets/teaser.png (deflated 1%)\n",
            "  adding: content/taming-transformers/assets/lake_in_the_mountains.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/drin.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/assets/faceshq.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/imagenet.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/stormy.jpeg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/sunset_and_ocean.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/mountain.jpeg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/scene_images_samples.svg (deflated 25%)\n",
            "  adding: content/taming-transformers/assets/first_stage_mushrooms.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/first_stage_squirrels.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/birddrawnbyachild.png (deflated 0%)\n",
            "  adding: content/EfficientDM/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/HEAD (stored 0%)\n",
            "  adding: content/EfficientDM/.git/branches/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/info/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/info/exclude (deflated 28%)\n",
            "  adding: content/EfficientDM/.git/refs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/heads/main (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/EfficientDM/.git/hooks/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/EfficientDM/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/EfficientDM/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/EfficientDM/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/EfficientDM/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/EfficientDM/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/EfficientDM/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/EfficientDM/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/EfficientDM/.git/index (deflated 55%)\n",
            "  adding: content/EfficientDM/.git/logs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/HEAD (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/heads/main (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/origin/HEAD (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/packed-refs (deflated 9%)\n",
            "  adding: content/EfficientDM/.git/description (deflated 14%)\n",
            "  adding: content/EfficientDM/.git/objects/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/info/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/pack-981733d82cdf3418a37a580efdd4179c422f0000.idx (deflated 18%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/pack-981733d82cdf3418a37a580efdd4179c422f0000.pack (deflated 0%)\n",
            "  adding: content/EfficientDM/.git/config (deflated 31%)\n",
            "  adding: content/EfficientDM/configs/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/cin-ldm-vq-f8.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/lsun_bedrooms-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/cin256-v2.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/lsun_churches-ldm-kl-8.yaml (deflated 60%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/txt2img-1p4B-eval.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/celebahq-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/ffhq-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_8x8x64.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_32x32x4.yaml (deflated 55%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_64x64x3.yaml (deflated 55%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_16x16x16.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/configs/retrieval-augmented-diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/retrieval-augmented-diffusion/768x768.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis512/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis512/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ldm/layout2img-openimages256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/layout2img-openimages256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_beds256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_beds256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/cin256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/cin256/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/ffhq256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/ffhq256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/inpainting_big/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/inpainting_big/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/bsr_sr/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/bsr_sr/config.yaml (deflated 67%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_churches256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_churches256/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/celeba256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/celeba256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/text2img256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/text2img256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8-n256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8-n256/config.yaml (deflated 56%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4-noattn/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4-noattn/config.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f16/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f16/config.yaml (deflated 58%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f32/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f32/config.yaml (deflated 58%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f4/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f4/config.yaml (deflated 56%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f8/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f8/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4/config.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f16/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f16/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/ldm/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/autoencoder.py (deflated 80%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ddpm.py (deflated 79%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ddim.py (deflated 86%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/plms.py (deflated 76%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ddpm.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/__init__.cpython-310.pyc (deflated 24%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ddim.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/classifier.py (deflated 71%)\n",
            "  adding: content/EfficientDM/ldm/models/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/__pycache__/autoencoder.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/EfficientDM/ldm/globalvar.py (deflated 61%)\n",
            "  adding: content/EfficientDM/ldm/data/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/data/base.py (deflated 54%)\n",
            "  adding: content/EfficientDM/ldm/data/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/data/imagenet.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/data/lsun.py (deflated 72%)\n",
            "  adding: content/EfficientDM/ldm/lr_scheduler.py (deflated 80%)\n",
            "  adding: content/EfficientDM/ldm/modules/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/modules.py (deflated 70%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/__init__.cpython-310.pyc (deflated 26%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/modules.cpython-310.pyc (deflated 55%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/contperceptual.py (deflated 76%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/vqperceptual.py (deflated 74%)\n",
            "  adding: content/EfficientDM/ldm/modules/ema.py (deflated 68%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/distributions.py (deflated 66%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/__init__.cpython-310.pyc (deflated 25%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/distributions.cpython-310.pyc (deflated 50%)\n",
            "  adding: content/EfficientDM/ldm/modules/attention.py (deflated 74%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/openaimodel.py (deflated 82%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/model.py (deflated 85%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/__init__.cpython-310.pyc (deflated 29%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/model.cpython-310.pyc (deflated 57%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/openaimodel.cpython-310.pyc (deflated 58%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/util.cpython-310.pyc (deflated 50%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/util.py (deflated 66%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/bsrgan.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/__init__.py (deflated 60%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils/test.png (deflated 1%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils_image.py (deflated 77%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/bsrgan_light.py (deflated 73%)\n",
            "  adding: content/EfficientDM/ldm/modules/x_transformer.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/x_transformer.cpython-310.pyc (deflated 52%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/attention.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/ema.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/util.cpython-310.pyc (deflated 43%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/globalvar.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/EfficientDM/ldm/util.py (deflated 62%)\n",
            "  adding: content/EfficientDM/LICENSE (deflated 41%)\n",
            "  adding: content/EfficientDM/.gitignore (deflated 4%)\n",
            "  adding: content/EfficientDM/quant_scripts/ (stored 0%)\n",
            "  adding: content/EfficientDM/quant_scripts/quantize_ldm_naive.py (deflated 60%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_layer.py (deflated 82%)\n",
            "  adding: content/EfficientDM/quant_scripts/collect_input_4_calib.py (deflated 61%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_dataset.py (deflated 78%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_model.py (deflated 84%)\n",
            "  adding: content/EfficientDM/quant_scripts/train_efficientdm.py (deflated 71%)\n",
            "  adding: content/EfficientDM/quant_scripts/save_naive_2_intmodel.py (deflated 62%)\n",
            "  adding: content/EfficientDM/quant_scripts/downsample_talsq_ckpt.py (deflated 41%)\n",
            "  adding: content/EfficientDM/quant_scripts/sample_lora_intmodel.py (deflated 64%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/quant_model.cpython-310.pyc (deflated 59%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/quant_layer.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/EfficientDM/scripts/ (stored 0%)\n",
            "  adding: content/EfficientDM/scripts/download_models.sh (deflated 80%)\n",
            "  adding: content/EfficientDM/scripts/sample_diffusion.py (deflated 68%)\n",
            "  adding: content/EfficientDM/scripts/txt2img.py (deflated 67%)\n",
            "  adding: content/EfficientDM/scripts/download_first_stages.sh (deflated 84%)\n",
            "  adding: content/EfficientDM/scripts/inpaint.py (deflated 65%)\n",
            "  adding: content/EfficientDM/scripts/knn2img.py (deflated 71%)\n",
            "  adding: content/EfficientDM/scripts/generate_samples_4_evaluation_FP.py (deflated 69%)\n",
            "  adding: content/EfficientDM/scripts/latent_imagenet_diffusion.ipynb (deflated 25%)\n",
            "  adding: content/EfficientDM/scripts/train_searcher.py (deflated 68%)\n",
            "  adding: content/EfficientDM/README.md (deflated 51%)\n",
            "  adding: content/sample_data/ (stored 0%)\n",
            "  adding: content/sample_data/anscombe.json (deflated 83%)\n",
            "  adding: content/sample_data/README.md (deflated 39%)\n",
            "  adding: content/sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "  adding: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "  adding: content/sample_data/mnist_test.csv (deflated 88%)\n",
            "  adding: content/ (stored 0%)\n",
            "  adding: content/.config/ (stored 0%)\n",
            "  adding: content/.config/gce (stored 0%)\n",
            "  adding: content/.config/.last_update_check.json (deflated 24%)\n",
            "  adding: content/.config/configurations/ (stored 0%)\n",
            "  adding: content/.config/configurations/config_default (deflated 15%)\n",
            "  adding: content/.config/logs/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.11.07/ (stored 0%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.09.372862.log (deflated 58%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.26.532854.log (deflated 57%)\n",
            "  adding: content/.config/logs/2024.11.07/20.55.50.913514.log (deflated 57%)\n",
            "  adding: content/.config/logs/2024.11.07/20.55.26.763095.log (deflated 92%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.27.350325.log (deflated 56%)\n",
            "  adding: content/.config/logs/2024.11.07/20.56.08.122576.log (deflated 85%)\n",
            "  adding: content/.config/default_configs.db (deflated 98%)\n",
            "  adding: content/.config/active_config (stored 0%)\n",
            "  adding: content/.config/.last_opt_in_prompt.yaml (stored 0%)\n",
            "  adding: content/.config/hidden_gcloud_config_universe_descriptor_data_cache_configs.db (deflated 97%)\n",
            "  adding: content/.config/config_sentinel (stored 0%)\n",
            "  adding: content/.config/.last_survey_prompt.yaml (stored 0%)\n",
            "  adding: content/sample_data.zip (stored 0%)\n",
            "  adding: content/DiffusionInput_250steps.pth (deflated 68%)\n",
            "  adding: content/models/ (stored 0%)\n",
            "  adding: content/models/ldm/ (stored 0%)\n",
            "  adding: content/models/ldm/cin256-v2/ (stored 0%)\n",
            "  adding: content/models/ldm/cin256-v2/model.ckpt (deflated 7%)\n",
            "  adding: content/models.zip (stored 0%)\n",
            "  adding: content/EfficientDM.zip (stored 0%)\n",
            "  adding: content/.ipynb_checkpoints/ (stored 0%)\n",
            "  adding: content/taming-transformers/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/HEAD (stored 0%)\n",
            "  adding: content/taming-transformers/.git/branches/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/info/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/info/exclude (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/refs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/heads/master (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/taming-transformers/.git/hooks/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/taming-transformers/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/taming-transformers/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/taming-transformers/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/taming-transformers/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/taming-transformers/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/taming-transformers/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/taming-transformers/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/taming-transformers/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/taming-transformers/.git/index (deflated 63%)\n",
            "  adding: content/taming-transformers/.git/logs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/HEAD (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/heads/master (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/logs/refs/remotes/origin/HEAD (deflated 28%)\n",
            "  adding: content/taming-transformers/.git/packed-refs (deflated 42%)\n",
            "  adding: content/taming-transformers/.git/description (deflated 14%)\n",
            "  adding: content/taming-transformers/.git/objects/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/info/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/pack-34e24108d627c8d57cba87fd86552b8cca48134a.pack (deflated 0%)\n",
            "  adding: content/taming-transformers/.git/objects/pack/pack-34e24108d627c8d57cba87fd86552b8cca48134a.idx (deflated 2%)\n",
            "  adding: content/taming-transformers/.git/config (deflated 33%)\n",
            "  adding: content/taming-transformers/environment.yaml (deflated 45%)\n",
            "  adding: content/taming-transformers/configs/ (stored 0%)\n",
            "  adding: content/taming-transformers/configs/coco_cond_stage.yaml (deflated 60%)\n",
            "  adding: content/taming-transformers/configs/faceshq_vqgan.yaml (deflated 56%)\n",
            "  adding: content/taming-transformers/configs/imagenetdepth_vqgan.yaml (deflated 54%)\n",
            "  adding: content/taming-transformers/configs/faceshq_transformer.yaml (deflated 64%)\n",
            "  adding: content/taming-transformers/configs/imagenet_vqgan.yaml (deflated 55%)\n",
            "  adding: content/taming-transformers/configs/drin_transformer.yaml (deflated 72%)\n",
            "  adding: content/taming-transformers/configs/coco_scene_images_transformer.yaml (deflated 67%)\n",
            "  adding: content/taming-transformers/configs/custom_vqgan.yaml (deflated 55%)\n",
            "  adding: content/taming-transformers/configs/sflckr_cond_stage.yaml (deflated 56%)\n",
            "  adding: content/taming-transformers/configs/open_images_scene_images_transformer.yaml (deflated 70%)\n",
            "  adding: content/taming-transformers/License.txt (deflated 41%)\n",
            "  adding: content/taming-transformers/taming/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/models/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/models/cond_transformer.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/models/dummy_cond_stage.py (deflated 58%)\n",
            "  adding: content/taming-transformers/taming/models/vqgan.py (deflated 85%)\n",
            "  adding: content/taming-transformers/taming/data/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/data/utils.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/data/coco.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_open_images.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_dataset.py (deflated 76%)\n",
            "  adding: content/taming-transformers/taming/data/annotated_objects_coco.py (deflated 72%)\n",
            "  adding: content/taming-transformers/taming/data/base.py (deflated 67%)\n",
            "  adding: content/taming-transformers/taming/data/custom.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/data/image_transforms.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/data/faceshq.py (deflated 82%)\n",
            "  adding: content/taming-transformers/taming/data/imagenet.py (deflated 78%)\n",
            "  adding: content/taming-transformers/taming/data/sflckr.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/utils.py (deflated 66%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/objects_bbox.py (deflated 64%)\n",
            "  adding: content/taming-transformers/taming/data/conditional_builder/objects_center_points.py (deflated 71%)\n",
            "  adding: content/taming-transformers/taming/data/open_images_helper.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/data/helper_types.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/data/ade20k.py (deflated 73%)\n",
            "  adding: content/taming-transformers/taming/lr_scheduler.py (deflated 65%)\n",
            "  adding: content/taming-transformers/taming/modules/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/segmentation.py (deflated 63%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/lpips.py (deflated 69%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/__init__.py (deflated 2%)\n",
            "  adding: content/taming-transformers/taming/modules/losses/vqperceptual.py (deflated 76%)\n",
            "  adding: content/taming-transformers/taming/modules/discriminator/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/discriminator/model.py (deflated 64%)\n",
            "  adding: content/taming-transformers/taming/modules/diffusionmodules/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/diffusionmodules/model.py (deflated 89%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/quantize.py (deflated 78%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/__pycache__/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/vqvae/__pycache__/quantize.cpython-310.pyc (deflated 51%)\n",
            "  adding: content/taming-transformers/taming/modules/misc/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/misc/coord.py (deflated 62%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/ (stored 0%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/permuter.py (deflated 83%)\n",
            "  adding: content/taming-transformers/taming/modules/transformer/mingpt.py (deflated 74%)\n",
            "  adding: content/taming-transformers/taming/modules/util.py (deflated 71%)\n",
            "  adding: content/taming-transformers/taming/util.py (deflated 64%)\n",
            "  adding: content/taming-transformers/data/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/celebahqvalidation.txt (deflated 80%)\n",
            "  adding: content/taming-transformers/data/sflckr_examples.txt (deflated 47%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001498.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000509.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000125.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001845.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000573.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001578.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000734.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001583.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001698.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000203.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001388.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001412.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000880.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000532.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000126.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001766.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000123.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000875.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000289.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001209.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000262.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001177.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000636.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001966.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000287.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00000603.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001851.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001200.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_images/ADE_val_00001947.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ILSVRC2012_val_00032333.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101006/ILSVRC2012_val_00047325.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02085782/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02085782/ILSVRC2012_val_00012298.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02111889/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02111889/ILSVRC2012_val_00042625.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00046802.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00001336.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01828970/ILSVRC2012_val_00008236.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02102318/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02102318/ILSVRC2012_val_00024691.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02086646/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02086646/ILSVRC2012_val_00011473.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02089973/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02089973/ILSVRC2012_val_00000028.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02105505/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02105505/ILSVRC2012_val_00031252.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ILSVRC2012_val_00047491.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01820546/ILSVRC2012_val_00034784.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02088466/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02088466/ILSVRC2012_val_00013651.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02096294/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02096294/ILSVRC2012_val_00042133.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02110627/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02110627/ILSVRC2012_val_00008310.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01847000/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01847000/ILSVRC2012_val_00022364.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02093256/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02093256/ILSVRC2012_val_00046547.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099712/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099712/ILSVRC2012_val_00023471.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01795545/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01795545/ILSVRC2012_val_00023344.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101556/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02101556/ILSVRC2012_val_00030540.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02100877/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02100877/ILSVRC2012_val_00039863.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099601/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n02099601/ILSVRC2012_val_00005697.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01819313/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01819313/ILSVRC2012_val_00003068.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01843065/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_depth/n01843065/ILSVRC2012_val_00022439.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation-annotations-bbox.csv (deflated 77%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation-images.csv (deflated 10%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train-images-boxable.csv (deflated 89%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/class-descriptions-boxable.csv (deflated 53%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000afe7726e121ea.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba40bf7a2b458.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b59a7822679e6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1b3b85edd850.jpg (deflated 9%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc5ad4cc3ae73.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b5bc07c0c5df7.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4671075914cd.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b09d5d3fc821f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b38d9f2f664fe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1971d8daaeef.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc75d38907c78.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bcd3bcd95cbb3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000af180a3163f17.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adef7197e3118.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab31e6be35fed.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b3940e7d25c03.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abc075d659122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9c365c9e307a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b567c26dd4e5d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b825dea3016eb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9a97776b3634.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b397382b2464a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad0ecfb21ee63.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb0ae453283b0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4935979bf4b5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba3ca8a2ca955.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000aee0af66d4237.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000acf666d991c39.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9b00d7aef8f5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000baa6f7dae9b79.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad3d42653f5f6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba28d70b1a999.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b393437134262.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b299b5f5ed902.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000aecd78b230135.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9b61afea2cd4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b7dfaa1810a83.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac8c676b6077a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bbdf0dc8099d8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab7bec71cc50a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc7b0a1889bcb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9f3ba4891c11.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2a982a903d0d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ae235808cc1e8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b260e1f08a32a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b8d80f7386698.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b50bdd1933a36.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b06c0eed42a4c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb200fc78fc30.jpg (deflated 6%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad20b5e452b24.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba940f8cfc9bf.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bcee5bed5446b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abc821f66a892.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b21663becc68e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc387c731dd97.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b168e791f591d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ba221f70676c6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b72e1446f8849.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b432ae644b679.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b093da01e5bfe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b76a9b80ba43a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ae28755d2d20e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac34008b0ba4c.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc1eb7f74adae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b0f5159f54105.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb81adefe7332.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b70a84aab664b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b63a1445f53c8.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad6c520be9ec5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b55559b0244d7.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2b00065e564a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b65a36ad46f9e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adfe5b817011c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b4fcdf1af3361.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb8bd9b1bca65.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000adcdd7244ce4a.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b93644609911f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9007a01f7405.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b485cedacbf97.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b42cae15622e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ac95750ac7399.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bab5b1a67844e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9814a07fd974.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ab8c20b3e5b58.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b606e130bdf5e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc5006eb7fd98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b81b5757963e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b87119cc301cf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b29496f75c8e5.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000abe5eddc5b303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b9d6c0f7d794d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b2d1789d5f80d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bc33717a6371f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b55e339f0b131.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000af631fb329557.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b0f235dcf2caa.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb2f7132013dc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000bb846e2629e83.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000b1b92f0800e94.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/train/000ad6fa67b5ad96.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/oidv6-train-annotations-bbox.csv (deflated 76%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ada35baba28134b.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c6ddd2c210450e.jpg (deflated 6%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad96a2881998657.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09df63bd01367ca3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a41cda5f44baaf6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac166d12e401a98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a94296ff543a1dc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aaad833ac61ac9d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0adc1330287b2e66.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f8e760f60df0da.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab2b64f27f8baca.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09e094375efab7fe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d64f43c7111879.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9ff75a7897e757.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ade7aef439e2102.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab050b51e78acdb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a02e8b6820064f5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a917bbca24cf75d.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a78374f2d3949ae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7c597abf1e90d4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1f4761dc7fe1eb.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a08a4711c728078.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aacbdb54e853a0a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a03326036647703.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a556c8163b58fae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a392d80c905a9df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9f183e46c76019.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac2f91a7995aa8b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a23d3f0e7d850f4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1bd356f90aaab6.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a02c648d24f39fb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aad9fc79a35bd53.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c7f89055cf399b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f8b77a88f224d9.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aa206fa7ea80036.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab5c690eebfad95.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09cbba9f5e097a19.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a2c6ef66896fb92.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7ffd65766a4741.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09dcb9b52055d40f.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a4db5693da70448.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a73064c82730ff5.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a47e7d602855f93.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d354dbd3dcc857.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c67960e389e4df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c5b4d6bc25788d.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7074a2a5515531.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d45c49c4adbae4.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad99d610a9092e6.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac3c1db1b3645f2.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a37aa0734ac8016.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09ebcee57699eb98.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d2112596d9155b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0adc373e996aadc2.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09d8aa2d19ff724d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c993afacd01547.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a4abf0a8071b917.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a563d05ebab4fe3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a13dcaaab9a35e0.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09f531fe4f6d95f3.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a82f0443c940816.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a50911d08250183.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09c863d76bcf6b00.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a278d979b63fc72.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aae34863935e33a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a34d80ee1db201e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac51477636a6933.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ac52440f73b5c80.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3c01759e77a02d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a877314ca2039d9.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7d56b2fb989fe8.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a8657e8b5c9d7bb.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a72fef43a51c479.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7fbc1d68e4e5ae.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a39325e5ad7f5a0.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7f4d9a0ccb9afe.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a600f1148d1023c.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a599940d33b6b2b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a9f73b3c2557150.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09fa093bcd300c1a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a120822d362dddf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a5972c68b6bb265.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09e617d9d3120b32.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7be0b883a12966.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0add91a2efb3f33d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a6a03c8f23ee744.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ab10a6417ef2301.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3f577a327ca7cc.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09ea349ee555b61d.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a7f13330a5d0023.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/09dd0671cd633432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a6bc386b28f2aac.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3f9b3d57ef354a.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0aa3a6c33fca122b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad7884032419621.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad7bad30cd432df.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a3873442ad329c2.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0a1b11867383b13e.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0ad602c943c9f568.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/open_images_annotations_100/validation/0acfa779589204bf.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000303653.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000205834.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000406997.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000255824.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000356347.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000522393.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000361180.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000119445.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000166259.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000350405.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000057672.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000299720.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000517069.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000348045.png (deflated 30%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000237928.png (deflated 11%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000110638.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000569273.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000323895.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000403385.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000299355.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000348481.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000175387.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000052507.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000185599.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000154358.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000098392.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000335529.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000256775.png (deflated 11%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000064898.png (deflated 35%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000166563.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000018380.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000491464.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000231169.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000128658.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_segmentations/000000452122.png (deflated 27%)\n",
            "  adding: content/taming-transformers/data/flickr_tags.txt (deflated 56%)\n",
            "  adding: content/taming-transformers/data/ade20k_examples.txt (deflated 79%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000126.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000636.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001412.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001498.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000603.png (deflated 10%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000532.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000734.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001845.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001177.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000303.png (deflated 7%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000203.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000289.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000875.png (stored 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000287.png (deflated 33%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001766.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001388.png (deflated 24%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001578.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000509.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001966.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000262.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001851.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001947.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001583.png (deflated 16%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000123.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000880.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001200.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000573.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00000125.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001698.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/ade20k_segmentations/ADE_val_00001209.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/instances_train2017.json (deflated 70%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/instances_val2017.json (deflated 70%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/stuff_train2017.json (deflated 54%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/annotations/stuff_val2017.json (deflated 54%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016598.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018150.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013546.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013774.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014007.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016439.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012120.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015660.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012062.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018193.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015335.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013177.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014038.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010092.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018737.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015440.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019924.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016451.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017031.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015079.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016958.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014439.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010707.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018491.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016502.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019042.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020059.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017436.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013201.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013348.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018575.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015254.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015751.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017182.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010995.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017029.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013597.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017379.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016249.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018837.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015746.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017905.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017627.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018833.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016010.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011051.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020333.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018519.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015272.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019402.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011511.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017714.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013659.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010363.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017207.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017178.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000016228.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013004.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019221.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012639.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019109.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012667.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012670.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011149.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010977.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015517.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011813.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011197.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013291.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014831.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013729.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011699.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011615.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010583.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020247.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014888.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012576.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015597.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011760.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014226.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019742.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000019786.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015278.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015956.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017959.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015338.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012748.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017115.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000010764.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000017899.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000015497.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000018770.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000012280.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000013923.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000011122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000020107.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/val2017/000000014473.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010145.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010229.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010014.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010123.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010343.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010179.jpg (deflated 5%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010400.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010058.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010309.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010313.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010056.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010142.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010012.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010463.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010449.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010430.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010393.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010161.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010290.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010241.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010114.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010276.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010176.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010440.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010219.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010073.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010281.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010040.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010039.jpg (deflated 10%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010024.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010107.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010303.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010321.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010239.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010244.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010248.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010046.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010232.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010324.jpg (deflated 7%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010005.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010407.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010358.jpg (deflated 4%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010108.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010369.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010327.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010405.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010115.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010211.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010386.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010249.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010444.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010256.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010008.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010205.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010319.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010192.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010230.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010216.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010136.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010442.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010175.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010265.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010432.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010275.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010337.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010243.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010346.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010023.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010138.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010041.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010421.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010130.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010434.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010428.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010166.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010097.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010094.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010125.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010083.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010015.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010403.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010414.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010263.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010388.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010077.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010318.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010342.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010069.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010420.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010104.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010037.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010149.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010445.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010082.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010222.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010084.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010217.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010196.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010245.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_annotations_100/train2017/000000010395.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/ffhqtrain.txt (deflated 70%)\n",
            "  adding: content/taming-transformers/data/celebahqtrain.txt (deflated 80%)\n",
            "  adding: content/taming-transformers/data/subreddits.txt (deflated 37%)\n",
            "  adding: content/taming-transformers/data/ffhqvalidation.txt (deflated 69%)\n",
            "  adding: content/taming-transformers/data/coco_examples.txt (deflated 71%)\n",
            "  adding: content/taming-transformers/data/coco_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000403385.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000064898.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000299720.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000569273.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000166563.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000205834.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000348045.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000237928.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000356347.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000350405.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000303653.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000119445.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000154358.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000522393.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000231169.jpg (deflated 3%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000018380.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000361180.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000323895.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000452122.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000491464.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000335529.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000110638.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000348481.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000098392.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000057672.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000185599.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000255824.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000256775.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000175387.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000166259.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000517069.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000299355.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000406997.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000128658.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/coco_images/000000052507.jpg (deflated 7%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/newzealand_np/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/newzealand_np/7942812194_9348729b93_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/4748115806_7219c2b3be_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/14996762478_a9bdbf959a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/3542389801_a2cbfee1e1_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/geysir/26320755536_7c769b6218_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/meadow/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/meadow/18864473291_844325caab_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/volcano/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/volcano/50254383883_27ed6ea93a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/desert/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/desert/4534149722_3cc4f92891_b.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/25735082181_999927fe5a_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/norway/20099378793_cc2df820af_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ireland/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/ireland/15570753471_74db396d14_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_mountains/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_mountains/33509672006_bf4c416afd_b.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/mongolia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/mongolia/6076373946_e9ea2aee32_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/cliff_ocean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/cliff_ocean/36142796444_45d452f567_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/8364557382_c6c9ee2fd6_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/black_forest/44974691685_8e7372e2b1_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/14351041152_ef77484a1f_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/carribean/18176301_c9d27557cf_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/alaska_lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/alaska_lakes/43259216952_59352d7204_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_landscape/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/swiss_landscape/4079319632_0133685b2c_b.jpg (deflated 2%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/12822389285_a7723081b5_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/australia/8720651218_ca82a6608e_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/256743165_9f130ba95b_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/canada/2883773_881c197107_c.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_images/lakes/39933489595_f0e5d85b6d_b.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/newzealand_np/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/newzealand_np/7942812194_9348729b93_b.png (deflated 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/3542389801_a2cbfee1e1_b.png (deflated 14%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/26320755536_7c769b6218_b.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/14996762478_a9bdbf959a_b.png (deflated 9%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/geysir/4748115806_7219c2b3be_b.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/meadow/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/meadow/18864473291_844325caab_b.png (deflated 4%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/volcano/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/volcano/50254383883_27ed6ea93a_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/desert/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/desert/4534149722_3cc4f92891_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/25735082181_999927fe5a_b.png (deflated 20%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/norway/20099378793_cc2df820af_b.png (deflated 10%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ireland/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/ireland/15570753471_74db396d14_b.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_mountains/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_mountains/33509672006_bf4c416afd_b.png (deflated 32%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/mongolia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/mongolia/6076373946_e9ea2aee32_b.png (deflated 13%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/cliff_ocean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/cliff_ocean/36142796444_45d452f567_b.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/8364557382_c6c9ee2fd6_b.png (deflated 43%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/black_forest/44974691685_8e7372e2b1_b.png (deflated 12%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/14351041152_ef77484a1f_b.png (deflated 6%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/carribean/18176301_c9d27557cf_b.png (deflated 28%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/alaska_lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/alaska_lakes/43259216952_59352d7204_b.png (deflated 29%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_landscape/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/swiss_landscape/4079319632_0133685b2c_b.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/12822389285_a7723081b5_b.png (deflated 1%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/australia/8720651218_ca82a6608e_b.png (deflated 5%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/2883773_881c197107_c.png (deflated 3%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/canada/256743165_9f130ba95b_b.png (deflated 8%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/lakes/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/sflckr_segmentations/lakes/39933489595_f0e5d85b6d_b.png (deflated 2%)\n",
            "  adding: content/taming-transformers/data/drin_examples.txt (deflated 75%)\n",
            "  adding: content/taming-transformers/data/drin_images/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ILSVRC2012_val_00032333.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101006/ILSVRC2012_val_00047325.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02085782/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02085782/ILSVRC2012_val_00012298.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02111889/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02111889/ILSVRC2012_val_00042625.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00046802.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00001336.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01828970/ILSVRC2012_val_00008236.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02102318/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02102318/ILSVRC2012_val_00024691.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02086646/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02086646/ILSVRC2012_val_00011473.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02089973/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02089973/ILSVRC2012_val_00000028.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02105505/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02105505/ILSVRC2012_val_00031252.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ILSVRC2012_val_00034784.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01820546/ILSVRC2012_val_00047491.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02088466/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02088466/ILSVRC2012_val_00013651.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02096294/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02096294/ILSVRC2012_val_00042133.JPEG (deflated 3%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02110627/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02110627/ILSVRC2012_val_00008310.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01847000/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01847000/ILSVRC2012_val_00022364.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02093256/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02093256/ILSVRC2012_val_00046547.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099712/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099712/ILSVRC2012_val_00023471.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01795545/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01795545/ILSVRC2012_val_00023344.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101556/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02101556/ILSVRC2012_val_00030540.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02100877/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02100877/ILSVRC2012_val_00039863.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099601/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n02099601/ILSVRC2012_val_00005697.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01819313/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01819313/ILSVRC2012_val_00003068.JPEG (deflated 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01843065/ (stored 0%)\n",
            "  adding: content/taming-transformers/data/drin_images/n01843065/ILSVRC2012_val_00022439.JPEG (deflated 1%)\n",
            "  adding: content/taming-transformers/scripts/ (stored 0%)\n",
            "  adding: content/taming-transformers/scripts/taming-transformers.ipynb (deflated 25%)\n",
            "  adding: content/taming-transformers/scripts/make_scene_samples.py (deflated 67%)\n",
            "  adding: content/taming-transformers/scripts/sample_conditional.py (deflated 68%)\n",
            "  adding: content/taming-transformers/scripts/extract_submodel.py (deflated 49%)\n",
            "  adding: content/taming-transformers/scripts/extract_segmentation.py (deflated 60%)\n",
            "  adding: content/taming-transformers/scripts/reconstruction_usage.ipynb (deflated 25%)\n",
            "  adding: content/taming-transformers/scripts/sample_fast.py (deflated 70%)\n",
            "  adding: content/taming-transformers/scripts/make_samples.py (deflated 69%)\n",
            "  adding: content/taming-transformers/scripts/extract_depth.py (deflated 63%)\n",
            "  adding: content/taming-transformers/README.md (deflated 65%)\n",
            "  adding: content/taming-transformers/main.py (deflated 74%)\n",
            "  adding: content/taming-transformers/setup.py (deflated 39%)\n",
            "  adding: content/taming-transformers/assets/ (stored 0%)\n",
            "  adding: content/taming-transformers/assets/coco_scene_images_training.svg (deflated 85%)\n",
            "  adding: content/taming-transformers/assets/teaser.png (deflated 1%)\n",
            "  adding: content/taming-transformers/assets/lake_in_the_mountains.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/drin.jpg (deflated 1%)\n",
            "  adding: content/taming-transformers/assets/faceshq.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/imagenet.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/stormy.jpeg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/sunset_and_ocean.jpg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/mountain.jpeg (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/scene_images_samples.svg (deflated 25%)\n",
            "  adding: content/taming-transformers/assets/first_stage_mushrooms.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/first_stage_squirrels.png (deflated 0%)\n",
            "  adding: content/taming-transformers/assets/birddrawnbyachild.png (deflated 0%)\n",
            "  adding: content/EfficientDM/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/HEAD (stored 0%)\n",
            "  adding: content/EfficientDM/.git/branches/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/info/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/info/exclude (deflated 28%)\n",
            "  adding: content/EfficientDM/.git/refs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/heads/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/heads/main (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/tags/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/refs/remotes/origin/HEAD (stored 0%)\n",
            "  adding: content/EfficientDM/.git/hooks/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-receive.sample (deflated 40%)\n",
            "  adding: content/EfficientDM/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
            "  adding: content/EfficientDM/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-rebase.sample (deflated 59%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
            "  adding: content/EfficientDM/.git/hooks/commit-msg.sample (deflated 44%)\n",
            "  adding: content/EfficientDM/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
            "  adding: content/EfficientDM/.git/hooks/post-update.sample (deflated 27%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-commit.sample (deflated 45%)\n",
            "  adding: content/EfficientDM/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
            "  adding: content/EfficientDM/.git/hooks/update.sample (deflated 68%)\n",
            "  adding: content/EfficientDM/.git/hooks/pre-push.sample (deflated 49%)\n",
            "  adding: content/EfficientDM/.git/index (deflated 55%)\n",
            "  adding: content/EfficientDM/.git/logs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/HEAD (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/heads/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/heads/main (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/origin/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/logs/refs/remotes/origin/HEAD (deflated 26%)\n",
            "  adding: content/EfficientDM/.git/packed-refs (deflated 9%)\n",
            "  adding: content/EfficientDM/.git/description (deflated 14%)\n",
            "  adding: content/EfficientDM/.git/objects/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/info/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/ (stored 0%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/pack-981733d82cdf3418a37a580efdd4179c422f0000.idx (deflated 18%)\n",
            "  adding: content/EfficientDM/.git/objects/pack/pack-981733d82cdf3418a37a580efdd4179c422f0000.pack (deflated 0%)\n",
            "  adding: content/EfficientDM/.git/config (deflated 31%)\n",
            "  adding: content/EfficientDM/configs/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/cin-ldm-vq-f8.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/lsun_bedrooms-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/cin256-v2.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/lsun_churches-ldm-kl-8.yaml (deflated 60%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/txt2img-1p4B-eval.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/celebahq-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/latent-diffusion/ffhq-ldm-vq-4.yaml (deflated 61%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_8x8x64.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_32x32x4.yaml (deflated 55%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_64x64x3.yaml (deflated 55%)\n",
            "  adding: content/EfficientDM/configs/autoencoder/autoencoder_kl_16x16x16.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/configs/retrieval-augmented-diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/configs/retrieval-augmented-diffusion/768x768.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis512/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis512/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/semantic_synthesis256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ldm/layout2img-openimages256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/layout2img-openimages256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_beds256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_beds256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/cin256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/cin256/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/ffhq256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/ffhq256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/inpainting_big/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/inpainting_big/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/bsr_sr/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/bsr_sr/config.yaml (deflated 67%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_churches256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/lsun_churches256/config.yaml (deflated 65%)\n",
            "  adding: content/EfficientDM/models/ldm/celeba256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/celeba256/config.yaml (deflated 63%)\n",
            "  adding: content/EfficientDM/models/ldm/text2img256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/ldm/text2img256/config.yaml (deflated 64%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8-n256/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8-n256/config.yaml (deflated 56%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4-noattn/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4-noattn/config.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f16/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f16/config.yaml (deflated 58%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f32/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f32/config.yaml (deflated 58%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f4/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f4/config.yaml (deflated 56%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f8/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/kl-f8/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f4/config.yaml (deflated 54%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f16/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f16/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8/ (stored 0%)\n",
            "  adding: content/EfficientDM/models/first_stage_models/vq-f8/config.yaml (deflated 57%)\n",
            "  adding: content/EfficientDM/ldm/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/autoencoder.py (deflated 80%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ddpm.py (deflated 79%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/ddim.py (deflated 86%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/plms.py (deflated 76%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ddpm.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/__init__.cpython-310.pyc (deflated 24%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/__pycache__/ddim.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/models/diffusion/classifier.py (deflated 71%)\n",
            "  adding: content/EfficientDM/ldm/models/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/models/__pycache__/autoencoder.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/EfficientDM/ldm/globalvar.py (deflated 61%)\n",
            "  adding: content/EfficientDM/ldm/data/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/data/base.py (deflated 54%)\n",
            "  adding: content/EfficientDM/ldm/data/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/data/imagenet.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/data/lsun.py (deflated 72%)\n",
            "  adding: content/EfficientDM/ldm/lr_scheduler.py (deflated 80%)\n",
            "  adding: content/EfficientDM/ldm/modules/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/modules.py (deflated 70%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/__init__.cpython-310.pyc (deflated 26%)\n",
            "  adding: content/EfficientDM/ldm/modules/encoders/__pycache__/modules.cpython-310.pyc (deflated 55%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/contperceptual.py (deflated 76%)\n",
            "  adding: content/EfficientDM/ldm/modules/losses/vqperceptual.py (deflated 74%)\n",
            "  adding: content/EfficientDM/ldm/modules/ema.py (deflated 68%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/distributions.py (deflated 66%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/__init__.cpython-310.pyc (deflated 25%)\n",
            "  adding: content/EfficientDM/ldm/modules/distributions/__pycache__/distributions.cpython-310.pyc (deflated 50%)\n",
            "  adding: content/EfficientDM/ldm/modules/attention.py (deflated 74%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/openaimodel.py (deflated 82%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__init__.py (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/model.py (deflated 85%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/__init__.cpython-310.pyc (deflated 29%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/model.cpython-310.pyc (deflated 57%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/openaimodel.cpython-310.pyc (deflated 58%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/__pycache__/util.cpython-310.pyc (deflated 50%)\n",
            "  adding: content/EfficientDM/ldm/modules/diffusionmodules/util.py (deflated 66%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/bsrgan.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/__init__.py (deflated 60%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils/test.png (deflated 1%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/utils_image.py (deflated 77%)\n",
            "  adding: content/EfficientDM/ldm/modules/image_degradation/bsrgan_light.py (deflated 73%)\n",
            "  adding: content/EfficientDM/ldm/modules/x_transformer.py (deflated 75%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/x_transformer.cpython-310.pyc (deflated 52%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/attention.cpython-310.pyc (deflated 53%)\n",
            "  adding: content/EfficientDM/ldm/modules/__pycache__/ema.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/util.cpython-310.pyc (deflated 43%)\n",
            "  adding: content/EfficientDM/ldm/__pycache__/globalvar.cpython-310.pyc (deflated 45%)\n",
            "  adding: content/EfficientDM/ldm/util.py (deflated 62%)\n",
            "  adding: content/EfficientDM/LICENSE (deflated 41%)\n",
            "  adding: content/EfficientDM/.gitignore (deflated 4%)\n",
            "  adding: content/EfficientDM/quant_scripts/ (stored 0%)\n",
            "  adding: content/EfficientDM/quant_scripts/quantize_ldm_naive.py (deflated 60%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_layer.py (deflated 82%)\n",
            "  adding: content/EfficientDM/quant_scripts/collect_input_4_calib.py (deflated 61%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_dataset.py (deflated 78%)\n",
            "  adding: content/EfficientDM/quant_scripts/quant_model.py (deflated 84%)\n",
            "  adding: content/EfficientDM/quant_scripts/train_efficientdm.py (deflated 71%)\n",
            "  adding: content/EfficientDM/quant_scripts/save_naive_2_intmodel.py (deflated 62%)\n",
            "  adding: content/EfficientDM/quant_scripts/downsample_talsq_ckpt.py (deflated 41%)\n",
            "  adding: content/EfficientDM/quant_scripts/sample_lora_intmodel.py (deflated 64%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/ (stored 0%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/quant_model.cpython-310.pyc (deflated 59%)\n",
            "  adding: content/EfficientDM/quant_scripts/__pycache__/quant_layer.cpython-310.pyc (deflated 54%)\n",
            "  adding: content/EfficientDM/scripts/ (stored 0%)\n",
            "  adding: content/EfficientDM/scripts/download_models.sh (deflated 80%)\n",
            "  adding: content/EfficientDM/scripts/sample_diffusion.py (deflated 68%)\n",
            "  adding: content/EfficientDM/scripts/txt2img.py (deflated 67%)\n",
            "  adding: content/EfficientDM/scripts/download_first_stages.sh (deflated 84%)\n",
            "  adding: content/EfficientDM/scripts/inpaint.py (deflated 65%)\n",
            "  adding: content/EfficientDM/scripts/knn2img.py (deflated 71%)\n",
            "  adding: content/EfficientDM/scripts/generate_samples_4_evaluation_FP.py (deflated 69%)\n",
            "  adding: content/EfficientDM/scripts/latent_imagenet_diffusion.ipynb (deflated 25%)\n",
            "  adding: content/EfficientDM/scripts/train_searcher.py (deflated 68%)\n",
            "  adding: content/EfficientDM/README.md (deflated 51%)\n",
            "  adding: content/sample_data/ (stored 0%)\n",
            "  adding: content/sample_data/anscombe.json (deflated 83%)\n",
            "  adding: content/sample_data/README.md (deflated 39%)\n",
            "  adding: content/sample_data/mnist_train_small.csv (deflated 88%)\n",
            "  adding: content/sample_data/california_housing_train.csv (deflated 79%)\n",
            "  adding: content/sample_data/california_housing_test.csv (deflated 76%)\n",
            "  adding: content/sample_data/mnist_test.csv (deflated 88%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTM1GNPyvozG",
        "outputId": "bbc9f811-df1f-483f-a38e-6d7983ebd775"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: not a git repository (or any of the parent directories): .git\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvz7o9LQzeQ7",
        "outputId": "cc2e8da9-b3fd-4824-8524-c3c4ea8356a1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DiffusionInput_250steps.pth  models\t  sample_data.zip\n",
            "EfficientDM\t\t     models.zip   taming-transformers\n",
            "EfficientDM.zip\t\t     sample_data  taming-transformers.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtDjszM5zgb1",
        "outputId": "e8a1ce39-29c4-442f-e22a-7ae93b97bf8e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mhint: Using 'master' as the name for the initial branch. This default branch name\u001b[m\n",
            "\u001b[33mhint: is subject to change. To configure the initial branch name to use in all\u001b[m\n",
            "\u001b[33mhint: of your new repositories, which will suppress this warning, call:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit config --global init.defaultBranch <name>\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\u001b[m\n",
            "\u001b[33mhint: 'development'. The just-created branch can be renamed via this command:\u001b[m\n",
            "\u001b[33mhint: \u001b[m\n",
            "\u001b[33mhint: \tgit branch -m <name>\u001b[m\n",
            "Initialized empty Git repository in /content/.git/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GwDGN3WMzixk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}